{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Now, we will consider the situations when we do not know in advance how many clusters we want. Hierarchical clustering does not require a pre-set **K** value. Another advantage of this approach is that it creates a hierarchy of clusters; we can analyze and pick the clustering at any scale we want. \n",
    "\n",
    "\n",
    "Once the algorithm is run, we end up with a tree-like visual representation of the observations, \n",
    "called a dendrogram, that allows us to view at once the clusterings \n",
    "obtained for each possible number of clusters, from 1 to n.\n",
    "\n",
    "Bottom-up or agglomerative clustering is the most common type of hierarchical clustering, \n",
    "and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the images shown below, each leaf of the dendrogram represents an observation. \n",
    "As we move up the tree, some leaves begin to fuse into branches. \n",
    "These correspond to observations that are similar to each other. \n",
    "For any two observations, we can look for the point in the tree where branches \n",
    "containing those two observations are first fused. \n",
    "The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. \n",
    "Thus, observations that fuse at the very bottom of the tree are quite similar to each other, \n",
    "whereas observations that fuse close to the top of the tree will tend to be quite different. \n",
    "\n",
    "<img src=\"../images/dendogram.JPG\">\n",
    "$$ Figure\\  1$$\n",
    "\n",
    "The dashed line represents **the cut**. \n",
    "The left image is not cut which gives us one cluster for all observations. \n",
    "The middle image is cut in such a way that two clusters are generated. \n",
    "The cut on the right image generates 3 clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#ce7721; font-size:18px; font-weight:700\">  Interpreting a dendrogram\n",
    "\n",
    "Consider the simple dendrogram below:\n",
    "\n",
    "<img src=\"../images/simple_dendogram.PNG\" height=\"1200\" width=\"600\">\n",
    "$$ Figure\\  2$$\n",
    "\n",
    "It is obtained from hierarchically clustering nine observations. \n",
    "One can see that observations 5 and 7 are quite similar to each other, \n",
    "since they fuse at the lowest point on the dendrogram. \n",
    "Observations 1 and 6 are also quite similar to each other. \n",
    "However, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7, even though observations 9 and 2 are close together in terms of horizontal distance. This is because\n",
    "observations 2, 8, 5, and 7 all fuse with observation 9 at the same height, approximately\n",
    "1.8.  This can be seen from the right-hand panel of the above figure. Conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it mathematically, there are $2^{n−1}$ possible reorderings of the dendrogram, \n",
    "where n is the number of leaves. \n",
    "This is because at each of the $n − 1$ points where fusions occur, \n",
    "the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram.\n",
    "Therefore, we cannot draw conclusions about the similarity of two observations based \n",
    "on their proximity along the horizontal axis. \n",
    "Rather, we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused.\n",
    "\n",
    "Now, we can move on to the issue of identifying clusters on the basis of a dendrogram. \n",
    "In order to do this, we make a horizontal cut across the dendrogram, \n",
    "as shown in the center and the right-hand panels of Figure (1). \n",
    "The height of the cut to the dendrogram serves the same role as the K in K-means clustering: \n",
    "it controls the number of clusters obtained. \n",
    "A very attractive aspect of hierarchical clustering: \n",
    "one single dendrogram can be used to obtain any number of clusters.\n",
    "More specifically, a user can qualitatively assess the effects of different choices of $K$ after\n",
    "the algorithm has run to completion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:#1576b2; font-size:20px; font-weight:700\"> Hierarchical Clustering Algorithm\n",
    "\n",
    "A dissimilarity measure, most often Euclidean distance, is used between each pair of observations. \n",
    "Starting out at the bottom of the dendrogram, each of the $n$ observations is treated as its own cluster. \n",
    "The two clusters that are most similar to each other are then fused so that there are now $n−1$ clusters. \n",
    "Next, the two clusters that are most similar to each other are fused again, \n",
    "so that there are now $n−2$ clusters. \n",
    "The algorithm proceeds in this fashion until all of the observations belong to one single cluster, \n",
    "and the dendrogram is complete.\n",
    "\n",
    "The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. \n",
    "This extension is achieved by developing the notion of **linkage**, \n",
    "which defines the dissimilarity between two groups of observations. \n",
    "The four most common types of linkage are complete, average, single, and centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Linkage</th>\n",
    "        <th style=\"text-align:center\">Linkage Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Complete</b></td>\n",
    "        <td style=\"text-align:center\">Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Single</b></td>\n",
    "        <td style=\"text-align:center\">Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Average</b></td>\n",
    "        <td style=\"text-align:center\">Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Centroid</b></td>\n",
    "        <td style=\"text-align:center\">Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<span style=\"color:#1576b2; font-size:16px; font-weight:700\"> Algorithm : Hierarchical Clustering\n",
    "\n",
    "-------\n",
    "\n",
    "1. Begin with $n$ observations and a measure (such as Euclidean distance) of all the ${n}\\choose{2}$ = $n(n−1)/2$ pairwise dissimilarities. Treat each observation as its own cluster.\n",
    "2. For $i = n, n − 1, . . . , 2$:\n",
    "\n",
    "    a. Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n",
    "    \n",
    "    b. Compute the new pairwise inter-cluster dissimilarities among the $i − 1$ remaining clusters.\n",
    "-----\n",
    "\n",
    "Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:#1576b2; font-size:18px; font-weight:700\"> Choice of Dissimilarity Measure\n",
    "\n",
    "We have used Euclidean distance as the dissimilarity measure till now. \n",
    "But sometimes other dissimilarity measures might be preferred. \n",
    "For example, correlation-based distance considers two observations to be similar if their features are highly correlated, \n",
    "even though the observed values may be far apart in terms of Euclidean distance.\n",
    "\n",
    "Normally correlation is computed between variables, \n",
    "but here it is computed between the observation profiles for each pair of observations. \n",
    "The choice of dissimilarity measure is very important as it has a strong effect on the resulting dendrogram. \n",
    "In general, attention should be paid to the type of data being clustered and the scientific question at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "**Example:** \n",
    "Consider an online retailer interested in clustering shoppers based on their past shopping histories. \n",
    "The goal is to identify subgroups of similar shoppers, \n",
    "so that shoppers within each subgroup can be shown items and advertisements that are \n",
    "particularly likely to interest them. \n",
    "\n",
    "\n",
    "Suppose the data takes the form of a matrix where the rows are the shoppers and the \n",
    "columns are the items available for purchase; \n",
    "the elements of the data matrix indicate the number of times a given shopper has purchased a given item \n",
    "(i.e. a 0 if the shopper has never purchased this item, a 1 if the shopper has purchased it once, etc.).\n",
    "\n",
    "**What type of dissimilarity measure should be used to cluster the shoppers?** \n",
    "If Euclidean distance is used, then shoppers who have bought very few items overall \n",
    "(i.e. infrequent users of the online shopping site) will be clustered together. \n",
    "This may not be desirable. \n",
    "On the other hand, if correlation-based distance is used, then shoppers with similar preferences \n",
    "(e.g. shoppers who have bought items A and B but never items C or D) will be clustered together, \n",
    "even if some shoppers with these preferences are higher-volume shoppers than others. \n",
    "Therefore, for this application, correlation-based distance may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:#1576b2; font-size:16px; font-weight:700\"> Standardizing Data</span>\n",
    "\n",
    "\n",
    "In addition to carefully selecting the dissimilarity measure used, \n",
    "one must also consider whether or not the variables should be scaled to have a standard deviation \n",
    "of one (1.0) before the dissimilarity between the observations is computed. \n",
    "If the variables are scaled to have standard deviation of one before the inter-observation dissimilarities are computed, \n",
    "then each variable will in effect be given equal importance in the hierarchical clustering performed.\n",
    "\n",
    "We might also want to scale the variables to have standard deviation of one if they are measured on different scales; \n",
    "otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular \n",
    "variable will greatly affect the dissimilarity measure obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Things to consider when performing Clustering\n",
    "\n",
    "1. Should the observations or features first be standardized?\n",
    "2. In the case of hierarchical clustering:\n",
    "    * What dissimilarity measure should be used?\n",
    "    * What type of linkage should be used?\n",
    "    * Where should we cut the dendrogram in order to obtain clusters?\n",
    "3. In the case of K-means clustering, how many clusters should we look for in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EXAMPLE:**\n",
    "\n",
    "\n",
    "First, we will generate some random data set to perform hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed to reproduce the results\n",
    "set.seed(4)\n",
    "\n",
    "# Generate a random normal distribution of 100 values. Generate a matrix out of this normal distribution.\n",
    "x = matrix(rnorm(50 * 2), ncol = 2)\n",
    "\n",
    "# Add 3 to rows 1 to 25 in first column of the matrix\n",
    "x[1:25, 1] = x[1:25, 1] + 3\n",
    "\n",
    "# Subtract 4 from rows 1 to 25 in second column of the matrix\n",
    "x[1:25, 2] = x[1:25, 2] - 4\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "The `hclust()` function is used to implement hierarchical clustering in R. \n",
    "We are going to create matrix $x$, similar to K-Means lab, \n",
    "and use that data to plot the hierarchical clustering dendrogram using \n",
    "complete, single, and average linkage clustering with Euclidean distance as the dissimilarity measure. \n",
    "\n",
    "\n",
    "\n",
    "Let's begin by clustering observations using complete linkage. \n",
    "Use `dist()` function to compute the 50 × 50 inter-observation Euclidean distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc.complete = hclust(dist(x), method=\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform hierarchical clustering with average and single linkage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc.average = hclust(dist(x), method =\"average\")\n",
    "hc.single = hclust(dist(x), method =\"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can plot the dendrograms obtained using the `plot()` function.\n",
    "The numbers at the bottom of the plot identify each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange 3 plots in a row\n",
    "par(mfrow = c(1,3))\n",
    "\n",
    "# Plot all the 3 dendrograms generated using different linkage schemes.\n",
    "plot(hc.complete, main = \"Complete Linkage\", xlab = \"\", sub = \"\", cex = .9)\n",
    "plot(hc.average, main = \"Average Linkage\", xlab = \"\", sub = \"\", cex = .9)\n",
    "plot(hc.single, main = \"Single Linkage\", xlab = \"\", sub = \"\", cex = .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the cluster labels for each observation associated with a given number of cuts of the dendrogram, \n",
    "we can use the `cutree()` function as shown below.  Here, we are specifiying that the dendrogram has 2 cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutree(hc.complete, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutree(hc.average, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutree(hc.single, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "In the cells above, we have chosen a $k=2$ number of clusters.\n",
    "This was based on our qualitative assessment of the dendograms.\n",
    "What happens if we make a different choice?\n",
    "\n",
    "**Your Turn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k <- 6   # Keep changing me... rerun\n",
    "cutree(hc.single, k)  # Also change from complete, average, and single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "To scale the variables before performing hierarchical clustering of the observations, we use the `scale()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsc = scale(x)\n",
    "plot(hclust(dist(xsc), method = \"complete\"), main = \"Hierarchical Clustering with Scaled Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Changing distance metric\n",
    "\n",
    "Correlation-based distance can be computed using the `as.dist()` function, \n",
    "which converts an arbitrary square symmetric matrix into a form that the `hclust()` \n",
    "function recognizes as a distance matrix. \n",
    "However, this only makes sense for data with at least three features since the absolute \n",
    "correlation between any two observations with measurements on two features is always 1. \n",
    "Hence, we will cluster a three-dimensional data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = matrix(rnorm(30 * 3), ncol = 3)  # generate a 3-D normal data set\n",
    "dd = as.dist(1 - cor(t(x)))\n",
    "plot(hclust(dd, method = \"complete\"), main = \"Complete Linkage with Correlation-Based Distance \", \n",
    "         xlab = \"\", sub = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<span style=\"color:#1576b2; font-size:20px; font-weight:700;\"> Divisive Clustering\n",
    "\n",
    "The most common divisive clustering technique is the DIANA (DIvisive ANAlysis Clustering) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(diana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute diana()\n",
    "# x : could be matrix or data frame, or dissimilarity matrix or object.\n",
    "# metric: character string specifying the metric to be used for calculating dissimilarities between observations.\n",
    "# The currently available options are \"euclidean\" and \"manhattan\"\n",
    "\n",
    "res_diana <- diana(x, metric = \"euclidean\", stand = FALSE)\n",
    "print(res_diana)\n",
    "# Plot the tree\n",
    "pltree(res_diana, cex = 0.6, hang = -1, main = \"Dendrogram of diana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then analyze the dendogram and make the cuts as done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut into 2 groups:\n",
    "diana2 <- cutree(as.hclust(res_diana), k = 2)\n",
    "\n",
    "table(diana2) # 8 and 42 group members\n",
    "\n",
    "rownames(x)[diana2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divise coefficient; amount of clustering structure found\n",
    "res_diana$dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
