{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Examples\n",
    "\n",
    "This notebook contains some examples of clustering to show how to look for clusters in data and how to visualize several aspects of clustering. \n",
    "\n",
    "\n",
    "We will start with the simple iris data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=10)\n",
    "\n",
    "\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It already comes with the class labels; we know the Species of each observation. Below is the original data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + geom_point() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a data frame and get only two variables; Petal.Length and Petal.Width. We are not taking the Species variable, so in this data frame, there is no labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_c = iris[,3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iris_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unsupervised setting, we don't know where the observation belong: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris_c, aes(x=Petal.Length, y=Petal.Width)) + geom_point() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a clustering using K-Means. By looking at the above plot, we can tell that there are probably 2 or 3 clusters. If we know that we have to deal with three species, we can specify k as 3. Also, make sure to assign `nstart` so that kmeans can run multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "i_clust <- kmeans(iris_c, 3, nstart = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_clust # Look at the return value; it contains several structures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are the centroids of the clusters \n",
    "i_clust$centers\n",
    "\n",
    "# These are the clluster labels assigned to each observation \n",
    "i_clust$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc <- data.frame(i_clust$centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the clusters and the cluster centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ggplot() + \n",
    "\n",
    "geom_point(data=iris_c, aes(x=Petal.Length, y=Petal.Width, color=factor(i_clust$cluster))) + \n",
    "\n",
    "geom_point(data=dfc, aes(x=Petal.Length, y=Petal.Width), color=\"black\",size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute a confusion table. \n",
    "\n",
    "**IMPORTANT!** Remember that this is an unsupervised modeling example. We do not know what actual labels are. All we can do is to create clusters and randomly label them as 1, 2, 3, etc. What the clustering algorithm labels as 1, for example,  may actually correspond to a label 3, etc. So the confusion table can show gross errors. Since cluster labels are random, we can simply shuffle the labels for clusters to get the highest accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(i_clust$cluster, iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the clustering algorithm created clusters that mostly conforms with the actual groups. \n",
    "\n",
    "We can visualize clustering results in the following ways, too: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(fpc)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcluster(iris_c, i_clust$cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusplot(iris_c, i_clust$cluster, color=TRUE, shade=TRUE, labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see above is a generalized way of showing clusters; if we have more than two dimensions, these methods will produce two dimensional plots similar to PCA to show the clusters projected onto two dimensions. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's look at the US arrests data set. **Remember that scaling data is good practice;** we don't want our analysis affected by the different scales of the variables. Variables with larger units would dominate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- USArrests\n",
    "df <- na.omit(df)\n",
    "df <- scale(df)\n",
    "head(df)\n",
    "dim(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways to analyze the data is to see if there are natural groupings (clusters) in it; observations that share similar characteristics can be grouped into clusters and analyzing those clusters as sub-groups, we can get insights from the data. \n",
    "\n",
    "\n",
    "First, we should have some idea if the data is suitable for clustering. The following code creates a distance matrix. If we see blocks in the distance matrix, that shows that there are some clusters in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(factoextra)\n",
    "distance <- get_dist(df)\n",
    "fviz_dist(distance, gradient = list(low = \"blue\", mid = \"white\", high = \"red\"), order=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above distance matrix suggests that there are some clusters in the data. Small distances are blue, large distances are red. \n",
    "\n",
    "\n",
    "We can also use the following function to get the same idea; the Hopkins statistic above 0.5 suggests clusterable data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clust_tendency(data = df, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a kmeans clustering with two clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust <- kmeans(df, centers = 2, nstart = 20)\n",
    "str(kclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize the clusters:\n",
    "\n",
    "clusplot(df, kclust$cluster, color=TRUE, shade=TRUE, labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use this function to visualize. \n",
    "\n",
    "fviz_cluster(kclust, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not know the optimal number of clusters; we can try a few methods to see if we  can justify two clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_nbclust(df, kmeans, method = \"wss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_nbclust(df, kmeans, method = \"silhouette\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_stat <- clusGap(df, FUN = kmeans, nstart = 20, K.max = 10, B = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_gap_stat(gap_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(NbClust)\n",
    "nb <- NbClust(df, distance = \"euclidean\", min.nc = 2,\n",
    "        max.nc = 10, method = \"complete\", index =\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by nbclust, this data seems to have two clusters. \n",
    "\n",
    "Let's apply `pamk` and see what it finds for number of clusters and clusters themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pamk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pamclust <- pamk(df, krange=1:5, critout=TRUE)\n",
    "\n",
    "pamclust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pamk` also suggests two clusters. Below are the clusters produced by `pamk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusplot(df, pamclust$pamobject$clustering, color=TRUE, shade=TRUE, labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further analyze these clusters to find out what they have in common. You can look at the univariate statistics, figure out if factor analysis groups the variables for each cluster in some meaningful way, or see if there are associations between the variables that only exist in the clusters. \n",
    "\n",
    "---\n",
    "\n",
    "Let's create another data set: we will read the red and white wine-quality data sets and combine them into one set. We will create a new variable named `type` and assign 1 for white and 2 for red wines. Then, we will try to cluster the data without looking at the `type` column and see if the unsupervised methods can find two natural clusters which conform with the red vs. white wine groups. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.r <- read.csv(\"/dsa/data/all_datasets/wine-quality/winequality-red.csv\", sep=\";\")\n",
    "\n",
    "wine.w <- read.csv(\"/dsa/data/all_datasets/wine-quality/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "# combine two data sets and add a type column: 1=white, 2=red \n",
    "\n",
    "wdf <- rbind(cbind(wine.w, type=rep(1,dim(wine.w)[1])),cbind(wine.r, type=rep(2,dim(wine.r)[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(wdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the type column\n",
    "wdf_c_ <-  wdf[, which(names(wdf) != \"type\")]\n",
    "\n",
    "wdf_c <- scale(wdf_c_) # try and see what happens if you do NOT scale.. \n",
    "head(wdf_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to find out if we can differentiate between red and white wines, we'll take k=2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wclust <- kmeans(wdf_c, centers = 2, nstart = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(wclust, data=wdf_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a confusion table and compute an accuracy. **Remember** that we discussed how these cluster numbers are random; you should try 1 vs. 2 and find which cluster number assignment produces the highest accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctable_k <- table(wclust$cluster, wdf$type)\n",
    "ctable_k\n",
    "sum(diag(ctable_k))/dim(wdf_c)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the clustering algorithm calls 1 and 2 actually correspond to 2 and 1, so if we compute the accuracy by considering that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(4804+1574)/dim(wdf_c)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that this is an unsupervised way of guessing red and white wines, it's actually pretty good. \n",
    "\n",
    "Remember that kmeans is not robust to outliers; let's try `pamk` and see what happens. \n",
    "\n",
    "**It takes a while to compute:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pamclust2 <- pamk(wdf_c, krange=1:5, critout=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pamk` also suggests two clusters although the criteria are very close to each other. Let's look at the confusion table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctable_p <- table(pamclust2$pamobject$clustering, wdf$type)\n",
    "ctable_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(diag(ctable_p))/dim(wdf_c)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods are able to separate red and white wines successfully. This suggests that the variables form natural, and most probably convex, separable clusters for red and white wines. \n",
    "\n",
    "You can use similar approach to find natural clusters in data and analyze further what they have in common. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
