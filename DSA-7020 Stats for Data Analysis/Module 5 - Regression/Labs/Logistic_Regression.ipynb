{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is also referred to as logit regression. \n",
    "It is a regression model where the **dependent variable is categorical and binary**, where it can take only two values, such as pass/fail, win/lose, healthy/sick etc. \n",
    "Dependent variables with more than two categories are referred to as multinomial logistic regression.\n",
    "\n",
    "Logistic regression is a special case of the generalized linear model and is analogous to linear regression. \n",
    "It has the following assumptions between dependent ($y$) and independent variables($x_1,x_2,..,x_n$). \n",
    "\n",
    "* The conditional distribution $y\\ |\\ x$ is a Bernoulli distribution rather than a Gaussian distribution for linear regression, because the dependent variable is binary. \n",
    "\n",
    "* The predicted values are restricted to (0,1) because logistic regression predicts the probability of particular outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "\n",
    "The predictors can be continuous or categorical or a mix of both types. \n",
    "We will work with framingham dataset in this notebook. \n",
    "In this dataset we will be predicting the 10 year coronary heart disease (CHD). \n",
    "The dependent variable is modeled as the binary variable where 1 represents the presence risk of CHD and 0 represents the absence of risk of CHD in 10 years. \n",
    "\n",
    "Logistic regression predicts the probability of the outcome variable being TRUE (1).\n",
    "    \n",
    "\n",
    "When predicting the disease, we want to know the variables which are risk factors which increase the chance of disease. \n",
    "Some of the risk factors in the dataset include demographic information like male, age, education(school-1, high school/GED -2, some college/vocational - 3, college - 4), etc. \n",
    "Behavioral factors like \"current smoker\" and \"cigsPerDay\" are also included. \n",
    "The rest of the possible predictors are all medical history risk factors. \n",
    "\n",
    "Let's build the model using the risk factors in the data. \n",
    "**We will predict whether or not a patient experienced CHD within 10 years of first examination.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Logistic_Function\"></a>\n",
    "\n",
    "`TenYearCHD` is the dependent variable ($y$). We should predict $P(y=1)$. Then $P(y=0)=1-P(y=1)$. \n",
    "\n",
    "### Logistic Function \n",
    "\n",
    "The **sigmoid function** used in logistic regression outputs the conditional probabilities of the prediction, \n",
    "the class probabilities. \n",
    "It works based on \"odds ratio\" $\\frac{p}{1 - p}$, \n",
    "which describes the ratio between the probability that a certain positive event occurs and the probability that it doesn't occur. \n",
    "Here positive refers to the \"event that we want to predict\" that is $p(y=1 | x)$. \n",
    "As in the image below, the more likely that the positive event occurs, \n",
    "the larger the odds ratio will be.\n",
    "\n",
    "<img src=\"../images/odds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take the natural log of this odds ratio, logit function, we get the following\n",
    "\n",
    "$$logit(\\ p(y=1\\ |\\ x)\\ ) = log\\bigg(\\frac{p}{1-p}\\bigg) = log(p)-log(1-p) = β_0 + β_1.{x_1} + ... + β_k.{x_k}$$\n",
    "\n",
    "We don't have to predict the right part of the equation above, \n",
    "since p(y=1|x) is what we are really interested in.\n",
    "So, taking an inverse of this logit function, we get the logistic sigmoid shown below. \n",
    "We use this function to make predictions.\n",
    "\n",
    "$$ logit^{-1}(P(y=1\\ |\\ x)) = \\frac{1}{1+e^{-(β_0 + β_1.{x_1} + β_2.{x_2} + ... + β_k.{x_k})}}$$\n",
    "\n",
    "The equation looks like a complicated nonlinear equation but it's the familiar linear regression equation $β_0 + β_1.{x_1} + β_2.{x_2} + ... + β_k.{x_k}$ in this logistic response function. \n",
    "This function is used to produce a number between 0 and 1. \n",
    "\n",
    "<img src=\"../images/logistic_response_function.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The plot above shows the logistic response function for different values of the linear regression piece. \n",
    "The logistic response function always takes values between 0 and 1 which makes sense because it's probability. \n",
    "A positive coefficient for the variables increases the linear regression piece $β_0 + β_1.{x_1} + β_2.{x_2} + ... + β_k.{x_k}$  which increases the probability that y=1 or the probability of risk of CHD. \n",
    "On the other hand a negative coefficient for the variables decreases the linear regression piece which in turn decreases the probability of y=1 or decreases the probability of risk of CHD. \n",
    "The coefficients, or betas, are selected to predict a high probability for the risk of CHD and to predict a low probability no risk of CHD.\n",
    "\n",
    "Another way to think about logistic response function is in terms of odds. The odds is the probability of 1 divided by probability of 0. Odds are greater than 1 if y=1 is more likely. Odds are less than 1 if y=0 is more likely. \n",
    "\n",
    "$$Odds = \\frac{P(y=1)}{P(y=0)}$$\n",
    "\n",
    "Odds > 1, if y=1 is more likely\n",
    "\n",
    "Odds < 1, if y=0 is more likely\n",
    "\n",
    "Odds =1 , if both outcomes are equally likely.\n",
    "\n",
    "If you substitute the logistic response function for the probabilities in the odds equation you will see that the odds are equal to e raised to the power of the linear regression equation, as shown below.\n",
    "\n",
    "$$Odds = e^{(β_0 + β_1.{x_1} + β_2.{x_2} + ... + β_k.{x_k})}$$ \n",
    "\n",
    "By taking the log of both sides $log (Odds) = (β_0 + β_1.{x_1} + β_2.{x_2} + ... + β_k.{x_k})$ the log (Odds) or **logit** looks exactly like the linear regression equation. This helps us understand how coefficients affect our prediction of the probability. A positive beta value increases the **Logit** which in turn increases the odds of 1. A negative beta value decreases the **Logit** which in turn decreases the odds of 1. \n",
    "\n",
    "---\n",
    "\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 16</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>male</th><th scope=col>age</th><th scope=col>education</th><th scope=col>currentSmoker</th><th scope=col>cigsPerDay</th><th scope=col>BPMeds</th><th scope=col>prevalentStroke</th><th scope=col>prevalentHyp</th><th scope=col>diabetes</th><th scope=col>totChol</th><th scope=col>sysBP</th><th scope=col>diaBP</th><th scope=col>BMI</th><th scope=col>heartRate</th><th scope=col>glucose</th><th scope=col>TenYearCHD</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>39</td><td>4</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>195</td><td>106.0</td><td> 70</td><td>26.97</td><td>80</td><td> 77</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0</td><td>46</td><td>2</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>250</td><td>121.0</td><td> 81</td><td>28.73</td><td>95</td><td> 76</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1</td><td>48</td><td>1</td><td>1</td><td>20</td><td>0</td><td>0</td><td>0</td><td>0</td><td>245</td><td>127.5</td><td> 80</td><td>25.34</td><td>75</td><td> 70</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0</td><td>61</td><td>3</td><td>1</td><td>30</td><td>0</td><td>0</td><td>1</td><td>0</td><td>225</td><td>150.0</td><td> 95</td><td>28.58</td><td>65</td><td>103</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>46</td><td>3</td><td>1</td><td>23</td><td>0</td><td>0</td><td>0</td><td>0</td><td>285</td><td>130.0</td><td> 84</td><td>23.10</td><td>85</td><td> 85</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0</td><td>43</td><td>2</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>228</td><td>180.0</td><td>110</td><td>30.30</td><td>77</td><td> 99</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 16\n",
       "\\begin{tabular}{r|llllllllllllllll}\n",
       "  & male & age & education & currentSmoker & cigsPerDay & BPMeds & prevalentStroke & prevalentHyp & diabetes & totChol & sysBP & diaBP & BMI & heartRate & glucose & TenYearCHD\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 39 & 4 & 0 &  0 & 0 & 0 & 0 & 0 & 195 & 106.0 &  70 & 26.97 & 80 &  77 & 0\\\\\n",
       "\t2 & 0 & 46 & 2 & 0 &  0 & 0 & 0 & 0 & 0 & 250 & 121.0 &  81 & 28.73 & 95 &  76 & 0\\\\\n",
       "\t3 & 1 & 48 & 1 & 1 & 20 & 0 & 0 & 0 & 0 & 245 & 127.5 &  80 & 25.34 & 75 &  70 & 0\\\\\n",
       "\t4 & 0 & 61 & 3 & 1 & 30 & 0 & 0 & 1 & 0 & 225 & 150.0 &  95 & 28.58 & 65 & 103 & 1\\\\\n",
       "\t5 & 0 & 46 & 3 & 1 & 23 & 0 & 0 & 0 & 0 & 285 & 130.0 &  84 & 23.10 & 85 &  85 & 0\\\\\n",
       "\t6 & 0 & 43 & 2 & 0 &  0 & 0 & 0 & 1 & 0 & 228 & 180.0 & 110 & 30.30 & 77 &  99 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 16\n",
       "\n",
       "| <!--/--> | male &lt;int&gt; | age &lt;int&gt; | education &lt;int&gt; | currentSmoker &lt;int&gt; | cigsPerDay &lt;int&gt; | BPMeds &lt;int&gt; | prevalentStroke &lt;int&gt; | prevalentHyp &lt;int&gt; | diabetes &lt;int&gt; | totChol &lt;int&gt; | sysBP &lt;dbl&gt; | diaBP &lt;dbl&gt; | BMI &lt;dbl&gt; | heartRate &lt;int&gt; | glucose &lt;int&gt; | TenYearCHD &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1 | 39 | 4 | 0 |  0 | 0 | 0 | 0 | 0 | 195 | 106.0 |  70 | 26.97 | 80 |  77 | 0 |\n",
       "| 2 | 0 | 46 | 2 | 0 |  0 | 0 | 0 | 0 | 0 | 250 | 121.0 |  81 | 28.73 | 95 |  76 | 0 |\n",
       "| 3 | 1 | 48 | 1 | 1 | 20 | 0 | 0 | 0 | 0 | 245 | 127.5 |  80 | 25.34 | 75 |  70 | 0 |\n",
       "| 4 | 0 | 61 | 3 | 1 | 30 | 0 | 0 | 1 | 0 | 225 | 150.0 |  95 | 28.58 | 65 | 103 | 1 |\n",
       "| 5 | 0 | 46 | 3 | 1 | 23 | 0 | 0 | 0 | 0 | 285 | 130.0 |  84 | 23.10 | 85 |  85 | 0 |\n",
       "| 6 | 0 | 43 | 2 | 0 |  0 | 0 | 0 | 1 | 0 | 228 | 180.0 | 110 | 30.30 | 77 |  99 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  male age education currentSmoker cigsPerDay BPMeds prevalentStroke\n",
       "1 1    39  4         0              0         0      0              \n",
       "2 0    46  2         0              0         0      0              \n",
       "3 1    48  1         1             20         0      0              \n",
       "4 0    61  3         1             30         0      0              \n",
       "5 0    46  3         1             23         0      0              \n",
       "6 0    43  2         0              0         0      0              \n",
       "  prevalentHyp diabetes totChol sysBP diaBP BMI   heartRate glucose TenYearCHD\n",
       "1 0            0        195     106.0  70   26.97 80         77     0         \n",
       "2 0            0        250     121.0  81   28.73 95         76     0         \n",
       "3 0            0        245     127.5  80   25.34 75         70     0         \n",
       "4 1            0        225     150.0  95   28.58 65        103     1         \n",
       "5 0            0        285     130.0  84   23.10 85         85     0         \n",
       "6 1            0        228     180.0 110   30.30 77         99     0         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "framingham_data = read.csv(\"/dsa/data/all_datasets/framingham/framingham.csv\")\n",
    "head(framingham_data)\n",
    "framingham_data = framingham_data[complete.cases(framingham_data),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      male             age          education    currentSmoker   \n",
       " Min.   :0.0000   Min.   :32.00   Min.   :1.00   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1.00   1st Qu.:0.0000  \n",
       " Median :0.0000   Median :49.00   Median :2.00   Median :0.0000  \n",
       " Mean   :0.4437   Mean   :49.55   Mean   :1.98   Mean   :0.4891  \n",
       " 3rd Qu.:1.0000   3rd Qu.:56.00   3rd Qu.:3.00   3rd Qu.:1.0000  \n",
       " Max.   :1.0000   Max.   :70.00   Max.   :4.00   Max.   :1.0000  \n",
       "   cigsPerDay         BPMeds        prevalentStroke     prevalentHyp   \n",
       " Min.   : 0.000   Min.   :0.00000   Min.   :0.000000   Min.   :0.0000  \n",
       " 1st Qu.: 0.000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.0000  \n",
       " Median : 0.000   Median :0.00000   Median :0.000000   Median :0.0000  \n",
       " Mean   : 9.025   Mean   :0.03034   Mean   :0.005741   Mean   :0.3116  \n",
       " 3rd Qu.:20.000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:1.0000  \n",
       " Max.   :70.000   Max.   :1.00000   Max.   :1.000000   Max.   :1.0000  \n",
       "    diabetes          totChol          sysBP           diaBP       \n",
       " Min.   :0.00000   Min.   :113.0   Min.   : 83.5   Min.   : 48.00  \n",
       " 1st Qu.:0.00000   1st Qu.:206.0   1st Qu.:117.0   1st Qu.: 75.00  \n",
       " Median :0.00000   Median :234.0   Median :128.0   Median : 82.00  \n",
       " Mean   :0.02706   Mean   :236.8   Mean   :132.4   Mean   : 82.92  \n",
       " 3rd Qu.:0.00000   3rd Qu.:263.0   3rd Qu.:143.9   3rd Qu.: 90.00  \n",
       " Max.   :1.00000   Max.   :600.0   Max.   :295.0   Max.   :142.50  \n",
       "      BMI          heartRate         glucose         TenYearCHD    \n",
       " Min.   :15.54   Min.   : 44.00   Min.   : 40.00   Min.   :0.0000  \n",
       " 1st Qu.:23.08   1st Qu.: 68.00   1st Qu.: 71.00   1st Qu.:0.0000  \n",
       " Median :25.38   Median : 75.00   Median : 78.00   Median :0.0000  \n",
       " Mean   :25.78   Mean   : 75.73   Mean   : 81.85   Mean   :0.1523  \n",
       " 3rd Qu.:28.04   3rd Qu.: 82.00   3rd Qu.: 87.00   3rd Qu.:0.0000  \n",
       " Max.   :56.80   Max.   :143.00   Max.   :394.00   Max.   :1.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(framingham_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t3658 obs. of  16 variables:\n",
      " $ male           : int  1 0 1 0 0 0 0 0 1 1 ...\n",
      " $ age            : int  39 46 48 61 46 43 63 45 52 43 ...\n",
      " $ education      : int  4 2 1 3 3 2 1 2 1 1 ...\n",
      " $ currentSmoker  : int  0 0 1 1 1 0 0 1 0 1 ...\n",
      " $ cigsPerDay     : int  0 0 20 30 23 0 0 20 0 30 ...\n",
      " $ BPMeds         : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ prevalentStroke: int  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ prevalentHyp   : int  0 0 0 1 0 1 0 0 1 1 ...\n",
      " $ diabetes       : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ totChol        : int  195 250 245 225 285 228 205 313 260 225 ...\n",
      " $ sysBP          : num  106 121 128 150 130 ...\n",
      " $ diaBP          : num  70 81 80 95 84 110 71 71 89 107 ...\n",
      " $ BMI            : num  27 28.7 25.3 28.6 23.1 ...\n",
      " $ heartRate      : int  80 95 75 65 85 77 60 79 76 93 ...\n",
      " $ glucose        : int  77 76 70 103 85 99 85 78 79 88 ...\n",
      " $ TenYearCHD     : int  0 0 0 1 0 0 1 0 0 0 ...\n"
     ]
    }
   ],
   "source": [
    "str(framingham_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use all predictors to determine if the patient developed CHD in next 10 years. \n",
    "\n",
    "By splitting the data into training and testing sets, we will evaluate the predictive capability of our model on the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TenYearCHD, is the dependent variable and is equal to 1 if the person has risk of CHD and equal to 0 if the person had no risk of CHD. Let's see how many people had risk of CHD and how many didn't by using the table function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1 \n",
       "3101  557 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(framingham_data$TenYearCHD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.847731000546747"
      ],
      "text/latex": [
       "0.847731000546747"
      ],
      "text/markdown": [
       "0.847731000546747"
      ],
      "text/plain": [
       "[1] 0.847731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "3101/3658"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 557 out of the 3658 patients in the data set had risk of CHD, or 1, and 3101 patients had no risk of CHD, or those labeled with 0. \n",
    "\n",
    "Before building a predictive model, let's consider using a simple baseline method. In [linear_and_multiple_regression lab](Linear_and_Multiple_Regression.ipynb#R-squared), we computed the R-squared for linear regression, we compared our predictions to the baseline method of predicting the average distance to stop for all data points. \n",
    "\n",
    "In a classification problem, a standard baseline method is to just predict the most frequent outcome for all observations. Since no risk is more common than risk of CHD, in this case, we would predict that all of them have not shown any risk of CHD. \n",
    "\n",
    "**If we did this, we would get 3101 out of the 3658 observations correct, or have an accuracy of about 85%. So our baseline model has an accuracy of 85%. This is what we'll try to beat with our logistic regression model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets \n",
    "\n",
    "Let's split the data set randomly into a training set and testing set so that we'll have a test set to measure our out-of-sample accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1000) # set.seed() will help us to reproduce the results.\n",
    "split = sample.split(framingham_data$TenYearCHD, SplitRatio=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample.split` randomly splits the dataset based on the outcome variable while keeping the SplitRatio in mind.\n",
    "**It also makes sure that the outcome variable is well-balanced in each piece.** \n",
    "\n",
    "We have seen that about 85% of people don't have risk of CHD. \n",
    "This function makes sure that in the training set, 85% of people don't have risk of CHD and in the testing set 85% of people don't have risk of CHD. So, the test and training sets are representatives of the data.\n",
    "\n",
    "The `SplitRatio` tells how much percentage of data we want in the training set. \n",
    "When there are more data, we can afford to put less data in the training set and more in testing. \n",
    "This will increase our confidence on the model. \n",
    "Normally, you would want to put 50% to 80% of the data in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** [set.seed()](http://rfunction.com/archives/62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data using subset(). split==TRUE will give 70% of data as we mentioned above using SplitRatio parameter. \n",
    "train_data  = subset(framingham_data, split==TRUE)\n",
    "\n",
    "# Test data will have the remaining 30% of data\n",
    "test_data  = subset(framingham_data, split==FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is called train_data and we use the subset function to take a subset of `framingham_data` and only taking the observations for which spit is equal to TRUE. \n",
    "Our testing set is called test_data and, again, we use the subset function to take the observations of framingham_data, but this time those for which split is equal to FALSE.\n",
    "\n",
    "We can build a logistic regression model using all the independent variables present in the dataset. \n",
    "We use the `glm` function for \"generalized linear model\" to build our logistic regression model. \n",
    "We start with the dependent variable, and then the tilde sign, and then the independent variables. \n",
    "Since we want to use all independent variables, \n",
    "they can be included using a dot(.) as shown in below code cell. \n",
    "We then give the name of the data set we want to use to build the model, in this case, train_data. \n",
    "For a logistic regression model, we need one last argument, which is `family=binomial`. \n",
    "This tells the glm function to build a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "framingham_log = glm(TenYearCHD ~., \n",
    "                     data=train_data,family=binomial) \n",
    "                # family=binomial tell glm() to build  \n",
    "                # a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model using the summary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = TenYearCHD ~ ., family = binomial, data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-1.9465  -0.6019  -0.4168  -0.2723   2.8342  \n",
       "\n",
       "Coefficients:\n",
       "                 Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)     -8.147517   0.856122  -9.517  < 2e-16 ***\n",
       "male             0.562997   0.131368   4.286 1.82e-05 ***\n",
       "age              0.066380   0.007983   8.315  < 2e-16 ***\n",
       "education       -0.130789   0.060676  -2.156  0.03112 *  \n",
       "currentSmoker    0.031966   0.188375   0.170  0.86525    \n",
       "cigsPerDay       0.019760   0.007455   2.650  0.00804 ** \n",
       "BPMeds           0.146584   0.283906   0.516  0.60564    \n",
       "prevalentStroke  0.633471   0.527053   1.202  0.22940    \n",
       "prevalentHyp     0.254990   0.166855   1.528  0.12646    \n",
       "diabetes         0.138585   0.368311   0.376  0.70671    \n",
       "totChol          0.003480   0.001325   2.626  0.00864 ** \n",
       "sysBP            0.012884   0.004570   2.819  0.00482 ** \n",
       "diaBP           -0.003368   0.007699  -0.437  0.66176    \n",
       "BMI             -0.001536   0.015467  -0.099  0.92089    \n",
       "heartRate       -0.003204   0.005094  -0.629  0.52945    \n",
       "glucose          0.007366   0.002807   2.624  0.00868 ** \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2185.3  on 2560  degrees of freedom\n",
       "Residual deviance: 1914.3  on 2545  degrees of freedom\n",
       "AIC: 1946.3\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(framingham_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(family) # Run this cell if you want to learn more about family parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**[glm()](http://www.statmethods.net/advstats/glm.html)\n",
    "\n",
    "The output looks similar to that of a linear regression model.\n",
    "What we want to focus on is the coefficients table. \n",
    "This gives the estimate values for the coefficients, or the betas, for our logistic regression model. \n",
    "\n",
    "\n",
    "According to the p-value; variables age, male, sysBP, cigsPerDay, glucose, and totChol are all significant variables in the model. \n",
    "\n",
    "All these variables have a positive coefficient which means higher values in these variables contribute higher risk in tenYearCHD. Some of the variables have negative coefficients. \n",
    "One more important thing to look at in the output is the AIC value. \n",
    "This is a measure of the quality of the model like Adjusted R-squared which accounts for the number of variables used compared to the number of observations.\n",
    "\n",
    "Unfortunately, it can only be compared between models on the same data set. \n",
    "But it provides a means for model selection. \n",
    "**The preferred model is the one with the minimum AIC.** \n",
    "\n",
    "\n",
    "Now, let's make predictions in the training set. \n",
    "We'll call them predictTest and use the `predict` function to make predictions using the model framingham_log, \n",
    "and we'll give a second argument, which is type=\"response\". \n",
    "This tells the predict function to give us probabilities as opposed to class labels (0 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "predictTest = predict(framingham_log, type=\"response\", newdata=test_data) # Type='response' gives us probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the statistical summary of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(predictTest)\n",
    "# Let's do a summary on our predictions. Since the predict function has generated probabilities as output you should see values are \n",
    "# lying between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion Matrix \n",
    "\n",
    "The outcome of logistic regression function is a probability. \n",
    "We want to make a binary prediction if the subject had risk of CHD or not. We can do this using a threshold value t. \n",
    "\n",
    " - If P (risk of CHD = 1) >= t, predict risk of CHD\n",
    "\n",
    " - If P (risk of CHD = 1) < t, predict no risk of CHD\n",
    "\n",
    "So, the question is what value should be picked for `t`. \n",
    "In this example **if t is large**, it rarely predicts risk of CHD but there is actually a risk of CHD. \n",
    "People who have the risk of CHD can go undetected; **this is called a FALSE NEGATIVE** (falsely predicting that people will not have the disease).\n",
    "\n",
    "\n",
    "On the other hand, **if t is small**, more subjects are identified as risk prone of CHD, which may create more **FALSE POSITIVES** (falsely predicting that people will have the disease).\n",
    "\n",
    "\n",
    "There is a tradeoff between false positives and false negatives. You may prefer to have more false positives in order to not to miss risky patients so that false negatives are small.  When you have a preference over the error, t value will have to reflect that. \n",
    "\n",
    "For no preference between errors, we can select `t=0.5`; it predicts the more likely outcome.  \n",
    "\n",
    "We can decide on a threshold value t using **a confusion matrix**. \n",
    "\n",
    "**In a confusion matrix, rows are labelled with actual outcome and columns are labelled with predicted outcome.**\n",
    "\n",
    "\n",
    " - **True Negatives(TN)** are the observations which we predicted as no risk of CHD and there is actually no risk of CHD.\n",
    " - **True Positives(TP)** are the observations which we predicted as risk of CHD and there is actually risk of CHD.\n",
    " - **False Positives(FP)** are the observations which we predicted as risk of CHD but there is no risk of CHD.\n",
    " - **False Negatives(FN)** are the observations which we predicted as no risk of CHD but there is a risk of CHD. \n",
    "\n",
    "```\n",
    "|        |  Predicted=0      | Predicted=1       |\n",
    "|--------|-------------------|-------------------|\n",
    "|Actual=0|True Negatives(TN) |False Positives(FP)|\n",
    "|Actual=1|False Negatives(FN)|True Positives(TP) |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute two outcome measures that help us determine what type of errors we are making. \n",
    "They are called [**Sensitivity** and **Specificity**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity). \n",
    "\n",
    "$Sensitivity = \\frac{TP}{TP+FN}$, \n",
    "this is often called the true positive rate, or also as **Recall**.\n",
    "\n",
    "$Specificity = \\frac{TN}{TN+FP}$, \n",
    "this is called the true negative rate. \n",
    "\n",
    "Positive predictive value $PPV = \\frac{TP}{TP+FP}$, is also known as **Precision**. \n",
    "\n",
    "F-measure, also known as F1 score, combines recall and precision into one measure:\n",
    "$F = 2\\times \\frac{precision\\times recall}{precision + recall}$\n",
    "\n",
    "A model with a higher threshold will have a lower sensitivity and a higher specificity. \n",
    "A model with a lower threshold will have a higher sensitivity and a lower specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a confusion matrix using a threshold of 0.5\n",
    "table(test_data$TenYearCHD, predictTest>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity = TP/(TP+FN) = 16/(16+151) = 0.095\n",
    "\n",
    "Specificity = TN/(TN+FP) = 926/(926+4) = 99.56\n",
    "\n",
    "With a threshold of 0.5 we predict an outcome of 1 in the true column of the above output very rarely. This means our model rarely predicts a Ten Year CHD risk above 50%. Decrease the threshold value to 0.3 and build the confusion matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a confusion matrix using a threshold of 0.3\n",
    "table(test_data$TenYearCHD, predictTest>0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity = 43/(124+43) = 0.257\n",
    "\n",
    "Specificity = 840/(840+90) = 0.90\n",
    "\n",
    "As you can see from the above results, as you decrease the threshold value, sensitivity will increase and specificity will decrease. It works opposite way if you increase the threshold value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the **accuracy of the model** with a threshold value of 0.5. It's the sum of the cases we predicted correctly divided by all the observations in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(test_data$TenYearCHD, predictTest>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TP+TN) / number of observations in the dataset.\n",
    "(16+926)/(926+4+151+16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The accuracy of our model is about 86%. We need to compare this with a simple baseline model. The more frequent outcome in this method is zero so it will always predicts a zero or no CHD. The baseline model will have the accuracy  calculated below... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerator will have outcomes which are predicted as 0 divided by total number of observations.\n",
    "(926+4)/(926+4+151+16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the baseline model has an accuracy of 85%. Our model barely beats the baseline model, but do we have a valuable model? We still have to compute **an AUC value**.\n",
    "\n",
    "---\n",
    "\n",
    "### ROC Curves\n",
    "\n",
    "**[Receiver Operating Characteristic (ROC) curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)** are a popular way to visualize the tradeoffs between sensitivitiy and specificity in a binary classifier. It is a plot of true positive rate (specificity) vs. false positive rate (1-sensitivity). \n",
    "Computing the area under this curve is one way to summarize the performance of the classifier in a single value; this metric is so common that if data scientists say \"area under the curve\" or \"AUC\", you can assume that an ROC curve is meant. \n",
    "\n",
    "\n",
    "\n",
    "Probably the most straightforward and intuitive metric for classifier performance is accuracy as computed above. Unfortunately, there are circumstances where simple accuracy does not work well. For example, with a disease that only affects 1 in a million people, a completely bogus screening test that always reports \"negative\" would be 99.9999% accurate. **Unlike accuracy, ROC curves are insensitive to class imbalance; the bogus screening test would have an AUC of 0.5**, which is like not having a test at all.\n",
    "\n",
    "<img src=\"../images/Roc_curve.png\">\n",
    "\n",
    "Let's compute the AUC for our classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ROCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict function of ROCR package to make the predictions\n",
    "ROCR_predictions = prediction(predictTest, test_data$TenYearCHD)\n",
    "\n",
    "perf <- performance(ROCR_predictions,\"tpr\",\"fpr\")\n",
    "\n",
    "plot(perf,colorize=TRUE)\n",
    "abline(0,1)\n",
    "as.numeric(performance(ROCR_predictions,\"auc\")@y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an AUC of about 72% on the test set, which means the model can differentiate between low risk and high risk patients somewhat well.\n",
    "\n",
    "Our model rarely predicted 10 year CHD risk above 50%. So, the accuracy of the model is barely over the baseline model. \n",
    "However, the model was actually able to differentiate between low risk patients and high risk patients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
