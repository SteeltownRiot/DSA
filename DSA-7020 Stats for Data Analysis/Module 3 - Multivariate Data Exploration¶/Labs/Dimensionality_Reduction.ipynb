{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction \n",
    "\n",
    "\n",
    "Dimensionality reduction helps with reducing the width of the data set (the number columns/variables). \n",
    "Reducing the data set width comes in two flavors:\n",
    "\n",
    "  - Feature Selection - Selecting from existing features\n",
    "  \n",
    "  - Dimensionality Reduction - Using numerical methods to transform the feature space from known variables to computed variables\n",
    "\n",
    "\n",
    "In this notebook, we will have a brief overview of feature selection methods, then we will focus on the dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "If you are dealing with multivariate data, the data usually contains many variables. Not all features are equally significant. You will be able to make better predictions using the minimum possible number of features from the dataset.  When the dataset is huge, computation time matters a great deal. Building models with a minimum number of features helps to reduce the computational effort required. \n",
    "\n",
    "Feature selection acts like a filter, eliminating features that aren’t useful. It helps in building predictive models that are free from correlated variables, biases, and unwanted noise.  You might be interested in knowing which features of your data provide the most information about the target variable of interest. \n",
    "\n",
    "For example, suppose we’d like to predict iris species using the variables contained in R's iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the above four features provides the “purest” segmentation with respect to the target? Or put differently, if you were to place a bet on the correct species and could only ask for the value of one feature, which feature would give you the greatest likelihood of winning your bet?\n",
    "\n",
    "### Filter Methods: \n",
    "\n",
    "These methods apply a statistical measure and assign a score to each feature. The features are selected to be kept or removed from the dataset. The methods are often univariate or with regard to the dependent variable. Some of the  methods that fall into this category include the Chi squared test, information gain, and correlation coefficient scores. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chi-squared Test:**\n",
    "\n",
    "The chi square / goodness of fit test will check whether significant differences occur within a single category in a categorical variable. We can know the distribution of a variable; if values are equally distributed among different categories then the variable is not providing any new information.\n",
    "    \n",
    "Let's see how it works on iris data. \n",
    "\n",
    "syntax: chisq.test(x, p)\n",
    "\n",
    "- x: a numeric vector\n",
    "- p: a vector of probabilities of the same length of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tChi-squared test for given probabilities\n",
       "\n",
       "data:  observed\n",
       "X-squared = 1.4843e-28, df = 2, p-value = 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observed = c(50, 50, 50)        # observed frequencies\n",
    "expected = c(0.333333333333333,0.333333333333333,0.333333333333333)      # expected proportions\n",
    "\n",
    "chisq.test(x = observed, p = expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of the test is 1, which is greater than the significance level alpha = 0.05. We can conclude that the observed proportions are not significantly different from the expected proportions.\n",
    "\n",
    "\n",
    "**We can use this test to see if two factors are statistically independent or not.** We can find out if two categorical variables provide meaningful information separately or if they are correlated. Null hypothesis in that case would be \"two categorical variables are independent\" and alternate hypothesis would be \"two categorical variables are correlated\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy:**\n",
    "\n",
    "**A helpful video explaining entropy: https://www.youtube.com/watch?v=YtebGVx-Fxw**\n",
    "\n",
    "Entropy represents the amount of \"surprise\", \"randomness\", \"uncertainty\" in the data; it is a measure of *new* information. In a basket of eggs, every time you pick something without looking, it will be an egg; so there is no surprise; entropy is zero. \n",
    "\"Sun sets every evening\" also does not contain any new information or \"surprise\", so the level of information it contains is zero. \n",
    "\n",
    "For a data set, entropy is always a calculation on a vector of categorical variable values.  It is a summation across each of the possible values the vector can take on.\n",
    "\n",
    "\n",
    "$$H = -\\sum_{i=1}^{n} p_i\\log_2 p_i$$\n",
    "\n",
    "\n",
    "**So to calculate entropy, first multiply the probability of _each value_ within the vector by the quantity (log2 multiplied by that probability).  Then sum those calculations together and multiply by -1.**\n",
    "\n",
    "\n",
    "For example in the iris dataset, we have 3 possible values for Species (Setosa, Versicolor, Virginica), each representing $\\frac{1}{3}$ of the data. (The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.) Therefore\n",
    "\n",
    "\n",
    "$$-\\Bigg(\\frac{1}{3} \\log_2 (\\frac{1}{3}) + \\frac{1}{3} \\log_2 (\\frac{1}{3}) + \\frac{1}{3} \\log_2 (\\frac{1}{3})\\Bigg) = 1.59$$\n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"float:left;width:600px\" id=\"container\">\n",
    "    <div id=\"leftContainer\" style=\"float:left;width:500px;\">\n",
    "        <p><b>Example:</b> What is the entropy of a group in which all examples belong to the same class?</p>\n",
    "    </div>\n",
    "    <div id=\"rightContainer\" style=\"float:right;width:100px;\">\n",
    "        <img src=\"../images/minimum_entropy.PNG\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy = H = \n",
    "   $$- (1\\ *\\ log_2(1)) = 0$$\n",
    "\n",
    "The entropy is 0. This particular feature (class, color, etc.) makes no distinction between the observations, so not very useful for machine learning purposes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:600px\" id=\"container\">\n",
    "    <div id=\"leftContainer\" style=\"float:left;width:500px;\">\n",
    "        <p><b> Example:</b> What is the entropy of a group with 50% in either class?</p>\n",
    "    </div>\n",
    "    <div id=\"rightContainer\" style=\"float:right;width:100px;\">\n",
    "        <img src=\"../images/maximum_entropy.PNG\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy = H = \n",
    "    \n",
    "$$-(0.5\\ *\\ log_2(0.5)\\ +\\ 0.5\\ *\\ log_2(0.5)) = 1$$\n",
    "    \n",
    "This is a useful feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<b>Information Gain: </b>\n",
    "\n",
    "<span style=\"color:#4286f4; font-weight: bold\">Information Gain</span> = <span style=\"color:#e57f2b; font-weight: bold\">Parent Entropy – Weighted Average Entropy of Children</span>\n",
    "\n",
    "\n",
    "Information gain helps in making two important decisions when building decision trees on data. What is the best split(s) and which is the best variable to split a node.\n",
    "\n",
    "Along a similar line, we want to determine which attribute in a given set of training feature vectors is most useful for discriminating between the classes to be learned.\n",
    "\n",
    "    - Information gain tells us the importance of a given attribute of the feature vectors.\n",
    "    - We will use it to decide the ordering of attributes.\n",
    "    \n",
    "Consider following data with 30 elements of which 16 elements are green circles and remaining 14 are pink crosses. \n",
    "\n",
    "<img src=\"../images/circles_and_crosses.PNG\">\n",
    "\n",
    "\n",
    "Subset 1 Child entropy =  $-\\Bigg(\\frac{13}{17} \\log_2 (\\frac{13}{17}) + \\frac{4}{17} \\log_2 (\\frac{4}{17})\\Bigg) = 0.787$\n",
    "\n",
    "Subset 2 Child entropy =  $-\\Bigg(\\frac{1}{13} \\log_2 (\\frac{1}{13}) + \\frac{12}{13} \\log_2 (\\frac{12}{13})\\Bigg) = 0.391$\n",
    "\n",
    "Parent entropy =  $-\\Bigg(\\frac{16}{30} \\log_2 (\\frac{16}{30}) + \\frac{14}{30} \\log_2 (\\frac{14}{30})\\Bigg) = 0.996$\n",
    "\n",
    "Weighted Average Entropy of Children = $\\Bigg(\\frac{17}{30} * 0.787 \\Bigg) + \\Bigg(\\frac{13}{30} * 0.391 \\Bigg) = 0.615$\n",
    "\n",
    "    \n",
    "    Information Gain for this split = 0.996 - 0.615 = 0.38 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods: \n",
    "\n",
    "Wrapper methods use a subset of features and train a model using them. Based on the results drawn from the previous model, features are either added or removed from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "**Forward Selection:** Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model until the addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "**Backward Elimination:** In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "**Recursive Feature elimination:** It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the features left until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "Read more about Recursive Feature Elimination implementation in the caret package. \n",
    "\n",
    "[Feature selection using Caret package](https://www.r-bloggers.com/feature-selection-with-carets-genetic-algorithm-option/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "\n",
    "Let's continue the discussion with the communities and crime dataset. \n",
    "The data is socio-economic data with a total of 1994 instances and 128 features. \n",
    "Out of the 128 variables, 122 are predictive, 5 are non-predictive and one variable is a target variable. \n",
    "The first five variables are non predictive so we don't have to consider them when building the model.\n",
    "\n",
    "The dataset has missing values. \n",
    "The per-capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States-- namely murder, rape, robbery, and assault. \n",
    "There was apparently some controversy in some states concerning the counting of rapes. \n",
    "These resulted in missing values for rape, which resulted in incorrect values for per capita violent crime. \n",
    "These cities are not included in the dataset. \n",
    "\n",
    "Missing values should be treated before building any models. \n",
    "All numeric data is normalized into the decimal range 0.00-1.00 using an unsupervised, equal-interval binning method. \n",
    "Read the description about the dataset by opening a terminal and running this command:\n",
    "\n",
    "```Bash\n",
    "less /dsa/data/all_datasets/crime/readme.txt\n",
    "```\n",
    "\n",
    "The actual data doesn't have any column headers. \n",
    "You need to grab the headers information from the readme file. \n",
    "We have to do a little bit of data carpentry before we can start using the data to apply linear regression on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers information is present in readme file. \n",
    "Keep this information in a separate file called names.txt so we can access only the part of data we are interetsed in. The headers data in names.txt has so much unwanted information.\n",
    "A sample record is shown below\n",
    "\n",
    "    '-- state: US state (by number) - not counted as predictive above, but if considered, should be consided nominal (nominal)'\n",
    "\n",
    "The only thing we are interested in is the first word in every line, which is the actual column name. \n",
    "So read the data separating every word using the parameter sep=\"\". \n",
    "Header will be FALSE, because we don't have the header in the actual data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = read.csv('/dsa/data/all_datasets/crime/names.txt', header = FALSE, sep=\"\")\n",
    "head(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute names are extracted but they still need some cleaning. Every attribute name has a ':' appended at the end. Get rid of the ':' from every word using gsub() function. It will replace characters in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute names are in 2nd column. Extract them. \n",
    "column_names = column_names[,2]\n",
    "\n",
    "# The first argument to gsub() ':' is replaced with second argument ''(nothing here) from every string in names.\n",
    "column_names = gsub(':','', column_names)\n",
    "head(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set to assign these names to crime dataset.\n",
    "\n",
    "**Note** Error expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines of code below and run it.\n",
    "crime_data <- read.csv('/dsa/data/all_datasets/crime/communities_and_crime.txt',header=FALSE)\n",
    "names(crime_data)=column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The error** is saying something about the lengths of vector column_names and names() attribute. Check the lengths of column_names vector and number of columns in crime_data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol(crime_data)\n",
    "length(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 132 names in column_names vector which we are trying to assign to 128 columns/variables in crime_data dataframe. Some how we ended up extracting 132 names instead of 128. If we observe the names vector closely we can see what are those extra names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"-\", \"and\", \"(numeric\", \"Part\" are the four names that were created that are not actual column names. Once we eliminate these we should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below command, we are using the negation operator ('!') to select strings in the names vector which \n",
    "# are not in the specified list \n",
    "column_names = column_names [! column_names %in% c('-', 'and', '(numeric', 'Part')]\n",
    "length(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have names for our columns in actual crime_data, let's assign them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(crime_data) = column_names\n",
    "head(crime_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test sets\n",
    "\n",
    "(**How will you check the accuracy or how good is the fit of your model?**)\n",
    "\n",
    "You cannot build and test the model on the same data. You have to test the accuracy of the model on unknown test data. R has libraries to split the data into train and test datasets. \n",
    "\n",
    "Split the dataset into training and testing datasets. We can do this using the caTools package, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set.seed() is used return a reproducible sample. It helps to split the data in the same, equal partitions \n",
    "# no matter how many times it's split.\n",
    "set.seed(144)\n",
    "\n",
    "library(devtools)\n",
    "library(caTools)\n",
    "\n",
    "split = sample.split(crime_data$ViolentCrimesPerPop, SplitRatio = 0.7)\n",
    "\n",
    "crime_train_data = subset(crime_data, split == TRUE)\n",
    "\n",
    "crime_test_data = subset(crime_data, split == FALSE)\n",
    "\n",
    "nrow(crime_train_data)\n",
    "nrow(crime_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(crime_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality reduction is not the same as feature selection.** Even though both try to reduce the number of attributes in the dataset, dimensionality reduction methods create new combinations of attributes, whereas the feature selection method includes and excludes attributes present in the data without altering them. Principal Component Analysis, Singular Value Decomposition, Factor Analysis and Sammon’s Mapping, etc. are all examples of dimensionality reduction.\n",
    "\n",
    "Here are some of the simplest of techniques for dimensionality reduction/variable exclusion...\n",
    "\n",
    "**Missing Values Ratio:** Columns with many missing values carry less useful information. Thus, if the number of missing values in a column is greater than a threshold value it can be removed.\n",
    "\n",
    "**Low Variance Filter:** Columns with little variance in data carry little information. Thus, if the number of values in a column is less than a threshold value it can be removed. Variance is range dependent. Therefore, data should be normalized before applying this technique.\n",
    "\n",
    "**High Correlation Filter:** Columns with high correletion provide almost the same information. One of them is enough to feed data to the model. Correlation is scale sensitive. So, column normalization should be done for a meaningful correlation comparison.\n",
    "\n",
    "**Random Forests / Ensemble Trees:**. Decision Tree Ensembles or random forests are useful for feature selection as well as data classification. Trees are constructed with attributes as nodes. If an attribute is selected as best split, it is likely to be the most informative feature of dataset.\n",
    "\n",
    "**Principal Component Analysis (PCA):**. Principal Component Analysis (PCA) is a statistical technique takes n features of the dataset to transform into a new set of n coordinates called principal components. The transformation helps the first principal component to explain the largest possible variance. The components following have the next highest possible variance without any correletion with other components.\n",
    "[Additional Reading](https://www.r-bloggers.com/principal-component-analysis-using-r/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(crime_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(crime_train_data$LemasSwFTFieldPerPop == '?')\n",
    "table(is.na(crime_train_data$LemasSwFTFieldPerPop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variables who have missing values filled with `?`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(crime_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "\n",
    "##### Centering and Standardizing Variables\n",
    "\n",
    "Principal Component Analysis in R: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/#biplot\n",
    "\n",
    "\n",
    "Standardizing the variables is very important if we have to perform principal component analysis on the variables. \n",
    "If the variables are not standardized, then variables with large variances dominate other variables.\n",
    "\n",
    "When the variables are standardized, they will all have variance 1 and mean 0. This would allow us to find the principal components that provide the best low-dimensional representation of the variation in the original data, without being overly biased by those variables that show the most variance in the original data.\n",
    "\n",
    "We will use `scale()` function In R to standardize the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_vars <- as.data.frame(scale(crime_train_data[!sapply(crime_train_data,class) %in% c('factor')]))\n",
    "dim(standard_vars)\n",
    "head(standard_vars)\n",
    "\n",
    "# It is also possible to normalize all the varibles by specifying \"center\" and \"scale.\" arguments\n",
    "# in the prcomp() function.  Do this by specifying that \"center = TRUE\" and \"scale. = TRUE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the means and standard deviations of the variables. The means will be nearly equal to zero and all standard deviations will equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(standard_vars,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(standard_vars,sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ea8d12; font-weight:700\">Helpful video for understanding and plotting results from prcomp(): </span>https://www.youtube.com/watch?v=0Jp4gsfOLMs\n",
    "\n",
    "\n",
    "<span style=\"color:#ea8d12; font-weight:700\">prcomp() versus princomp(): </span>http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(prcomp)\n",
    "crime_train_data_pca <- prcomp(standard_vars)\n",
    "\n",
    "# Note: prcomp() expects the samples to be rows and the variables to be columns.  If that's \n",
    "# not the case, you must use the t() function to transpose the matrix.\n",
    "\n",
    "# prcomp() returns several things: (1) x, (2) sdev, and (3) rotation.  \n",
    "\n",
    "# Each principal component is a normalized linear combination of original variables.  The rotations \n",
    "# (i.e., loadings) are the coefficients of the linear combinations of the continuous variables.\n",
    "\n",
    "# print(crime_train_data_pca) will return the principal components with the coefficients for each\n",
    "# continuous variable.  In these returned values, a positive coefficient indicates that as as a \n",
    "# particular PC increases, the variable(s) with positive coefficients also increase.  Variables \n",
    "# with negative coefficients decrease, relative to the PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(crime_train_data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Principal Components to Retain\n",
    "\n",
    "A scree plot helps us to decide on number of principal components to be retained. The plot will summarize the PCA analysis results. The `screeplot()` function in R will help us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot(crime_train_data_pca, type=\"lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious change in slope in the scree plot occurs at component 7, therefore first six components should be retained.\n",
    "\n",
    "Another approach to decide on number of PCA components to choose is by using Kaiser’s criterion. It suggests that we should only retain principal components for which the variance is above 1 (on standardized variables). We can check this by finding the variance of each of the principal components. The standard deviations of PCA components are saved in a standard variable called sdev. You can access it in crime_train_data_pca dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(crime_train_data_pca$sdev)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components 1 through 14 have variance above 1. Using Kaiser’s criterion, we can retain the first fourteen principal components.\n",
    "\n",
    "One more method to decide on number of PCA components to retain is to keep as few components as required to explain at least some minimum amount of the total variance. For example, if you want to explain at least 70% of the variance, we will retain the first eight principal components, as we can see from the output of `summary(crime_train_data_pca)` that the first eight principal components explain 70% of the variance (while the first four components explain 56%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the principal components are stored in a named element `x` of the variable returned by `prcomp()`. `x` contains a matrix where the first column contains the first principal component, the second column the second component, and so on.\n",
    "\n",
    "Thus, `housing_prices_pca$x[,1]` contains the first principal component, and `housing_prices_pca$x[,2]` contains the second principal component.\n",
    "\n",
    "We will make a scatterplot of the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "pca_comp1_comp2 <- ggplot(crime_train_data, aes(x=crime_train_data_pca$x[,1],y=crime_train_data_pca$x[,2]))\n",
    "\n",
    "pca_comp1_comp2+geom_point(alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total number of elements in the dataset so that we can use this, in the following line of code,\n",
    "# to replace the principal components values with dots (to differentiate them from the variable coefficients.)\n",
    "\n",
    "len <- length(as.matrix(crime_train_data)) / length(crime_train_data)\n",
    "\n",
    "len\n",
    "\n",
    "biplot(crime_train_data_pca, xlabs = rep('.', len))\n",
    "\n",
    "# Another option: Could put \"nrow(crime_train_data)\" in place of \"len\" in the last line of code.\n",
    "\n",
    "# rep() replicates the values in x\n",
    "# xlabs is a vector of character strings used to label the first set of points.  In this case, the xlabs() function \n",
    "# is being used simply to differentiate the PC-1 values from the PC-2 values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Factor Analysis\n",
    "\n",
    "\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus “error” terms. The information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. \n",
    "\n",
    "Latent variables (as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).  Since factors are latent, we cannot use methods like regression.\n",
    "\n",
    "The key concept of factor analysis is that multiple observed variables have similar patterns of responses because they are all associated with a latent (i.e. not directly measured) variable. For example, people may respond similarly to questions about income, education, and occupation, which are all associated with the latent variable \"socioeconomic status\". It is possible that variations in n observed variables reflect the variations in just two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables and aims to find independent latent variables.\n",
    "\n",
    "Factor Analysis is a method for analyzing the covariation among the observed variables to address following questions:\n",
    "\n",
    "* How many latent factors are needed to account for most of the variation among the observed variables?\n",
    "* Which variables appear to define each factor; hence what labels should we give to these factors?\n",
    "\n",
    "Factors are listed according to factor loadings, or how much variation in the data they can explain. \n",
    "\n",
    "So, we want to investigate if observable variables (e.g., X1,X2…XN) are linearly related to a small number of unobservable (latent) factors (e.g., F1,F2…FK), with K<<N.\n",
    "\n",
    "See this link for more information: http://www.di.fc.ul.pt/~jpn/r/factoranalysis/factoranalysis.html\n",
    "\n",
    "There are the following assumptions:\n",
    "\n",
    "1. The error terms are independent from each other  \n",
    "2. The unobservable factors are independent from each other\n",
    "\n",
    "The second assumption is stating that these latent variables do not influence one another, which might be too strong a condition. There are more advanced models where this is relaxed.\n",
    "\n",
    "With the loadings it is possible to compute the covariance of any two observed variables as well as the variance of each variable.\n",
    "\n",
    "The values of the loadings are not unique (in fact, they are infinite). This means that if the algorithm finds one solution that does not reveal the hypothesized structure of the problem, it is possible to apply a ‘rotation’ to find another set of loadings that might provide a better interpretation or more consistent with prior expectations about the dataset.\n",
    "\n",
    "There are a number of rotations in the literature. For example:\n",
    "\n",
    "1. Varimax: a rotation that seeks to maximize the variance of the squared loading for each factor (ie, make them as large as possible to capture as most signal as possible)\n",
    "2. Quartimax : seeks to maximize the variance of the squared loadings for each variable, and tends to produce factors with high loadings for all variables.\n",
    "\n",
    "Rotation methods can be described as orthogonal, which do not allow the resulting factors to be correlated, and oblique, which do allow the resulting factors to be correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of factor analysis: exploratory and confirmatory.\n",
    "\n",
    "##### Exploratory factor analysis\n",
    "It is done if a researcher doesn’t have any idea about the structure of data or how many dimensions are in a set of variables. It helps identify complex interrelationships among items and group items that are part of unified concepts.\n",
    "\n",
    "##### Confirmatory Factor Analysis\n",
    "It is used for verification where the researcher has specific idea about the structure of data or how many dimensions are in a set of variables. It helps test the hypothesis that the items are associated with specific factors. Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Analysis vs. PCA\n",
    "\n",
    "\n",
    "Factor analysis is related to principal component analysis (PCA), but the two are not identical. PCA is a more basic version of exploratory factor analysis (EFA). Factor Analysis reduces the information in a model by reducing the dimensions of the observations.  This procedure has multiple purposes.  It can be used to simplify the data, for example reducing the number of variables in predictive regression models.  If factor analysis is used for these purposes, most often factors are rotated after extraction.  Factor analysis has several different rotation methods—some of them ensure that the factors are orthogonal.  Then the correlation coefficient between two factors is zero, which eliminates problems of multicollinearity in regression analysis.\n",
    "\n",
    "Both factor analysis and PCA assume that the modelling subspace is linear. (Kernel PCA is a more recent techniques that attempts dimensionality reduction in non-linear spaces.)\n",
    "\n",
    "But while Factor Analysis assumes a model (that may or may not fit the data), PCA is just a data transformation and for this reason it always exists. Furthermore while Factor Analysis aims at explaining covariances or correlations, PCA concentrates on variances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Load in the dataset...\n",
    "\n",
    "Let's perform a factor analysis on student subject preferences data. The dataset contains a hypothetical sample of 300 responses on 6 items from a survey of college students’ favorite subject matter. The items range in value from 1 to 5, which represent a scale from Strongly Dislike to Strongly Like. Our 6 items asked students to rate their liking of different college subject matter areas, including biology (BIO), geology (GEO), chemistry (CHEM), algebra (ALG), calculus (CALC), and statistics (STAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data = read.csv(\"/dsa/data/all_datasets/student_prefs/student_subject_preferences.csv\")\n",
    "head(subjects_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(subjects_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `stats` has a function factanal() that can be used to perform factor analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factanal() performs maximum-likelihood factor analysis on a covariance matrix or data matrix.\n",
    "\n",
    "# The second argument in factanal() is \"factors\" -- meaning the number of factors to be fitted.\n",
    "\n",
    "# The scores argument scores specifies the type of scores to produce, if any. \n",
    "# The default is none; the \"regression\" argument gives Thompson's scores.\n",
    "\n",
    "n.factors <- 2   \n",
    "\n",
    "fit <- factanal(subjects_data, n.factors,  scores=c(\"regression\"), rotation=\"none\") # number of factors to extract\n",
    "print(fit, digits=2, cutoff=.3, sort=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fit$scores)\n",
    "\n",
    "# For a description of Thomson's regression method, see\n",
    "# https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/factanal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit$loadings[,1:2] \n",
    "\n",
    "# Factor loading is the correlation between the observed score and the latent score. Generally, the \n",
    "# higher the better since the square of factor loading can be directly translated as item reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot factor 1 by factor 2 \n",
    "load <- fit$loadings[,1:2] \n",
    "plot(load, type = \"n\") # Set up plot. type = 'n' tells R not to plot the points. \n",
    "\n",
    "text(load, labels = names(subjects_data), cex = 0.7) # text() adds variable names.  cex() controls the font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output maximizes variance for the 1st and subsequent factors, while all are orthogonal to each other.\n",
    "\n",
    "Rotation serves to make the output more understandable, by seeking so-called “Simple Structure”. \n",
    "\n",
    "Simple structure is a pattern of loadings where items load most strongly on one factor, \n",
    "and much more weakly onto the other factors. \n",
    "\n",
    "\n",
    "In other words, varimax rotation is an orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix, which has the effect of differentiating the original variables by the extracted factor. \n",
    "Each factor will tend to have either large or small loadings of any particular variable. \n",
    "\n",
    "\n",
    "A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- factanal(subjects_data, n.factors, rotation=\"varimax\")     # 'varimax' is an ortho rotation\n",
    "\n",
    "load <- fit$loadings[,] \n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(load, type = \"n\") # Set up plot  \n",
    " \n",
    "text(load,labels = names(subjects_data), cex = .7) # add variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Analysis Interpretation \n",
    "\n",
    "Looking at both plots we see that the courses Geology, Biology, and Chemistry all have high factor loadings around 0.8 on the first factor (PA1) while Calculus, Algebra, and Statistics load highly on the second factor (PA2). \n",
    "\n",
    "Note that STAT has a much lower loading on PA2 than ALG or CALC and that it has a slight loading on factor PA1. \n",
    "This suggests that statistics is less related to the concept of Math than Algebra and Calculus. \n",
    "Just below the loadings table, we can see that each factor accounted for around 30% of the variance in responses, \n",
    "leading to a factor solution that accounted for 66% of the total variance in students’ subject matter preference.\n",
    "\n",
    "The way to interpret factors is to look at the observed variables that each factor contribute to:\n",
    "\n",
    "__Factor 1__ : contributes to Biology, Geography, and Chemistry  \n",
    "__Factor 2__ : contributed to Algebra, Calculus, and Statistics  \n",
    "\n",
    "Can we assign a conceptual label to the factors, based on the measurement variables they are contributing to?\n",
    "\n",
    "Yes!  We can associate the first factor with **Science** and the second factor with **Math**.  If these were scores on standardized tests, we could use the factor analysis to plot students into sets of ''Science Kids'' and ''Math Kids''.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
