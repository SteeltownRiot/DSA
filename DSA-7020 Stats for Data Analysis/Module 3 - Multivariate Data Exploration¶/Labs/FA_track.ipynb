{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis on Track Records\n",
    "\n",
    "Factor analysis is not just a dimensionality reduction method; it is useful to find **latent** variables which are not directly measured in a single variable, but can be inferred from the vairables in the data set. The **latent** variables are called **factors**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes on Differences between PCA and FA\n",
    "\n",
    "Remember that PCA is a technique for reducing the dimensionality of data, whereas FA is a technique for identifying and measuring variables that cannot be measured directly (latent variables or factors). \n",
    "\n",
    " - In PCA, the resulting components are a mix of what the variables intended to measure and other sources of variance such as measurement error. The variability in measured variables in PCA cause the variance in the principal components. \n",
    " \n",
    " - In FA, latent factors are causing the variability and pattern of correlations among measured variables. \n",
    "\n",
    "Most multivariate data are correlated to some degree, so differences between PCA and FA may not be very distinct. Also, as the number of variables involved in the analysis grows, results from PCA and FA become more and more similar. Researchers have argued that analyses with at least 40 variables lead to minor differences. Also, if the communality of measured variables is high, then the results between PCA and EFA are also similar.\n",
    "\n",
    " - PCA is useful for reducing the number of variables while retaining the most amount of information in the data, whereas FA is useful for measuring unobserved (latent) variables.\n",
    "\n",
    " - When variables don’t have anything in common, FA won’t find a well-defined underlying factor, but PCA may still find a well-defined principal component that explains the maximal amount of variance in the data.\n",
    "\n",
    " - When the goal is to measure a latent variable but PCA is used, the component loadings will most likely be higher than they would’ve been if FA was used. This would mislead analysts into thinking they have a well-defined factor when in fact they have a well-defined component that’s a mixture of all the sources of variance in the data.\n",
    "\n",
    " - When the goal is to get a small subset of variables that retain the most amount of variability in the data but FA is used, the factor loadings will likely be lower than they would’ve been if PCA was used. This would mislead analysts into thinking they kept the maximal amount of variance in the data when in fact they kept the variance that’s in common across the measured variables.\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will do Factor Analysis on the National Track Records for Women data set reprsenting 55 countries in seven events. \n",
    "The description of the variables is following: \n",
    "\n",
    " - X1: 100m (sec)\n",
    " - X2: 200m (sec)\n",
    " - X3: 400m (sec)\n",
    " - X4: 800m (min)\n",
    " - X5: 1500m (min)\n",
    " - X6: 3000m (min)\n",
    " - X7: Marathon (min)\n",
    " \n",
    " \n",
    " \n",
    "We will do a factor analysis to see if there are factors that are represented by the variables in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read.csv(\"women_track_records.csv\")\n",
    "head(df)\n",
    "X = df[2:8]\n",
    "head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fa <- factanal(X,factors=2, rotation = \"varimax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **uniquenesses** ranges from 0 to 1 and sometimes is referred to as noise. It corresponds to the proportion of variability, which can not be explained by a linear combination of the factors. A high uniqueness for a variable indicates that the factors do not account well for its variance.\n",
    "\n",
    "\n",
    "The **loadings** range from −1 to 1. The loadings are the contribution of each original variable to the factor. Variables with a high loading are well explained by the factor.\n",
    "\n",
    "By squaring the loading, we compute the fraction of the variable’s total variance explained by the factor. This proportion of the variability is denoted as **communality**. Another way to calculate the **communality** is to subtract the uniquenesses from 1. **An appropriate factor model results in low values for uniqueness and high values for communality.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-X.fa$uniquenesses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table beneath the loadings shows the proportion of variance explained by each factor. \n",
    "\n",
    "The row `Cumulative Var` gives the cumulative proportion of variance explained. These numbers range from 0 to 1. \n",
    "The row `Proportion Var` gives the proportion of variance explained by each factor, and the row `SS loadings` \n",
    "gives the sum of squared loadings. This is sometimes used to determine the value of a particular factor. \n",
    "**A factor is worth keeping if the SS loading is greater than 1 (Kaiser’s rule).**\n",
    "\n",
    "The last section of the function output shows the results of a hypothesis test. The null hypothesis, H0, is that the number of factors in the model, in our example 2 factors, is sufficient to capture the full dimensionality of the data set. Conventionally, we reject H0 if the p-value is less than 0.05. Such a result indicates that the number of factors is too small. In contrast, we do not reject H0 if the p-value exceeds 0.05. Such a result indicates that there are likely enough (or more than enough) factors capture the full dimensionality of the data set. \n",
    "\n",
    "The high p-value in our example above leads us to not reject the H0, and indicates that we fitted an appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=12)\n",
    "\n",
    "plot(X.fa$loadings[,1], \n",
    "     X.fa$loadings[,2],\n",
    "     xlab = \"Factor 1\", \n",
    "     ylab = \"Factor 2\", \n",
    "     ylim = c(-1,1),\n",
    "     xlim = c(-1,1),\n",
    "     main = \"Loadings\")\n",
    "\n",
    "text(X.fa$loadings[,1]+0.03, \n",
    "     X.fa$loadings[,2]+0.03,\n",
    "      colnames(df)[-1],\n",
    "      col=\"blue\")\n",
    "abline(h = 0, v = 0)\n",
    "abline(a=0, b=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Interpretation of Factors\n",
    "\n",
    "The tricky aspect in factor analysis is interpreting the factors themselves. If two variables both have large loadings for the same factor, then we know they have something in common. As a data scientist, we have to understand the data and its meaning in order to give a name to that common ground. \n",
    "\n",
    "**In our data, variables X1, X2, X3 define Factor 2 (high loadings on Factor 2). Variables X3, X4, X5, X6, X7 define Factor 1.** \n",
    "\n",
    "\n",
    "To give names for the two factors, let’s focus on the domain knowledge of the field. In the given problem, the variables X1, X2, ..., X7 were the times recorded in seven events ranging from 100m to Marathon. \n",
    "\n",
    "\n",
    "Generally, in short-distance running (e.g. 100m, 200m, 400m), athletes should mainly focus on the speed. In long-distance running (e.g. 1500m, 3000m, Marathon), the athletes should mainly focus on tolerance or endurance. \n",
    "\n",
    "In our analysis, we can conclude that Factor 2 represents short-distance track records (since X1, X2 and X3 define Factor 2) and Factor 1 represents long-distance track records (since X4, X5, X6 and X7 define Factor 2). Therefore, we can give relevant names for the two factors as following:\n",
    "\n",
    " - Factor 1: tolerance or endurance factor\n",
    " - Factor 2: speed factor\n",
    "\n",
    "But also note that the loadings for X3 are very close to each other. We can argue that the athletes who participated in 400m or 800m events should have maintained a good balance between speed and tolerance. So we can say that 400m seems to be influenced by both speed and endurance factors. Logic would suggest that 800m (X4) should have the same influence, but our analysis does not support that. It might be due to data sampling while creating the data set. \n",
    "\n",
    "These are the **actionable** insights that can be obtained by performing FA on this data set. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
