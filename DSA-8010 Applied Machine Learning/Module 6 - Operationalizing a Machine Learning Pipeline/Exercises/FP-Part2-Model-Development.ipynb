{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model Development\n",
    "\n",
    "In this part, we develop three unique pipelines for predicting backorder. We use the smart sample from Part I to fit and evaluate these pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the smart sample here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload your smart sampling from local file \n",
    "# ----------------------------------\n",
    "X, y, train_undersamp = joblib.load('data/sample-data-v1.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize/standardize the data if required; otherwise ignore. You can perform this step inside the pipeline (if required). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the dataset features\n",
      "[1.49783531e+02 7.01597165e+00 2.31098684e+01 1.49170901e+02\n",
      " 3.75961981e+01 1.11286155e+02 3.39850410e+01 2.76166805e-03\n",
      " 2.44987572e+00 7.41287858e-01 2.81487619e+00 1.85952315e-01\n",
      " 5.98361410e-04 1.36242290e-01 9.74638682e-01 1.84111203e-04]\n",
      "Variance of data\n",
      "[2.05705652e+03 5.47785352e+00 4.83967322e+02 1.59776336e+03\n",
      " 4.81613634e+02 1.40028856e+03 4.18233925e+02 5.24789599e-02\n",
      " 4.14619611e+01 2.72113454e-01 6.30854082e+01 3.89068184e-01\n",
      " 2.44541075e-02 3.43045666e-01 1.57219979e-01 1.35675092e-02]\n",
      "-----------------------------------\n",
      "# Scaled data:\n",
      "[[-0.06989771 -0.91568196 -0.04775089 ... -0.39715497  0.16131104\n",
      "  -0.01357001]\n",
      " [-0.06795318  0.90985061 -0.04775089 ...  2.51790883  0.16131104\n",
      "  -0.01357001]\n",
      " [-0.06795318  0.17963758 -0.04775089 ... -0.39715497  0.16131104\n",
      "  -0.01357001]\n",
      " ...\n",
      " [-0.07086997  0.17963758 -0.04775089 ... -0.39715497  0.16131104\n",
      "  -0.01357001]\n",
      " [-0.06843931  0.90985061 -0.04775089 ... -0.39715497  0.16131104\n",
      "  -0.01357001]\n",
      " [-0.07184223  0.36219084 -0.04775089 ... -0.39715497  0.16131104\n",
      "  -0.01357001]]\n",
      "# Mean of scaled data\n",
      "[ 0.00000000e+00  5.23275512e-18  0.00000000e+00  1.30818878e-18\n",
      " -6.54094390e-18 -2.61637756e-18  1.43900766e-17 -4.84029849e-17\n",
      " -2.61637756e-17 -3.71525613e-16  5.23275512e-18  1.30818878e-18\n",
      "  1.50441710e-17 -7.84913268e-18  4.87300320e-17  1.30818878e-18]\n",
      "# Variance of scaled data\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "# Scaled & normalized values:\n",
      "[[1.97363517e-05 1.52438006e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  6.87208133e-03 0.00000000e+00]\n",
      " [3.28939195e-05 9.14628036e-03 0.00000000e+00 ... 1.83803656e-02\n",
      "  6.87208133e-03 0.00000000e+00]\n",
      " [3.28939195e-05 6.09752024e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  6.87208133e-03 0.00000000e+00]\n",
      " ...\n",
      " [1.31575678e-05 6.09752024e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  6.87208133e-03 0.00000000e+00]\n",
      " [2.96045275e-05 9.14628036e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  6.87208133e-03 0.00000000e+00]\n",
      " [6.57878390e-06 6.85971027e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  6.87208133e-03 0.00000000e+00]]\n",
      "# All the line has unit norm\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "''' # Combine X_scaled and y\n",
    "train_under_stand = pd.DataFrame()\n",
    "train_under_stand = X_scaled\n",
    "train_under_stand.reset_index(drop = True)\n",
    "train_under_stand['went_on_backorder'] = y '''\n",
    "\n",
    "print(\"Mean of the dataset features\")\n",
    "print(scaler.mean_)\n",
    "print(\"Variance of data\")\n",
    "print(scaler.scale_)\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"# Scaled data:\")\n",
    "print(X_scaled)\n",
    "\n",
    "print(\"# Mean of scaled data\")\n",
    "print(X_scaled.mean(axis = 0))\n",
    "print(\"# Variance of scaled data\")\n",
    "print(X_scaled.std(axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scaled & normalized values:\n",
      "[[1.97363517e-05 1.52438006e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.28939195e-05 9.14628036e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.28939195e-05 6.09752024e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [1.31575678e-05 6.09752024e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 9.59456104e-03]\n",
      " [2.96045275e-05 9.14628036e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 9.59456104e-03]\n",
      " [6.57878390e-06 6.85971027e-03 0.00000000e+00 ... 6.87208133e-03\n",
      "  0.00000000e+00 9.59456104e-03]]\n",
      "# All have unit norm\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "train_undersamp_normalized = preprocessing.normalize(train_undersamp, axis = 0, norm = 'l2')\n",
    "# Split into X and y\n",
    "train_undersamp_norm = pd.DataFrame(train_undersamp_normalized)\n",
    "train_undersamp_norm.columns=train_undersamp.columns\n",
    "X_norm = train_undersamp_norm.iloc[:, :-1]\n",
    "y_norm = train_undersamp_norm.went_on_backorder\n",
    "\n",
    "print('# Scaled & normalized values:')\n",
    "print(train_undersamp_normalized)\n",
    "\n",
    "print(\"# All have unit norm\")\n",
    "print(np.linalg.norm(train_undersamp_normalized, axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>national_inv</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.699387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_time</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.039634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_transit_qty</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_3_month</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.507334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_1_month</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.622923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_3_month</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.666536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_bank</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.556251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potential_issue</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pieces_past_due</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>0.008592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_bo_qty</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck_risk</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oe_constraint</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppap_risk</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_stop</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went_on_backorder</th>\n",
       "      <td>21726.0</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>0.009595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count      mean       std       min       25%       50%  \\\n",
       "national_inv       21726.0  0.000493  0.006767 -0.026743  0.000003  0.000016   \n",
       "lead_time          21726.0  0.005348  0.004175  0.000000  0.001524  0.006098   \n",
       "in_transit_qty     21726.0  0.000324  0.006777  0.000000  0.000000  0.000000   \n",
       "forecast_3_month   21726.0  0.000631  0.006755  0.000000  0.000000  0.000017   \n",
       "sales_1_month      21726.0  0.000528  0.006764  0.000000  0.000000  0.000014   \n",
       "sales_3_month      21726.0  0.000537  0.006763  0.000000  0.000000  0.000019   \n",
       "min_bank           21726.0  0.000549  0.006762  0.000000  0.000000  0.000000   \n",
       "potential_issue    21726.0  0.000357  0.006775  0.000000  0.000000  0.000000   \n",
       "pieces_past_due    21726.0  0.000400  0.006773  0.000000  0.000000  0.000000   \n",
       "perf_6_month_avg   21726.0  0.006369  0.002338  0.000000  0.005585  0.007045   \n",
       "local_bo_qty       21726.0  0.000302  0.006778  0.000000  0.000000  0.000000   \n",
       "deck_risk          21726.0  0.002926  0.006121  0.000000  0.000000  0.000000   \n",
       "oe_constraint      21726.0  0.000166  0.006783  0.000000  0.000000  0.000000   \n",
       "ppap_risk          21726.0  0.002504  0.006305  0.000000  0.000000  0.000000   \n",
       "stop_auto_buy      21726.0  0.006698  0.001080  0.000000  0.006872  0.006872   \n",
       "rev_stop           21726.0  0.000092  0.006784  0.000000  0.000000  0.000000   \n",
       "went_on_backorder  21726.0  0.004797  0.004797  0.000000  0.000000  0.004797   \n",
       "\n",
       "                        75%       max  \n",
       "national_inv       0.000076  0.699387  \n",
       "lead_time          0.006098  0.039634  \n",
       "in_transit_qty     0.000000  0.696251  \n",
       "forecast_3_month   0.000106  0.507334  \n",
       "sales_1_month      0.000098  0.622923  \n",
       "sales_3_month      0.000101  0.666536  \n",
       "min_bank           0.000065  0.556251  \n",
       "potential_issue    0.000000  0.129099  \n",
       "pieces_past_due    0.000000  0.506367  \n",
       "perf_6_month_avg   0.008248  0.008592  \n",
       "local_bo_qty       0.000000  0.696722  \n",
       "deck_risk          0.000000  0.015733  \n",
       "oe_constraint      0.000000  0.277350  \n",
       "ppap_risk          0.000000  0.018380  \n",
       "stop_auto_buy      0.006872  0.006872  \n",
       "rev_stop           0.000000  0.500000  \n",
       "went_on_backorder  0.009595  0.009595  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_undersamp_norm.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "# Split normalized dataset into training and testing sets\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Pipeline\n",
    "\n",
    "In this section, we design an operationalized machine learning pipeline, which includes:\n",
    "\n",
    "* Anomaly detection\n",
    "* Dimensionality Reduction\n",
    "* Train a classification model\n",
    "\n",
    "\n",
    "We are free to use any of the models that we learned in the past or we can use new models. Here is a pool of methods: \n",
    "\n",
    "### Pool of Anomaly Detection Methods (Discussed in M4)\n",
    "1. IsolationForest\n",
    "2. EllipticEnvelope\n",
    "3. LocalOutlierFactor\n",
    "4. OneClassSVM\n",
    "5. SGDOneClassSVM\n",
    "\n",
    "### Pool of Feature Selection Methods (Discussed in M3)\n",
    "\n",
    "1. VarianceThreshold\n",
    "1. SelectKBest with any scoring method (e.g, chi, f_classif, mutual_info_classif)\n",
    "1. SelectKPercentile\n",
    "3. SelectFpr, SelectFdr, or  SelectFwe\n",
    "1. GenericUnivariateSelect\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Variance Threshold\n",
    "5. RFE\n",
    "7. SelectFromModel\n",
    "\n",
    "\n",
    "### Classification Methods (Discussed in M1-M2\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Logistic Regression\n",
    "4. Naive Bayes\n",
    "5. Linear SVC\n",
    "6. SVC with kernels\n",
    "7. KNeighborsClassifier\n",
    "8. GradientBoostingClassifier\n",
    "9. XGBClassifier\n",
    "10. LGBM Classifier\n",
    "\n",
    "\n",
    "\n",
    "It is difficult to fit an anomaly detection method in the sklearn pipeline without writing custom codes. For simplicity, we avoid fitting an anomaly detection method within a pipeline. So we can create the workflow in two steps. \n",
    "* Step I: fit an outlier with the training set\n",
    "* Step II: define a pipeline using a feature selection and a classification method. Then cross-validate this pipeline using the training data without outliers. \n",
    "* Note: if your smart sample is somewhat imbalanced, you might want to change the scoring method in GridSearchCV (see the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "\n",
    "Once we fit the pipeline with gridsearch, we identify the best model and give an unbiased evaluation using the test set that we created in Part II. For unbiased evaluation we report confusion matrix, precision, recall, f1-score, accuracy, and other measures if you like. \n",
    "\n",
    "**Optional: Those who are interested in writing custom codes for adding an outlier detection method into the sklearn pipeline, please follow this discussion [thread](https://stackoverflow.com/questions/52346725/can-i-add-outlier-detection-and-removal-to-scikit-learn-pipeline).**\n",
    "\n",
    "\n",
    "**Note:** <span style='background:yellow'>We will be using Grid Search to find the optimal parameters of the pipelines.</span>\n",
    "\n",
    "You can add more notebook cells or import any Python modules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix \n",
    "from numpy import where\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 1st pipeline \n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation\n",
    "  \n",
    "Add cells as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E201)\n",
    "# ----------------------------------\n",
    "# Construct local outlier factor\n",
    "lof = LocalOutlierFactor(n_neighbors = 20, contamination = .03)\n",
    "\n",
    "# Fit model \n",
    "# Get labels from classifier to cull outliers\n",
    "outliers = lof.fit_predict(X_train) == -1\n",
    "\n",
    "X_lof = X_train[~outliers]\n",
    "y_lof = y_train[~outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.70245675        nan        nan        nan        nan 0.70251606\n",
      "        nan        nan        nan        nan 0.70251606        nan\n",
      "        nan        nan        nan 0.70251606        nan        nan\n",
      "        nan        nan 0.70251606        nan        nan        nan\n",
      "        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('PCA',\n",
       "                                        PCA(n_components=5, random_state=17)),\n",
       "                                       ('LR_model', LogisticRegression())]),\n",
       "             n_jobs=2,\n",
       "             param_grid={'LR_model__C': [0.001, 0.1, 1.0, 10, 100],\n",
       "                         'LR_model__max_iter': [1000],\n",
       "                         'PCA__n_components': [5, 20, 30, 35, 60]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E202)\n",
    "# ----------------------------------\n",
    "pca = PCA(n_components = 5, random_state = 17)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('PCA', pca),\n",
    "    ('LR_model', lr)\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'PCA__n_components': [5, 20, 30, 35, 60], \n",
    "              'LR_model__C': [0.001, 0.1, 1.0, 10, 100], \n",
    "              'LR_model__max_iter': [1000]\n",
    "              }\n",
    "\n",
    "# Use Grid Search to train Pipeline\n",
    "CV_log_reg = GridSearchCV(pipe, param_grid, n_jobs = 2, cv = 10)\n",
    "CV_log_reg.fit(X_lof, y_lof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter:\n",
      " {'LR_model__C': 0.1, 'LR_model__max_iter': 1000, 'PCA__n_components': 5}\n",
      "\n",
      "Confusion Matrix:\n",
      "      0     1\n",
      "0  856  1294\n",
      "1   87  2109\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.40      0.55      2150\n",
      "           1       0.62      0.96      0.75      2196\n",
      "\n",
      "    accuracy                           0.68      4346\n",
      "   macro avg       0.76      0.68      0.65      4346\n",
      "weighted avg       0.76      0.68      0.65      4346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given an unbiased evaluation  (Question #E203)\n",
    "# ----------------------------------\n",
    "# Show parameters of trained models and their rank\n",
    "pd.set_option(\"max_colwidth\", 80)\n",
    "CV_log_reg_df = pd.DataFrame(CV_log_reg.cv_results_)\n",
    "CV_log_reg_df[['params','rank_test_score']]\n",
    "#Show parameters of best model\n",
    "best_params = CV_log_reg.best_params_\n",
    "print('Best parameter:\\n',best_params)\n",
    "\n",
    "# Evaluate best model using test data\n",
    "predicted_y = CV_log_reg.predict(X_test)\n",
    "\n",
    "# Display confusion matrix\n",
    "print('\\nConfusion Matrix:\\n',pd.DataFrame(confusion_matrix(y_test, predicted_y)))\n",
    "\n",
    "# Create classification report\n",
    "print('\\nClassification Report:\\n',classification_report(y_test, predicted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E204)\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 2nd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E205)\n",
    "# ----------------------------------\n",
    "# Construct envelope\n",
    "env = EllipticEnvelope()\n",
    "# Fit data to envelope\n",
    "envelope = env.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "# Get labels from classifier to cull outliers\n",
    "outliers = envelope.predict(X_train) == -1\n",
    "\n",
    "# Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "X_env = X_train[~outliers]\n",
    "y_env = y_train[~outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-085b3cb3f2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Use Grid Search to train Pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mCV_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mCV_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E206)\n",
    "# ----------------------------------\n",
    "# Construct random forrest classifier\n",
    "rfc = RandomForestClassifier(random_state = 17)\n",
    "kb = SelectKBest(score_func = chi2, k = 3)\n",
    "# TO DO: Look for something other than chi2\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('kbest', kb), \n",
    "    ('rfc_model', rfc)\n",
    "])\n",
    "\n",
    "n_components = [4, 8, 16]\n",
    "K = [3, 5, 10, 15, 20]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'kbest__k': K,\n",
    "        'rfc_model__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "        'rfc_model__max_depth': [None, 15, 10, 5, 1],\n",
    "        'rfc_model__n_estimators':[25, 50, 100]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Use Grid Search to train Pipeline\n",
    "CV_rfc = GridSearchCV(pipe, param_grid = param_grid, n_jobs = 2, cv = 10)\n",
    "CV_rfc.fit(X_env, y_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an unbiased evaluation  (Question #E207)\n",
    "# ----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E208)\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 3rd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E209)\n",
    "# ----------------------------------\n",
    "# Construct IsolationForest \n",
    "iso_forest = IsolationForest(n_estimators = 250,bootstrap = True).fit(X_train, y_train)\n",
    "\n",
    "# Get labels from classifier to cull outliers\n",
    "iso_outliers = iso_forest.predict(X_train) == -1\n",
    "\n",
    "X_iso = X_train[~iso_outliers]\n",
    "y_iso = y_train[~iso_outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E210)\n",
    "# ----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an unbiased evaluation  (Question #E211)\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E212)\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare these three pipelines and discuss your findings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your analysis in this cell (Question #E213)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the required pipeline/models for Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a few commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Pipelines done`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
