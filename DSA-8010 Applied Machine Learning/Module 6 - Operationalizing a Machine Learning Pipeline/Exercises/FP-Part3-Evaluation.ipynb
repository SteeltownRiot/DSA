{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, train_undersamp = joblib.load('data/sample-data-v1.pkl')\n",
    "pipe, CV_rfc, CV_rfc_model = joblib.load('data/pipeline-v3.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "model = CV_rfc_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "# Pickle the best model\n",
    "joblib.dump(model, 'data/model-v1.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (follow the steps similar to Part I)\n",
    "* If we have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset_test = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "dataset_test.head().transpose()\n",
    "\n",
    "# Remove sku feature\n",
    "dataset_test.drop('sku', axis = 1, inplace = True)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataset_test.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Find highly-correlated features to drop\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop highly-correlated features \n",
    "dataset_test.drop(to_drop, axis = 1, inplace = True)\n",
    "\n",
    "# Get all the column names of yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset_test[i].dtype!=np.float64, dataset_test.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Fill missing values in discrete features with value that occurred most often in each column\n",
    "for column_name in yes_no_columns:\n",
    "    mode = dataset_test[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset_test[column_name].fillna(mode, inplace=True)\n",
    "\n",
    "# Fill missing lead time data with mean \n",
    "dataset_test['lead_time'].fillna((dataset_test['lead_time'].mean()), inplace = True)\n",
    "\n",
    "# Fill missing perf_6_month_avg data (-99) with mode\n",
    "mode_value = dataset_test['perf_6_month_avg'].mode()\n",
    "dataset_test['perf_6_month_avg'].mask(dataset_test['perf_6_month_avg'] == -99, mode_value, inplace=True)\n",
    "\n",
    "# Remove any rows with any remaining NaN values\n",
    "dataset = dataset_test.dropna(how = 'any')\n",
    "dataset = dataset_test.reset_index(drop = True)\n",
    "\n",
    "print(dataset.isnull().sum()) # view nan counts in columms\n",
    "\n",
    "# Fill in yes, no features with 1, 0 values\n",
    "for col in ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',\n",
    "            'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n",
    "    dataset[col] = (dataset[col] == 'Yes').astype(int)\n",
    "    \n",
    "dataset.info()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "# Make prediction using test dataset\n",
    "predicted_y = CV_rfc.predict(X)\n",
    "\n",
    "# Display confusion matrix\n",
    "print('\\nConfusion Matrix:\\n',pd.DataFrame(confusion_matrix(y, predicted_y)))\n",
    "# Create classification report\n",
    "print('\\nClassification Report:\\n',classification_report(y, predicted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
