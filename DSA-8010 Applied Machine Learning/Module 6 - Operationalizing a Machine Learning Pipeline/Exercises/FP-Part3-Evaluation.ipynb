{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectPercentile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load smart sample and preprocessed full training dataset\n",
    "X_train, y_train, train_undersamp = joblib.load('data/sample-data-v4.pkl')\n",
    "\n",
    "# Load pipeline\n",
    "pipe = joblib.load('data/pipeline-v5.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8950677410785443"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "training_model = pipe.fit(X_train, y_train)\n",
    "# Display full model score\n",
    "training_model.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "       0      1\n",
      "0  9895   1398\n",
      "1   972  10321\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89     11293\n",
      "           1       0.88      0.91      0.90     11293\n",
      "\n",
      "    accuracy                           0.90     22586\n",
      "   macro avg       0.90      0.90      0.90     22586\n",
      "weighted avg       0.90      0.90      0.90     22586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make prediction using full test data\n",
    "predicted_y = training_model.predict(X_train)\n",
    "\n",
    "# Display confusion matrix\n",
    "print('Confusion Matrix:\\n',pd.DataFrame(confusion_matrix(y_train, predicted_y)))\n",
    "\n",
    "# Create classification report\n",
    "print('\\nClassification Report:\\n',classification_report(y_train, predicted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/model-v5.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "# Pickle the best model\n",
    "joblib.dump(training_model, 'data/model-v5.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (follow the steps similar to Part I)\n",
    "* If we have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n",
      "national_inv         0\n",
      "lead_time            0\n",
      "in_transit_qty       0\n",
      "forecast_3_month     0\n",
      "sales_1_month        0\n",
      "sales_3_month        0\n",
      "min_bank             0\n",
      "potential_issue      0\n",
      "pieces_past_due      0\n",
      "perf_6_month_avg     0\n",
      "local_bo_qty         0\n",
      "deck_risk            0\n",
      "oe_constraint        0\n",
      "ppap_risk            0\n",
      "stop_auto_buy        0\n",
      "rev_stop             0\n",
      "went_on_backorder    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222974 entries, 0 to 222973\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   national_inv       222974 non-null  float64\n",
      " 1   lead_time          222974 non-null  float64\n",
      " 2   in_transit_qty     222974 non-null  float64\n",
      " 3   forecast_3_month   222974 non-null  float64\n",
      " 4   sales_1_month      222974 non-null  float64\n",
      " 5   sales_3_month      222974 non-null  float64\n",
      " 6   min_bank           222974 non-null  float64\n",
      " 7   potential_issue    222974 non-null  int64  \n",
      " 8   pieces_past_due    222974 non-null  float64\n",
      " 9   perf_6_month_avg   222974 non-null  float64\n",
      " 10  local_bo_qty       222974 non-null  float64\n",
      " 11  deck_risk          222974 non-null  int64  \n",
      " 12  oe_constraint      222974 non-null  int64  \n",
      " 13  ppap_risk          222974 non-null  int64  \n",
      " 14  stop_auto_buy      222974 non-null  int64  \n",
      " 15  rev_stop           222974 non-null  int64  \n",
      " 16  went_on_backorder  222974 non-null  int64  \n",
      "dtypes: float64(10), int64(7)\n",
      "memory usage: 28.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "test_dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "# Remove sku feature\n",
    "test_dataset.drop('sku', axis = 1, inplace = True)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = test_dataset.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Find highly-correlated features to drop\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop highly-correlated features \n",
    "test_dataset.drop(to_drop, axis = 1, inplace = True)\n",
    "\n",
    "# Get all the column names of yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: test_dataset[i].dtype!=np.float64, test_dataset.columns))\n",
    "\n",
    "# Fill missing values in discrete features with value that occurred most often in each column\n",
    "for column_name in yes_no_columns:\n",
    "    mode = test_dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    test_dataset[column_name].fillna(mode, inplace=True)\n",
    "\n",
    "# Fill missing lead time data with mean \n",
    "test_dataset['lead_time'].fillna((test_dataset['lead_time'].mean()), inplace = True)\n",
    "\n",
    "# Fill missing perf_6_month_avg data (-99) with mode\n",
    "mode_value = test_dataset['perf_6_month_avg'].mode()\n",
    "test_dataset['perf_6_month_avg'].mask(test_dataset['perf_6_month_avg'] == -99, mode_value, inplace=True)\n",
    "\n",
    "# Remove any rows with any remaining NaN values\n",
    "test_dataset = test_dataset.dropna(how = 'any')\n",
    "test_dataset = test_dataset.reset_index(drop = True)\n",
    "\n",
    "print(test_dataset.isnull().sum()) # view nan counts in columms\n",
    "\n",
    "# Fill in yes, no features with 1, 0 values\n",
    "for col in ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',\n",
    "            'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n",
    "    test_dataset[col] = (test_dataset[col] == 'Yes').astype(int)\n",
    "    \n",
    "test_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = test_dataset.iloc[:, :-1]\n",
    "y = test_dataset.went_on_backorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "# Load retrained pipeline\n",
    "full_model = joblib.load('data/model-v5.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "       0     1\n",
      "0  9502  1361\n",
      "1   934  9929\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89     10863\n",
      "           1       0.88      0.91      0.90     10863\n",
      "\n",
      "    accuracy                           0.89     21726\n",
      "   macro avg       0.89      0.89      0.89     21726\n",
      "weighted avg       0.89      0.89      0.89     21726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit training dataset to retrained model\n",
    "test_model = full_model.fit(X, y)\n",
    "# Display test model score\n",
    "test_model.score(X, y)\n",
    "\n",
    "# Make prediction using test dataset\n",
    "predicted_y = test_model.predict(X)\n",
    "\n",
    "# Display confusion matrix\n",
    "print('\\nConfusion Matrix:\\n',pd.DataFrame(confusion_matrix(y, predicted_y)))\n",
    "# Create classification report\n",
    "print('\\nClassification Report:\\n',classification_report(y, predicted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "Preprocessing\n",
    "=============\n",
    "To process the data I first removed the sku feature, as suggested. I then if any features were highly correlated (>95%) with each other. Eliminating these features would result in minimal, if any  loss of information about the products. This resulted in the removal of the following features: forecast_6_month, forecast_9_month, sales_6_month, sales_9_month, and perf_12_month_avg. Next I identified all features containing discrete yes/no variables. I filled in null values with the mode in each feature (potential_issue, deck_risk, oe_constraint, ppap_risk, rev_stop, and went_on_backorder with 'No' and stop_auto_buy with 'Yes'). Next, I filled null lead_time data with the mean and missing (-99) perf_6_month_avg data with the mean. I then removed the single remaining records that contained any nulls. I then converted all of the Yes and No entries with 1 and 0. The cleaned dataset remained heavily imbalanced, as only about 7% of all orders went on back order. To both rebalance and decrease the size of the dataset, I downsampled the features to match the total number of items that went on back order (11,293). Finally I saved the downsampled data.\n",
    "\n",
    "Processing\n",
    "=============\n",
    "I loaded the sampled data and eventually decided to lower the amount of data by 90% to make testing and debugging the pipelines take a lot less time. Next I scaled the data both by standardizing as one dataset and normalizing as another. I then split the training dataset into training and testing sets. Next I created three different pipelines. Each pipeline removed outliers, selected features, and used a classifier all potentially with varying parameters. Next, the pipeline was run through a grid search to optimize it for the best hyperparameters. For each pipeline the best estimator was displayed and a confusion matrix and classification report - detailing accuracy, precision, recall, and f1-score - were generated. The 1st pipeline used the local oulier factor method for anomaly detection, principal component analysis for feature selection, and a logistic regression classifier. The 2nd pipeline used the elliptic envelope method for anomaly detection, select k-percentile method of feature selection, and a decision tree classifier. The 3rd pipeline used the isolation forest method for anomaly detection, select k-best method of feature selection, and a random forest classifier. The third pipeline scored the best in all areas with an accuracy of 89% and weighted averages of precision, recall, and f1-scores of 89%..\n",
    "\n",
    "Evaluation\n",
    "=============\n",
    "I loaded the sampled data and the best pipeline. I then retrained the pipeline on the full-sampled training dataset and saw similar scores so saved the retrained model. Next, I loaded in the testing dataset and performed all of the same preprocessing as was used on the training dataset. I split the test data into X and y and loaded the retrained pipeline. Finally, I fit the model to the testing data and made predictions based on the model. The model scored exactly the same on the testing data with an accuracy of 89% and weighted averages of precision, recall, and f1-scores of 89%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "Currently, we are about 59% accurate in determining which products will go on back order resulting in an extra $677 thousand in inventory costs and $4 million in lost revenue due to discounts because of late deliveries. The model I have developed has an accuracy of 89% and, on average predicts 89% of all the products that will go on backorder while 11% of the products it predicts will be backordered will not wind up on backorder. The model is slightly better at finding which products will go on backorder (91%) as opposed to avoiding predicting a product will go on back order when it will not (88%). I tested 134 different permutations of the three models and various parameters before selecting the one with the best predictive ability in all categories. I am highly confident in the model's performance.\n",
    "\n",
    "Since our profit margin is more dependent on fullfilling orders on time than on storing excess inventory and we buy components on a quarterly basis, we can use this model quarterly to get a better understanding of what products are likely to go on backorder in the next quarter so as to order the appropriate amount of components. In this way, an increase in predictive accuracy of 32% means $1.34 million in savings in fees for late delivery and $513 thousand in saved inventory costs. If we were willing to switch to a more on-time order and delivery of components, we could utilize the model to work in near-real time, with periodic updates to the model to ensure it stays up to date with current operations. This would give us the ability to ensure the lowest possible inventory costs coupled with our highest ability to deliver our products to our customers on time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
