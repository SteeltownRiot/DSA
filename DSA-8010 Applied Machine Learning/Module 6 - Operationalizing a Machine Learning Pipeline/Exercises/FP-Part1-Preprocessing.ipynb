{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This dataset was originally posted on Kaggle. **The key task is to predict whether a product/part will go on backorder.**\n",
    "\n",
    "Product backorder may be the result of strong sales performance (e.g. the product is in such a high demand that production cannot keep up with sales). However, backorders can upset consumers, lead to canceled orders and decreased customer loyalty. Companies want to avoid backorders, but also avoid overstocking every product (leading to higher inventory costs).\n",
    "\n",
    "This dataset has ~1.9 million observations of products/parts in an 8 week period. The source of the data is unreferenced.\n",
    "\n",
    "* __Outcome__: whether the product went on backorder\n",
    "* __Predictors__: Current inventory, sales history, forecasted sales, recommended stocking amount, product risk flags etc. (22 predictors in total)\n",
    "\n",
    "The features and the target variable of the dataset are as follows:\n",
    "\n",
    "**Description**\n",
    "~~~\n",
    "# Features: \n",
    "sku - Random ID for the product\n",
    "national_inv - Current inventory level for the part\n",
    "lead_time - Transit time for product (if available)\n",
    "in_transit_qty - Amount of product in transit from source\n",
    "forecast_3_month - Forecast sales for the next 3 months\n",
    "forecast_6_month - Forecast sales for the next 6 months\n",
    "forecast_9_month - Forecast sales for the next 9 months\n",
    "sales_1_month - Sales quantity for the prior 1 month time period\n",
    "sales_3_month - Sales quantity for the prior 3 month time period\n",
    "sales_6_month - Sales quantity for the prior 6 month time period\n",
    "sales_9_month - Sales quantity for the prior 9 month time period\n",
    "min_bank - Minimum recommend amount to stock\n",
    "potential_issue - Source issue for part identified\n",
    "pieces_past_due - Parts overdue from source\n",
    "perf_6_month_avg - Source performance for prior 6 month period\n",
    "perf_12_month_avg - Source performance for prior 12 month period\n",
    "local_bo_qty - Amount of stock orders overdue\n",
    "deck_risk - Part risk flag\n",
    "oe_constraint - Part risk flag\n",
    "ppap_risk - Part risk flag\n",
    "stop_auto_buy - Part risk flag\n",
    "rev_stop - Part risk flag\n",
    "\n",
    "# Target \n",
    "went_on_backorder - Product actually went on backorder\n",
    "~~~\n",
    "\n",
    "Two data files are given and the files are accessible in the JupyterHub environment:\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Training_Dataset_v2.csv`\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    "\n",
    "\n",
    " \n",
    "<span style='background:yellow'>**NOTE:** The training data file is 117MB. **Do NOT try to version control any data files** (training, test, or created), you may blow-through the _push limit_.</span>  \n",
    "You can easily lock up a notebook with bad coding practices.  \n",
    "Please save you project early, and often, and use `git commits` to checkpoint your process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration, Training, and Validation\n",
    "\n",
    "You will examine the _training_ dataset and perform \n",
    " * **data preparation and exploratory data analysis**, \n",
    " * **anomaly detection / removal**,\n",
    " * **dimensionality reduction** and then\n",
    " * **train and validate**.\n",
    "\n",
    "We aim to develop at least 3 unique pipelines (see M5 to learn about pipeline). **By unique we mean that if an ML method (i.e. classification,  feature selection, or anomaly detection) is used in Pipeline 1, that classification method should not be used in Pipeline 2 and Pipeline 3.** \n",
    "\n",
    "Of the 3 different models, you are free to pick any models from Scikit-Learn or any custom models that work within sklearn pipeline. Here is a pool of methods. \n",
    "\n",
    "\n",
    "### Pool of Anomaly Detection Methods (Discussed in M4)\n",
    "1. IsolationForest\n",
    "2. EllipticEnvelope\n",
    "3. LocalOutlierFactor\n",
    "4. OneClassSVM\n",
    "5. SGDOneClassSVM\n",
    "\n",
    "### Pool of Feature Selection Methods (Discussed in M3)\n",
    "\n",
    "1. VarianceThreshold\n",
    "1. SelectKBest with any scoring method (e.g, chi, f_classif, mutual_info_classif)\n",
    "1. SelectKPercentile\n",
    "3. SelectFpr, SelectFdr, or  SelectFwe\n",
    "1. GenericUnivariateSelect\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Variance Threshold\n",
    "5. RFE\n",
    "7. SelectFromModel\n",
    "\n",
    "\n",
    "### Classification Methods (Discussed in M1-M2)\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Logistic Regression\n",
    "4. Naive Bayes\n",
    "5. Linear SVC\n",
    "6. SVC with kernels\n",
    "7. KNeighborsClassifier\n",
    "8. GradientBoostingClassifier\n",
    "9. XGBClassifier\n",
    "10. LGBM Classifier\n",
    "\n",
    "\n",
    "\n",
    "### Validation Assessment\n",
    "\n",
    "Your first, intermediate, result will be an **assessment** of the models' performance.\n",
    "This assessement should be grounded within a 5-fold or 10-fold cross-validation methodology. Give an unbiased evaluaiton of the best model within each pipeline. This should include the confusion matrix, precision, recall, F1-score, and accuracy for each classifier as a bare minimum.\n",
    "\n",
    "## Testing\n",
    "\n",
    "Once we have chosen our final model, we need to re-train it using all the training data. Then final evaluation should be performed on the given test dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "--- \n",
    "##  Overview / Roadmap\n",
    "\n",
    "**General steps**:\n",
    "* Part I: Preprocessing\n",
    "  * Dataset carpentry & Exploratory Data Analysis\n",
    "    * Develop functions to perform the necessary steps, you will have to carpentry the Training and the Testing data.\n",
    "  * Generate a **smart sample** of the the data\n",
    "* Part II: Training and Validation\n",
    "  * Create 3 alternative pipelines, each does:\n",
    "      * Anomaly detection\n",
    "      * Dimensionality reduction\n",
    "      * Classification\n",
    "* Part III: Testing\n",
    "  * Train chosen model full training data\n",
    "  * Evaluate model against testing\n",
    "  * Write a summary of your processing and an analysis of the model performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Preprocessing\n",
    "\n",
    "In this part, we preprocess the given training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "**Description**\n",
    "~~~\n",
    "sku - Random ID for the product\n",
    "national_inv - Current inventory level for the part\n",
    "lead_time - Transit time for product (if available)\n",
    "in_transit_qty - Amount of product in transit from source\n",
    "forecast_3_month - Forecast sales for the next 3 months\n",
    "forecast_6_month - Forecast sales for the next 6 months\n",
    "forecast_9_month - Forecast sales for the next 9 months\n",
    "sales_1_month - Sales quantity for the prior 1 month time period\n",
    "sales_3_month - Sales quantity for the prior 3 month time period\n",
    "sales_6_month - Sales quantity for the prior 6 month time period\n",
    "sales_9_month - Sales quantity for the prior 9 month time period\n",
    "min_bank - Minimum recommend amount to stock\n",
    "potential_issue - Source issue for part identified\n",
    "pieces_past_due - Parts overdue from source\n",
    "perf_6_month_avg - Source performance for prior 6 month period\n",
    "perf_12_month_avg - Source performance for prior 12 month period\n",
    "local_bo_qty - Amount of stock orders overdue\n",
    "deck_risk - Part risk flag\n",
    "oe_constraint - Part risk flag\n",
    "ppap_risk - Part risk flag\n",
    "stop_auto_buy - Part risk flag\n",
    "rev_stop - Part risk flag\n",
    "went_on_backorder - Product actually went on backorder. \n",
    "~~~\n",
    "\n",
    "**Note**: This is a real-world dataset without any preprocessing.  \n",
    "There will also be warnings due to fact that the 1st column is mixing integer and string values.  \n",
    "**NOTE:** The last column, `went_on_backorder`, is what we are trying to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Training_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "dataset.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "In this section, the goal is to figure out:\n",
    "\n",
    "* which columns we can use directly,  \n",
    "* which columns are usable after some processing,  \n",
    "* and which columns are not processable or obviously irrelevant (like product id) that we will discard.\n",
    "\n",
    "Then process and prepare this dataset for creating a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take samples and examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,12:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,18:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns that are obviously irrelevant or not processable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E101)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique values of string columns\n",
    "\n",
    "Now try to make sure that these Yes/No columns really only contains Yes or No.  \n",
    "If that's true, proceed to convert them into binaries (0s and 1s).\n",
    "\n",
    "**Tip**: use [unique()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) function of pandas Series.\n",
    "\n",
    "Example\n",
    "\n",
    "~~~python\n",
    "print('went_on_backorder', dataset['went_on_backorder'].unique())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E102)\n",
    "# ----------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see **nan** also as possible values representing missing values in the dataset.\n",
    "\n",
    "We fill them using most popular values, the [Mode](https://en.wikipedia.org/wiki/Mode_%28statistics%29) in Stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert yes/no columns into binary (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E103)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all columns should be either int64 or float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smartly sample the data into a more manageable size for cross-fold validation in Grid Search\n",
    "\n",
    "**Note**: This is a good point to re-balance dataset before actually moving on. For sampling we can either take advantage of pandas/numpy `sample` method or use `imblearn` [package](https://imbalanced-learn.org/stable/user_guide.html#user-guide). \n",
    "\n",
    "`imblearn` module has implemented a pipeline on top sklearn pipeline, and it is possible to add sampling strategies within the `imblearn` pipeline. We are not required to use `imblearn` pipeline for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_backorder = np.sum(dataset['went_on_backorder']==1)\n",
    "print('backorder ratio:', num_backorder, '/', len(dataset), '=', num_backorder / len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a smar sample of the dataset. You can either store the data to csv files or simply use `joblib` to dump the variables and load them in Part 2. \n",
    "\n",
    "**Example code for using joblib:**\n",
    "\n",
    "Say we need to store three objects (sampled_X, sampled_y, model) to a file. \n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# for dumping \n",
    "joblib.dump([sampled_X, sampled_y, model], 'data/sample-data-v1.pkl')\n",
    "\n",
    "# for loading\n",
    "sampled_X, sampled_y, model = joblib.load('data/sample-data-v1.pkl')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment   (Question #E104) \n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** After sampling the data, you may want to write the data to a file for reloading later.\n",
    "\n",
    "<span style=\"background: yellow;\">If required, remove the `dataset` variable to avoid any memory-related issue.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your smart sampling to local file  \n",
    "# ----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a couple commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Data Sampled`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
