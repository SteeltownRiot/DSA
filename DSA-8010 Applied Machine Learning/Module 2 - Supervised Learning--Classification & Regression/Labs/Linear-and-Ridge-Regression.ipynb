{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate $\\rightarrow$ Train, Test\n",
    "\n",
    "### Focus: Linear and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When constructing a model, data availability may become an issue. \n",
    "In order to avoid overfitting, it is necessary to withhold some portion of the data as a test set. \n",
    "However, overfitting *on the test set* may also occur without a secondary validation step. \n",
    "As such, `scikit` contains a number of methods for cross-validation of data.\n",
    "\n",
    "## References\n",
    "1. [Scikit documentation - LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "2. [Scikit documentation - Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load dataset \n",
    "raw = load_diabetes()\n",
    "X = raw.data# slice off only the first feature (.data is multi-dimensional)\n",
    "y = raw.target # the target data is a single label, so it can all be kept\n",
    "\n",
    "# test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "# we'll use the linear regression & ridge regression to predict diabetese\n",
    "basline_reg_model = DummyRegressor()\n",
    "linear_reg_model = LinearRegression()\n",
    "ridge_reg_model = Ridge()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Though a manual CV workflow was described in [the cross-validation lab](./CrossValidation.ipynb), the automated `cross_val_score()` will work well enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "print('Features: ', raw.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score : [-1.80462803e-03 -3.36823685e-06 -1.40266582e-02 -8.86415630e-07\n",
      " -7.02248801e-03]\n",
      "Linear score : [0.44978621 0.59947543 0.49575483 0.57183858 0.43842298]\n",
      "Ridge score  : [0.40684985 0.44450481 0.38745555 0.41612711 0.35038474]\n"
     ]
    }
   ],
   "source": [
    "# automated CV step\n",
    "\n",
    "baseline_scores = cross_val_score(basline_reg_model, X_train, y_train, cv=5)\n",
    "linear_scores = cross_val_score(linear_reg_model, X_train, y_train, cv=5)\n",
    "ridge_score = cross_val_score(ridge_reg_model, X_train, y_train, cv=5)\n",
    "print(\"Baseline score :\",baseline_scores) \n",
    "print(\"Linear score :\",linear_scores) \n",
    "print(\"Ridge score  :\", ridge_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are performing cross validation with the training set. These cross-validation values represent how well (with 1 being a perfect score) the model performed against a small, as-yet-untrained portion of the data for the classification task. In the regression setting **R^2 (coefficient of determination)** is used as the default regression scoring function. The best possible R^2 score is 1.0 and it can be negative (because the model can be arbitrarily worse). See [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score). \n",
    "\n",
    "## Training the new model\n",
    "\n",
    "Since the CV values are relatively high, we can create a model using all the data in the training set and test against the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model\n",
    "basline_reg_model.fit(X_train, y_train)\n",
    "linear_reg_model.fit(X_train, y_train)\n",
    "ridge_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# model.predict() returns predicted values\n",
    "y_pred_baseline = basline_reg_model.predict(X_test)\n",
    "y_pred_linear = linear_reg_model.predict(X_test)\n",
    "y_pred_ridge = ridge_reg_model.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       MSE    MAE  MAPE    R2\n",
      "Baseline           5420.61  64.27  0.67 -0.02\n",
      "Linear Regression  3137.69  45.20  0.44  0.41\n",
      "Ridge Regression   3280.30  48.55  0.49  0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "scores = [\n",
    "    [mean_squared_error(y_test, y_pred_baseline),\n",
    "     mean_absolute_error(y_test, y_pred_baseline),\n",
    "     mean_absolute_percentage_error(y_test, y_pred_baseline),\n",
    "     r2_score(y_test, y_pred_baseline)\n",
    "    ],    \n",
    "    [mean_squared_error(y_test, y_pred_linear),\n",
    "     mean_absolute_error(y_test, y_pred_linear),\n",
    "     mean_absolute_percentage_error(y_test, y_pred_linear),\n",
    "     r2_score(y_test, y_pred_linear)\n",
    "    ],    \n",
    "    [mean_squared_error(y_test, y_pred_ridge),\n",
    "     mean_absolute_error(y_test, y_pred_ridge),\n",
    "     mean_absolute_percentage_error(y_test, y_pred_ridge),\n",
    "     r2_score(y_test, y_pred_ridge)\n",
    "    ],\n",
    "    \n",
    "]\n",
    "\n",
    "scores_matrix = DataFrame(scores, \n",
    "                          columns=['MSE', 'MAE', 'MAPE', 'R2'],\n",
    "                         index=['Baseline', 'Linear Regression', 'Ridge Regression'])\n",
    "\n",
    "scores_matrix = np.around(scores_matrix, 2)\n",
    "print(scores_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment: Linear Regression is outperforming Ridge and baseline in terms of all the metrices. For MSE, MAE, and MAPE, the lower, the better. And for R^2, the higher, the better.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
