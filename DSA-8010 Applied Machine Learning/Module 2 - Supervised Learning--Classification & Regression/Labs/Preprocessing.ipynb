{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling: Standardization, Normalization, & Others\n",
    "\n",
    "In this notebook, we discuss feature scaling: standardization and normalization. \n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "\n",
    "Feature Scaling is a necessary, or even essential, step in before training any machine learning models. Why? Many of the models based on mathematical formulas which may not work well with variables of different scales. For example, an attribute such as age is drawn on a very different scale than an attribute such as salary. The latter attribute is typically orders of magnitude larger than the former. As a result, any aggregate function computed on the different features (e.g., Euclidean distances) will be dominated by the attribute of larger magnitude.\n",
    "\n",
    "Read the following article to learn more about feature scaling. \n",
    "\n",
    "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Standardization means scaling data with zero mean and unit varience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the dataset features\n",
      "[1.         0.         0.33333333]\n",
      "Variance of data\n",
      "[0.81649658 0.81649658 1.24721913]\n",
      "-------------------------\n",
      "# Scaled data:\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "# Mean of scaled data\n",
      "[0. 0. 0.]\n",
      "# Variance of scaled data\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x_train  = np.array([[1.0, -1.0, 2.0],\n",
    "                     [2.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, -1.0]])\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "print(\"Mean of the dataset features\")\n",
    "print(scaler.mean_)\n",
    "print(\"Variance of data\")\n",
    "print(scaler.scale_)\n",
    "print(\"-\" * 25)\n",
    "x_scaled = scaler.transform(x_train)\n",
    "print(\"# Scaled data:\")\n",
    "print(x_scaled)\n",
    "\n",
    "\n",
    "print(\"# Mean of scaled data\")\n",
    "print(x_scaled.mean(axis=0))\n",
    "print(\"# Variance of scaled data\")\n",
    "print(x_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization means scaling data to have unit norm. By default, L2-norm is considered. What is a norm? A norm a function of vector/matrix and it sometimes has geometric interpretation. E.g., L2-norm of a vector is essentially an Euclidean distance between a vector from the origin and L1-norm is the sum of the tmagnitudes of the vector elements (aka Manhattan distance or taxicab norm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scaled & normalized values:\n",
      "[[ 0.4472136  -0.70710678  0.89442719]\n",
      " [ 0.89442719  0.          0.        ]\n",
      " [ 0.          0.70710678 -0.4472136 ]]\n",
      "# All the line has unit norm\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x_train  = np.array([[1.0, -1.0, 2.0],\n",
    "                     [2.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, -1.0]])\n",
    "\n",
    "x_normalized = preprocessing.normalize(x_train, axis = 0, norm='l2')  # axis = 0 means column-wise\n",
    "\n",
    "print('# Scaled & normalized values:')\n",
    "print(x_normalized)\n",
    "\n",
    "print(\"# All the line has unit norm\")\n",
    "print(np.linalg.norm(x_normalized, axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000002236256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4472136 ** 2 + 0.89442719 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax Scaler\n",
    "\n",
    "MinMax Scaler converts data to the range [0,1]. We can also use **MaxAbsScalar** for scaling data in the range of [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range and Scaled converstion of x_train data\n",
      "[[0.5        0.         1.        ]\n",
      " [1.         0.5        0.33333333]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x_train  = np.array([[1.0, -1.0, 2.0],\n",
    "                     [2.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, -1.0]])\n",
    "\n",
    "\n",
    "processor = preprocessing.MinMaxScaler()\n",
    "\n",
    "range_scaled = processor.fit_transform(x_train)\n",
    "\n",
    "print(\"Range and Scaled converstion of x_train data\")\n",
    "print(range_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
