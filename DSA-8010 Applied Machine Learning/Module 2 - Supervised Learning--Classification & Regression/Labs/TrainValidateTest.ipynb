{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate $\\rightarrow$ Train, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When constructing a model, data availability may become an issue. \n",
    "In order to avoid overfitting, it is necessary to withhold some portion of the data as a test set. \n",
    "However, overfitting *on the test set* may also occur without a secondary validation step. \n",
    "As such, `scikit` contains a number of methods for cross-validation of data.\n",
    "\n",
    "## References\n",
    "1. [Scikit documentation - GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load dataset \n",
    "raw = load_iris()\n",
    "X = raw.data[:, :2] # slice off only the first feature (.data is multi-dimensional)\n",
    "y = raw.target # the target data is a single label, so it can all be kept\n",
    "\n",
    "# test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "# we'll use the Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Though a manual CV workflow was described in [the cross-validation lab](./CrossValidation.ipynb), the automated `cross_val_score()` will work well enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77777778 0.77777778 0.72222222 0.94444444 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "# automated CV step\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "print(scores) # TODO: visualization of CV process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are performing cross validation with the training set. These cross-validation values represent how well (with 1 being a perfect score) the model performed against a small, as-yet-untrained portion of the data for the classification task.\n",
    "\n",
    "## Training the new model\n",
    "\n",
    "Since the CV values are relatively high, we can create a model using all the data in the training set and test against the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        22\n",
      "           1       0.59      0.59      0.59        17\n",
      "           2       0.68      0.71      0.70        21\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.76      0.75      0.75        60\n",
      "weighted avg       0.77      0.77      0.77        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit new model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# GaussianNB.predict() returns class labels (integers)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models through pickling / `joblib`\n",
    "\n",
    "In many cases, the model being trained will be used more than once for the same or different data. \n",
    "For large datasets, training can be computationally expensive as well. \n",
    "For these and other reasons it is often necessary to save a trained model so it can later be loaded.\n",
    "This is relatively easy to do from within `scikit` via `joblib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GaussianIris.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming some model 'clf' is initialized and trained:\n",
    "import joblib\n",
    "joblib.dump(classifier, 'GaussianIris.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib` makes loading in pickled models similarly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        22\n",
      "           1       0.59      0.59      0.59        17\n",
      "           2       0.68      0.71      0.70        21\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.76      0.75      0.75        60\n",
      "weighted avg       0.77      0.77      0.77        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we'll load to a new variable this time, for clarity\n",
    "loaded_model = joblib.load('GaussianIris.pkl')\n",
    "\n",
    "# and predict with the 'new' model:\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results are exactly the same.\n",
    "This is no accident - the model being loaded below is *the same* as the one used above.\n",
    "\n",
    "Please read a little about Pickling (or Serialization) [here](https://docs.python.org/3/library/pickle.html) and [here](https://en.wikipedia.org/wiki/Serialization).\n",
    "\n",
    "## Notes on pickling - safety, efficiency\n",
    "\n",
    "`joblib` (and by extension the default `pickle` module) is by no means your only option for model storage.\n",
    "`cPickle` is a [faster](https://docs.python.org/2.2/lib/module-cPickle.html) \n",
    "C-based implementation of the same pickling algorithm, \n",
    "and for more significant models (as we may cover in later modules) it would be worth looking into.\n",
    "\n",
    "Moreover, consider reading the `scikit` docs on [persistence](http://scikit-learn.org/stable/modules/model_persistence.html)\n",
    "for considerations on the long-term safety of pickling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
