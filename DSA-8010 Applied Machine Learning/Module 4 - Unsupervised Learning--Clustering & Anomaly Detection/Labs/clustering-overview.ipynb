{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical & DBSCAN clustering methods\n",
    "\n",
    "This lab is a brief review of clustering you have seen previously.\n",
    "These examples are in Python using ScikitLearn instead of R language used in 7020.\n",
    "\n",
    "**sklearn ref:**\n",
    "\n",
    "* [K-Means clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "* [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
    "* [DBSCAN clustering](https://scikit-learn.org/stable/modules/clustering.html#dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means (Centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "\n",
    "# Run Clustering\n",
    "K = 3   # we are cheating and know this to be the number of \"clusters\"\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=K)\n",
    "k_means.fit(X_iris) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cluster id of each data points \n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the centroids of the clusters\n",
    "centroids = k_means.cluster_centers_\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each cluster with its center\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "for i in range(K):\n",
    "    # select only data observations with cluster label == i\n",
    "    ds = X_iris[np.where(labels==i)]\n",
    "    # plot the clusters (showing only feature 0 and 3 for making a 2D plot)\n",
    "    pyplot.plot(ds[:,0],ds[:,3],'o')  \n",
    "    # plot the centroids\n",
    "    lines = pyplot.plot(centroids[i,0],centroids[i,3],'kx')\n",
    "    # make the centroid x's bigger\n",
    "    pyplot.setp(lines,ms=15.0)\n",
    "    pyplot.setp(lines,mew=2.0)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best number of clusters in K-means\n",
    "\n",
    "As clustering is an unsuprevised learning, there is no right or wrong answer. But it is desirable to choose an optimal number of clusters which will better explain the underlying structure of the dataset.\n",
    "\n",
    "The Elbow Method is one of the most popular methods to determine this optimal value of k. In this example, we will use **inertia** to create an eblow plot. Inertia is the sum of squared distances of samples to their closest cluster center. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intertia = []\n",
    "\n",
    "for tot_cluster in range(1, 20):\n",
    "    kmeans = cluster.KMeans(init=\"k-means++\", n_clusters=tot_cluster, n_init=tot_cluster)\n",
    "    kmeans.fit(X_iris)\n",
    "    intertia.append(kmeans.inertia_)\n",
    "\n",
    "    \n",
    "plt.plot([i for i in range(1,20)], intertia, 'bo-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the elbow plot of interia, we see that inertia values gets smaller with the increase of the number of clusters. But after three clusters the rate of decreasing inertia very small. So we can choose k=3 or k=4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering (aka linkage or agglomerative clustering) \n",
    "\n",
    "This is an example using the hierarchical clustering algorithm on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo = cluster.AgglomerativeClustering(affinity='euclidean',\n",
    "                                     linkage='ward',\n",
    "                                     n_clusters=K)\n",
    "\n",
    "# Notice we need to transpose the data going into agglomeration. \n",
    "# This method expects each row is a feature and each column is an instance\n",
    "agglo.fit(X_iris) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cluster id/label for each data point\n",
    "labels = agglo.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color label mapping that accommodates up to 7 labels\n",
    "colors='rgbykcm'\n",
    "\n",
    "# create the per-datum color mapping\n",
    "color_map = [colors[i] for i in labels]\n",
    "                    # for every label (i) use it as an index into the string colors,\n",
    "                    # which is the list [r,g,b,y,k,c,m]\n",
    "\n",
    "plt.scatter(X_iris[:,0], X_iris[:,3], c=color_map)  # only feature 0 and 3 are shown in the plot\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can vary `affinity` (i.e., distance measure), `linkage`, and `n_clusters` to see the effect of these parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In agglomerative clustering clusters are created with bottom-up approach: \n",
    "\n",
    "1. Initially each data point is treated as a cluster\n",
    "2. In successive iterations points (or group of points) are merged and larger clusters are formed\n",
    "\n",
    "This formation of clusters can be shown as a dendrogram (i.e tree). To do so, we need to pass `distance_threshold` parameter, which is the distance threshold above which, clusters will not be merged. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "agglo = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "agglo = agglo.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(agglo, truncate_mode=\"level\", p=3)\n",
    "\n",
    "# No more than p levels of the dendrogram tree are displayed. \n",
    "# A “level” includes all nodes with p merges from the final merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DBSCAN (Density-based)\n",
    "\n",
    "This is an example using the DBSCAN clustering algorithm on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the parameters are density based\n",
    "p_eps=1   # Radius\n",
    "p_min_samples=10  # Strength of local neighborhood\n",
    "\n",
    "dbs = cluster.DBSCAN(eps=p_eps, min_samples=p_min_samples)\n",
    "dbs.fit(X_iris) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the label vector\n",
    "labels = dbs.labels_\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and plot the data with region labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "# Define a color label mapping that accommodates up to 7 labels\n",
    "colors='rgbykcm'\n",
    "\n",
    "# create the per-datum color mapping\n",
    "color_map = [colors[i] for i in labels]\n",
    "# print(color_map)\n",
    "                    # for every label (i) use it as an index into the string colors,\n",
    "                    # which is the list [r,g,b,y,k,c,m]\n",
    "\n",
    "plt.scatter(X_iris[:,0], X_iris[:,3], c=color_map)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"background:yellow\">What are we seeing from DBSCAN?</span>\n",
    "\n",
    "\n",
    " 1. How many clusters did you get from DBSCAN and is this what you expected?\n",
    " 1. Would you suggest we change the parameters to DBSCAN to attempt to get three clusters? Why or why not, and if \"yes\" ... How would you suggest to change the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the given dbscan we have only got 2 cluster. Its because two of our classes have commong spacial regions. And we are selecting the distance of 1 and minimum number of sample around the center should be 10. \n",
    "\n",
    "To get our expected number of cluster. There are multiple ways. We can reduce the distance and keep the number of items in the circle same. But lowering the distance will also increse the number of outliers points; this may not be desirable. E.g., let's use min number of samples equals to 10 and reduce the distance to 0.4. This will provide our expected 3 cluster. But it will also increase the number of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the parameters are density based\n",
    "p_eps=.4   # New redius\n",
    "p_min_samples=10  # Strength of local neighborhood\n",
    "\n",
    "dbs = cluster.DBSCAN(eps=p_eps, min_samples=p_min_samples)\n",
    "dbs.fit(X_iris) \n",
    "\n",
    "# Extract the label vector\n",
    "labels = dbs.labels_\n",
    "print(labels)\n",
    "\n",
    "# total cluster \n",
    "discovered_cluster = len(np.unique(labels[labels >= 0])) # outliers are labeled as -1\n",
    "\n",
    "# total outliers\n",
    "out = len(dbs.labels_) - len(labels[labels>=0])\n",
    "\n",
    "print(\"Total cluster \", discovered_cluster, \" outliers \", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color label mapping that accommodates up to 7 labels\n",
    "colors='rgbmkcy'\n",
    "\n",
    "# create the per-datum color mapping\n",
    "color_map = [colors[i] for i in labels]\n",
    "                    # for every label (i) use it as an index into the string colors,\n",
    "                    # which is the list [r,g,b,y,k,c,m]\n",
    "\n",
    "plt.scatter(X_iris[:,0], X_iris[:,3], c=color_map)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "print('All the yellow points are the outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the distance of 0.4 and the item count of 10, we found 83 outliers, which is quite a lot. We can say that we need to increase the distance, but it may increases the overlaps between the clusters. We can try reducing the number of items around the center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recall the parameters are density based\n",
    "p_eps=.4   # New redius\n",
    "p_min_samples=3  # Strength of local neighborhood\n",
    "\n",
    "dbs = cluster.DBSCAN(eps=p_eps, min_samples=p_min_samples)\n",
    "dbs.fit(X_iris) \n",
    "\n",
    "# Extract the label vector\n",
    "labels = dbs.labels_\n",
    "\n",
    "# total cluster \n",
    "discovered_cluster = len(np.unique(labels[labels >= 0]))\n",
    "\n",
    "# total outliers\n",
    "out = len(dbs.labels_) - len(labels[labels>=0])\n",
    "\n",
    "print(\"Total cluster \", discovered_cluster, \" outliers \", out)\n",
    "\n",
    "# Define a color label mapping that accommodates up to 7 labels\n",
    "colors='rgbmkcy'\n",
    "\n",
    "# create the per-datum color mapping\n",
    "color_map = [colors[i] for i in labels]\n",
    "                    # for every label (i) use it as an index into the string colors,\n",
    "                    # which is the list [r,g,b,y,k,c,m]\n",
    "\n",
    "plt.scatter(X_iris[:,0], X_iris[:,3], c=color_map)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "print('All the yellow points are the outliers')\n",
    "print('And below is the original class distribution with feature 0 and 3')\n",
    "\n",
    "labels = iris.target\n",
    "color_map = [colors[i] for i in labels]\n",
    "\n",
    "plt.scatter(X_iris[:,0], X_iris[:,3], c=color_map)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the number of clusters is increased by one, but the number of outliers is decreased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluating clustering\n",
    "\n",
    "\n",
    "#### Silhouette score\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for tot_cluster in range(3, 20):\n",
    "    kmeans = cluster.KMeans(init=\"k-means++\", n_clusters=tot_cluster, n_init=tot_cluster)\n",
    "    kmeans.fit(X_iris)\n",
    "    predicted_labels = kmeans.labels_\n",
    "    scores.append(silhouette_score(X_iris, predicted_labels, metric = 'euclidean'))\n",
    "    \n",
    "    \n",
    "plt.plot([i for i in range(3, 20)], scores, 'ko--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dunn index\n",
    "\n",
    "The Davies–Bouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979), a metric for evaluating clustering algorithms, is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. The lower the DB index denotes better clustering. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "scores = []\n",
    "for tot_cluster in range(3,20):\n",
    "    kmeans = cluster.KMeans(init=\"k-means++\", n_clusters=tot_cluster, n_init=tot_cluster)\n",
    "    kmeans.fit(X_iris)\n",
    "    predicted_labels = kmeans.labels_\n",
    "    scores.append(davies_bouldin_score(X_iris, predicted_labels))\n",
    "    \n",
    "plt.plot([i for i in range(3,20)], scores, 'ko--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both measures, we get the best performance when k=3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
