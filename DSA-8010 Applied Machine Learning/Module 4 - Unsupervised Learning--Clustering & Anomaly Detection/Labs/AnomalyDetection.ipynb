{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Outlier/Novelty Detection \n",
    "\n",
    "In this lab, we'll be covering some very basic algorithms for anomaly detection.\n",
    "\n",
    "## Getting started\n",
    "First, we'll need to import a few modules to construct a regression problem and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating synthetic data and first attempt with a linear model\n",
    "\n",
    "In this section, we create some synthetic data to experiment with linear regression and compare the differences between linear regression with and without outlier removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some data for regression\n",
    "X, y = make_regression(n_samples = 20, n_features = 1, noise=3.0, bias=100.0)\n",
    "\n",
    "# Generate some significant outliers\n",
    "X_outliers = np.random.normal(0, 0.5, size=(4, 1))\n",
    "y_outliers = np.random.normal(0, 2.0, size=4)\n",
    "X_outliers[:2, :] += X.max() + abs(X.mean()) * 3\n",
    "X_outliers[2:, :] += X.min() - abs(X.mean()) * 3\n",
    "y_outliers[:2] += y.min() - abs(y.mean()) * 2\n",
    "y_outliers[2:] += y.max() + abs(y.mean()) * 2\n",
    "\n",
    "# Add outliers to existing data\n",
    "X = np.vstack((X, X_outliers))\n",
    "y = np.concatenate((y, y_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to visually confirm any suspicions about a particular dataset. If we plot every data point, we should see four obvious outliers given the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the data above contains four strong outliers.\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X_outliers, y_outliers, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The outliers are the values in the corner.**\n",
    "Looking good so far. The outliers are the pairs of points in the upper left and lower right corners of the plot.\n",
    "\n",
    "#### What would you expect to be the result of fitting a linear regression model to this data?\n",
    "\n",
    "We should attempt to fit a model to the data, first without outlier reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict without running outlier reduction\n",
    "raw_model = Ridge().fit(X, y)\n",
    "y_pred = raw_model.predict(X)\n",
    "\n",
    "# plot results\n",
    "\n",
    "\n",
    "plt.scatter(X, y, color=\"black\")\n",
    "plt.scatter(X_outliers, y_outliers, color=\"red\")\n",
    "plt.plot(X, y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is obviously wrong - the high-variance outliers are causing strong *overfitting* in the model, producing a negative-slope model for a positive-slope trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reduce overfitting: EllipticEnvelope\n",
    "\n",
    "One method of outlier reduction is to fit an `EllipticEnvelope` to the data  and use its `predict()` method to detect and retain only those data points which are *inliers*.\n",
    "\n",
    "Elliptic Envelope is most suited for Gaussian distributed datasets because it detects outliers by estimating the covariance matrix. This algorithm attempts to minimize the volume of the ellipsoid corresponding to the covariance matrix by discarding a fixed fraction of contamination points.\n",
    "This is to say you would have to have a rough idea of the fraction of contamination points to be rejected in order to use this algorithm.\n",
    "\n",
    "Now let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_line(X_clean, y_clean):\n",
    "    # plot the given dataset\n",
    "    plt.scatter(X, y, color=\"black\")\n",
    "    plt.scatter(X_outliers, y_outliers, color=\"purple\")\n",
    "    \n",
    "    # Fit a regression line with the cleaned data points\n",
    "    plt.scatter(X_clean, y_clean, color=\"blue\")\n",
    "    model = Ridge().fit(X_clean, y_clean)\n",
    "    y_pred = model.predict(X_clean)\n",
    "\n",
    "    # Plot the regression\n",
    "    plt.plot(X_clean, y_pred, linewidth=3, color=\"red\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit envelope\n",
    "envelope = EllipticEnvelope(support_fraction=1, contamination=0.2).fit(X)\n",
    "\n",
    "# Create an boolean indexing array to pick up outliers\n",
    "outliers = envelope.predict(X)==-1\n",
    "\n",
    "# Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "X_clean = X[~outliers]\n",
    "y_clean = y[~outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg_line(X_clean, y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can successfully create a regression model on the cleaned dataset. \n",
    "It did a job of sequestering the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to the EllipticEnvelope: KMeans, LocalOutlierFactor\n",
    "\n",
    "Ultimately, most of the statistical learning algorithms available to you in `scikit` for outlier reduction are similar in usage and function.  \n",
    "\n",
    "Given that the goal of this exercise is to group 'good' inputs while avoiding 'bad' inputs,  and the 'bad' inputs are those with relatively high variance, we can attempt to preprocess our inputs with a round of `KMeans` clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means clustering\n",
    "km_clusters = KMeans(n_clusters=3, algorithm=\"full\").fit_predict(X, y)\n",
    "\n",
    "# create cluster distribution, this time they are in tuples so we can sort easily\n",
    "dist_clusters = ((np.sum(km_clusters==z), z) for z in np.unique(km_clusters))\n",
    "\n",
    "# sort clusters descendingly by number of data entries in cluster\n",
    "dist_clusters = sorted(dist_clusters, reverse = True)\n",
    "\n",
    "# find out the cluster with max number of data entries\n",
    "max_cluster = dist_clusters[0][1]\n",
    "\n",
    "# select data in max_cluster as inliers\n",
    "inliers = km_clusters == max_cluster\n",
    "\n",
    "X_clean = X[inliers]\n",
    "y_clean = y[inliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg_line(X_clean, y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! It seems we have managed to compensate for the outliers.\n",
    "\n",
    "### Further practice: tweaking parameters\n",
    "As an exercise, you could try changing to selector clusters other than `max_cluster`,  and see how the predictions change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other options: LocalOutlierFactor\n",
    "\n",
    "Though, KMeans is by no means the only way to achieve outlier reduction. There are many other valid measures of covariance which we can apply to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_labels = LocalOutlierFactor(n_neighbors=10).fit_predict(X, y)\n",
    "inliers = lof_labels == 1 # select inliers\n",
    "X_clean = X[inliers]\n",
    "y_clean = y[inliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg_line(X_clean, y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with LocalOutlierFactor\n",
    "As an exercise, try changing `n_neighbors` in the LocalOutlierFactor constructor call. Different values will change the fit and behavior of the model - this is especially true given such a small dataset, and given that the default value for n_neighbors is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
