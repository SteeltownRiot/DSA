{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Anomaly Detection\n",
    "## Practice: Outlier Reduction for Linear Regression\n",
    "In this session, we'll be fitting a `LinearRegression` model on the `boston` dataset included in `scikit-learn`.  \n",
    "\n",
    "Having already worked with this dataset,\n",
    "you may remember it as a simple yet broadly representative linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started - imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "First order of business is to load in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load boston housing dataset \n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, y shape: (506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "# Split into X and y sets   #P4001\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Print out some basic shape data on the arrays\n",
    "print(\"X, y shape:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (404, 13)\n",
      "y_train.shape:  (404,)\n",
      "X_test.shape:  (102, 13)\n",
      "y_test.shape:  (102,)\n"
     ]
    }
   ],
   "source": [
    "# perform train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  #P4002\n",
    "\n",
    "# verify split shapes and contents\n",
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cross validation (5-fold) on a linear ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.66285345 0.78752977 0.77137863 0.73129637 0.63939938]\n",
      "Mean score (5 folds):  0.718491518543017\n"
     ]
    }
   ],
   "source": [
    "naive_model = LinearRegression() #P4003\n",
    "scores = cross_val_score(estimator = naive_model, X = X_train, y = y_train)\n",
    "print(\"Scores: \", scores)\n",
    "print(\"Mean score (5 folds): \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit this model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a model with all the training data #P4004\n",
    "naive_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some predictions from testing dataset and print mean absolute measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 13) (102,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.061419182954701"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_predictions = naive_model.predict(X_test) #P4005\n",
    "print(X_test.shape, naive_predictions.shape)\n",
    "\n",
    "mean_absolute_error(y_test, naive_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What methods are available to us for outlier reduction?\n",
    "We could try `KMeans` or an `EllipticEnvelope` again, but we're going to explore a few more options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of outliers = 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Construct IsolationForest \n",
    "iso_forest = IsolationForest(contamination=0.05).fit(X_train, y_train)\n",
    "\n",
    "# Get labels from classifier and cull outliers #P4006\n",
    "iso_outliers = iso_forest.predict(X_train) == -1\n",
    "print(f\"Num of outliers = {np.sum(iso_outliers)}\")\n",
    "X_iso = X_train[~iso_outliers]\n",
    "y_iso = y_train[~iso_outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64998117 0.79432671 0.76187    0.76921453 0.60922113]\n",
      "Mean CV score w/ IsolationForest: 0.7169227069812166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.044947300109876"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# develop a liner regression model without outliers \n",
    "\n",
    "iso_model = LinearRegression()\n",
    "iso_model.fit(X_iso, y_iso)\n",
    "\n",
    "# Cross validate the new model\n",
    "iso_scores = cross_val_score(estimator = iso_model, X = X_iso, y = y_iso)\n",
    "print(iso_scores)\n",
    "print(\"Mean CV score w/ IsolationForest:\", np.mean(iso_scores))\n",
    "\n",
    "iso_predictions = iso_model.predict(X_test)\n",
    "mean_absolute_error(y_test, iso_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to IsolationForest: OneClassSVM\n",
    "This means it's time to try something else.  \n",
    "The code below will look very similar to the above, but using `OneClassSVM` in place of the `IsolationForest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of outliers = 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Construct OneClassSVM (kernel='rbf') and fit to full dataset\n",
    "svm = OneClassSVM(kernel='rbf').fit(X_train, y_train)\n",
    "\n",
    "# Get labels from classifier and cull outliers #P4007\n",
    "svm_outliers = svm.predict(X_train) == -1\n",
    "print(f\"Num of outliers = {np.sum(iso_outliers)}\")\n",
    "X_svm = X_train[~svm_outliers]\n",
    "y_svm = y_train[~svm_outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66400765  0.7291245   0.5624935   0.68477104 -0.48927007]\n",
      "Mean CV score w/ OneClassSVM: 0.43022532584013806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.9794908013350256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# develop a linear regression model without outliers \n",
    "\n",
    "svm_model = LinearRegression().fit(X_svm, y_svm)\n",
    "\n",
    "# Cross validate the new model\n",
    "svm_scores = cross_val_score(estimator = svm_model, X = X_svm, y = y_svm)\n",
    "print(svm_scores)\n",
    "print(\"Mean CV score w/ OneClassSVM:\", np.mean(svm_scores))\n",
    "\n",
    "# Make predictions with the fitted model\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "mean_absolute_error(y_test, svm_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Analysis\n",
    "\n",
    "Of the anomaly detection algorithms used, which had the highest marginal performance. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Provide your answers to the question above in this cell\n",
    "# ---------------------------------------------------------\n",
    "IsolationForest had the highest marginal performance compared to OneClassSVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addtional Tasks\n",
    "Vary various parameters and performance measures for the above practice and see the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
