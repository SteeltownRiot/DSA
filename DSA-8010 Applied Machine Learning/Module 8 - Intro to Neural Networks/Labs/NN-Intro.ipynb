{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neuron\n",
    "\n",
    "Recall the concept of a [neuron](https://en.wikipedia.org/wiki/Artificial_neuron) based on its mathematical formula.\n",
    "\n",
    "$$ y_k = \\varphi \\left( \\sum_{j=0}^{m}{w_{kj}x_j} +b_k \\right) $$\n",
    "\n",
    "This is a simple **linear** neuron. If you look closely, you will see the formula for multiple linear regression (if $\\varphi$ is removed)! If $\\varphi$ is a sigmoid funciton then it becomes the formula for losistic regression. \n",
    "\n",
    "PyTorch, as well as other NN packages, support numerous types of neurons. Typically, neurons are composed into layers, and a single layer has only a single type of neuron.\n",
    "\n",
    "In this lab, we devlop linear regression and logistic regression models with neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import scale, LabelBinarizer, StandardScaler, Normalizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Multiple Regression Model using Neural Network\n",
    "\n",
    "Let's explore the Boston housing dataset which is used in a regression setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_boston()\n",
    "dataset = datasets.fetch_california_housing()\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['Price'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization/Normalization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(data_scaled, columns=list(dataset.feature_names) + ['Price'])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled.drop('Price', axis=1).to_numpy()\n",
    "y = df_scaled['Price'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a neural network\n",
    "\n",
    "Now we will construct a basic Neural Network with\n",
    " * One hidden layer fed by 13 input values (as there are 13 features)\n",
    " * One output layer \n",
    " \n",
    "##### Note: The summary will show that we have 16 total learnable parameters:\n",
    "  * 14 for the hidden layer (13 feature values and bias)\n",
    "  * 1 for the output layer (Hidden ($H_0$) and without bias) \n",
    "  \n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/reg_as_nn.jpg\" width=600 height=400 alt=\"figure alt text\">\n",
    "  <figcaption>\n",
    "      <b>Fig. A neural network for solving muliple regression problem.</b> <!-- can also use <div>, <p>, etc. tags within <figcaption> -->\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary pytorch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "One way to define a neural network in PyTorch is to subclass the `nn.Module` class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRegNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        D_in: number of input\n",
    "        H: number of nurons in the hidden layer\n",
    "        D_out: number of output\n",
    "        \"\"\"\n",
    "        super(MyRegNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_in, H) # input to hidden layer\n",
    "        self.layer2 = nn.Linear(H, D_out, bias=False) # input to hidden layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_pred = self.layer1(x)        # h = dot(input,w1) \n",
    "        y_pred = self.layer2(h_pred)   \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an instance of the network class we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a network with 13 inputs to 1 hidden neurons to one output neuron \n",
    "\n",
    "D_in, H, D_out = X_train.shape[1], 1, 1    \n",
    "\n",
    "net = MyRegNN(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize this model using `summary` function from `torchsummary` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, (X_train.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer has 9 parameters to be learned: 8 input has 8 coefficients and the intercept b_0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()   # Mean Squared error loss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.3)  \n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Before training the model, we need to convert the pandas/numpy datasets to pytorch's tensor data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors from the train/test set\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better iteration over the train/test sets, there are two handy methods: TensorDataset and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1  # it is possible to feed more than one istances to the model. \n",
    "# These set of instances is called batch. For simplicity, let's keep one instance per batch\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the mdoel with 100 epochs. The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. Within an epoch each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. \n",
    "\n",
    "Note: For simplicity we are skipping k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100  # In each epoch, the model iterate over all the instances \n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = net(x)        # Forward pass: get the network output for this instance\n",
    "        l = loss(output, y)    # estimate error for this instance\n",
    "        epoch_loss += l.item() # Aggregate error\n",
    "        optimizer.zero_grad()  # As backward method accumulates gradient, we need to set it to 0\n",
    "        l.backward()           # Backward pass: Estimate gradient \n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch%5)==0:\n",
    "        print(f'Epoch {epoch+0:03}: | Total Loss: {epoch_loss:.5f} | Avg Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # notify all the layers that we are in eval mode\n",
    "\n",
    "with torch.no_grad(): \n",
    "    y_test_pred = net(X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "print(f\"R^2: {r2_score(y_test, y_test_pred.numpy())}\")\n",
    "print(f\"MSE:{mean_squared_error(y_test, y_test_pred.numpy())}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of MSE and R^2, the neural network performed better than the baseline which predicts mean as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Logistic Regression Model using Neural Network\n",
    "\n",
    "For this lab, we will use sklearn breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['class'] = cancer.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization/Normalization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1).to_numpy()\n",
    "y = df['class'].to_numpy()\n",
    "\n",
    "scaler = Normalizer()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=list(cancer.feature_names))\n",
    "df_scaled['class'] = y\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "df_scaled['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a neural network\n",
    "\n",
    "Now we will construct a basic Neural Network with\n",
    " * One hidden layer fed by 30 input values (as there are 30 features)\n",
    " * One output layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogitNN(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        D_in: number of input\n",
    "        \"\"\"\n",
    "        super(MyLogitNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_in, H) # input to hidden layer\n",
    "        self.layer2 = nn.Linear(H, D_out, bias=False) # input to hidden layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_pred = self.layer1(x)        \n",
    "        y_pred = torch.sigmoid(self.layer2(h_pred))   \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an instance of the network class we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a network with 13 inputs to 1 hidden neurons to one output neuron \n",
    "\n",
    "D_in, H, D_out = X_train.shape[1], 1, 1    \n",
    "\n",
    "net = MyLogitNN(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize this model using `summary` function from `torchsummary` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, (X_train.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.MSELoss()   \n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.3)  \n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Before training the model, we need to convert the pandas/numpy datasets to pytorch's tensor data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors from the train/test set\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better iteration over the train/test sets, there are two handy methods: TensorDataset and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1  # it is possible to feed more than one istances to the model. \n",
    "# These set of instances is called batch. For simplicity, let's keep one instance per batch\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the mdoel with 100 epochs. The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. Within an epoch each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. \n",
    "\n",
    "Note: For simplicity we are skipping k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100  # In each epoch, the model iterate over all the instances \n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        output = net(x)        # Forward pass: get the network output for this instance\n",
    "        l = loss(output, y)    # estimate error for this instance\n",
    "        epoch_loss += l.item() # Aggregate error\n",
    "        optimizer.zero_grad()  # As backward method accumulates gradient, we need to set it to 0\n",
    "        l.backward()           # Backward pass: Estimate gradient \n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch%5)==0:\n",
    "        print(f'Epoch {epoch+0:03}: | Total Loss: {epoch_loss:.5f} | Avg Loss: {epoch_loss/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # notify all the layers that we are in eval mode\n",
    "\n",
    "with torch.no_grad(): \n",
    "    y_test_pred = net(X_test_tensor)\n",
    "    \n",
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_class = torch.round(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_test_pred_class.numpy())}\")\n",
    "print(f\"Classification Report:\\n {classification_report(y_test, y_test_pred_class.numpy())}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we learned a step-by-step process for developing neural networks for solving regression and classification problems. These are elementary neural networks, but the process is similar even if our network architecture has more layers/neurons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PyTorch API and helpful links\n",
    "\n",
    " * Layers: https://pytorch.org/docs/stable/nn.html\n",
    " * Loss / Loss Functions : [link1](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7) [link2](https://neptune.ai/blog/pytorch-loss-functions)\n",
    " * Optimizers (learning algorithm) : https://pytorch.org/docs/stable/optim.html\n",
    " * Neuron Activation Functions : https://towardsdatascience.com/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-1-7d8ade494cee\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please restart the kernel and clear all output, then play around with parameters or add cells and create additional notebooks\n",
    "\n",
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
