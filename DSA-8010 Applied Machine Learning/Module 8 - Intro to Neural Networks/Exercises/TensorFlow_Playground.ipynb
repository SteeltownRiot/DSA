{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 : Tensor Flow Playground Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you will focus on interpretation and understanding of neural network behavior in the context of:\n",
    " 1. Input (Data)\n",
    " 1. Architecture\n",
    " 1. Activation Functions\n",
    " 1. Resulting Decision Planes\n",
    " 1. Hyper-parameter Tuning\n",
    "\n",
    "The link below should open the TensorFlow NN Playground to the simple data set.\n",
    "\n",
    "![TFPG_DS_2_blobs.png MISSING](../images/TFPG_DS_2_blobs.png)\n",
    "\n",
    "[Access the TensorFlow Playground](https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.00001&regularizationRate=0&noise=0&networkShape=1,1&seed=0.44359&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "![data](../images/Gaussian-Data.png)\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    " * Learnign Rate : <span style=\"background:yellow\">0.003</span> \n",
    " * Activation : <span style=\"background:yellow\">Linear</span>  \n",
    " * Regularization : <span style=\"background:yellow\">None</span>  \n",
    " * Regularization Rate : <span style=\"background:yellow\">0</span>  \n",
    " \n",
    " \n",
    "**Your hyperparameter bar should like below**\n",
    "![TFPG_HyperParameter_Bar.png MISSING](../images/TFPG_HyperParameter_Bar.png) \n",
    "\n",
    "\n",
    "#### Architecture:\n",
    " Start with a 2 hidden layers, one neuron each, only taking as input $X_1$ and $X_2$.\n",
    "\n",
    "![TFPG_E1_Architecture.png MISSING](../images/TFPG_E1_Architecture.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task E1\n",
    "  When you are ready, click the play button and watch the model's **training loss**.\n",
    "  When the loss reaches 0.0, pause the training (clicking the button again).\n",
    "  \n",
    "  The result should look similar to:\n",
    "  \n",
    "  ![TFPG_E1_sample_output.png](../images/TFPG_E1_sample_output.png)\n",
    "  \n",
    "  **Q**: How many epochs did you need to get the training loss down to 0.0?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E1: Write your Answer Below\n",
    "# ----------------------------\n",
    "602\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture a screen shot of your trained model, upload the screenshot, and embed it \n",
    "\n",
    "**In this Cell, Double Click**, then Adjust the file name at the bottom image markdown after you upload it to the `module8/exercises` folder\n",
    " * For example, you may need to adjust the file extension.\n",
    " \n",
    "```\n",
    "![M8_E1_image.png MISSING](./M8_E1_image.png) --> (./M8_E1_image.jpeg)\n",
    "```\n",
    "\n",
    "![Me E1 image MISSING](./M8_E1_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Question 2\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    " * Learnign Rate : <span style=\"background:yellow\">0.003</span> \n",
    " * **Activation** : <span style=\"background:yellow\">Sigmoid</span>  \n",
    " * Regularization : <span style=\"background:yellow\">None</span>  \n",
    " * Regularization Rate : <span style=\"background:yellow\">0</span>  \n",
    " \n",
    "\n",
    "#### Architecture:\n",
    " * Input : Only $X_1$ and $X_2$\n",
    " * 1 first hidden layer neuron \n",
    " * 1 second hidden layer neuron \n",
    "\n",
    "\n",
    "#### Task E2:\n",
    "  When you are ready, click the play button and watch the model's **training loss**.\n",
    "  When the loss reaches 0.0, pause the training (clicking the button again).\n",
    "  \n",
    "  **Q**: What is a difference you observe just by changing the activation function?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E2 Write your Answer Below\n",
    "# ----------------------------\n",
    "It takes orders of magnitude more epochs for the output to reach 0. This is because the y in a Sigmoid function over \"time\" (number of epochs) responds less and less to changes in X. This vanishing gradient means the neural network's learning slows down, approaching or becoming 0. I think this is what happened to me. I let it run for 10K epochs and it was still stuck on 0.001 where it had been since somewhere around 4K epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Question 3\n",
    "\n",
    "## Change to Four-Quadrant Data Set\n",
    "\n",
    "![TFPG_4_Quad_DS.png MISSING](../images/TFPG_4_Quad_DS.png)\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    " * Learnign Rate : <span style=\"background:yellow\">0.003</span> \n",
    " * **Activation** : <span style=\"background:yellow\">Linear</span>  \n",
    " * Regularization : <span style=\"background:yellow\">None</span>  \n",
    " * Regularization Rate : <span style=\"background:yellow\">0</span>  \n",
    " \n",
    "\n",
    "#### Architecture:\n",
    " * Input : Only $X_1$ and $X_2$\n",
    " * 1 first hidden layer neuron \n",
    " * 1 second hidden layer neuron \n",
    "\n",
    "#### Task E3:\n",
    "  When you are ready, click the play button and watch the model's **training loss**.\n",
    "  Let training run for 3,000 epochs.\n",
    "    \n",
    "  **Q**: Based on your readings and the videos, why did the NN not converge to near zero loss?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E3 Write your Answer Below\n",
    "# ----------------------------\n",
    "My NN's test and train loss started at 0.508 and 0.498 respectively. They both initially dipped down 1/1000, but eventually the test loss went back to 0.508, ending at 3,000 epochs as 0.508 and 0.497. The linear activation worked in the first question because the data points are pretty equally spread out on two sides where a linear plot can be made between them. So, just like non-linear activation functions the classification can wind up being equivalent to between 0 and 1. A linear activation function used with two-dimensional data or greater spread out in opposing quadrants may provide any number between -infinity to infinity and thus will never converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Question 4\n",
    "\n",
    "You will now adjust the activation type and the number of hidden layers to build a good classifier.\n",
    "Please keep track of what you try and log the efforts (Activation Fuction, Neuron Architecture, Final Loss, Epochs of training).\n",
    "\n",
    "**Constraint:** Try to keep the learning rate at 0.003 and only use the inputs $X_1$ and $X_2$.\n",
    "\n",
    "**Goal:** Get the training loss below 0.01; if you can down to 0.001.\n",
    "\n",
    "**Capture:** A screen shot of your final trained model.\n",
    "\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    " * Learnign Rate : <span style=\"background:yellow\">0.003</span> \n",
    " * **Activation** : <span style=\"background:yellow\"> Experiment</span>  \n",
    " * Regularization : <span style=\"background:yellow\">None</span>  \n",
    " * Regularization Rate : <span style=\"background:yellow\">0</span>  \n",
    " \n",
    "\n",
    "#### Architecture:\n",
    " * Input : Only $X_1$ and $X_2$\n",
    " * <span style=\"background:yellow\"> Experiment with </span> hidden layers \n",
    "\n",
    "\n",
    "#### Task E4:\n",
    "\n",
    "**For each architecture you evaluated**\n",
    " * **Q**: Describe your final architecture\n",
    " * **Q**: Detail your hyperparameters\n",
    " * **Q**: How many epochs, and what was your final loss\n",
    "  \n",
    "**Final Q**: What are you take-aways you have from this exercise in regards to architecture versus decision surfaces? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E4 Write your Answer Below\n",
    "# ----------------------------\n",
    "***NOTE: learning rate(0.003), regularization(none), regularizing rate(0), and input(X_1, X_2) used for all experiments***\n",
    "\n",
    "Experiment\tActivation\tLayers[neurons]\tEpochs\tTest Loss\tTraining Loss\tNotes\n",
    "==========\t==========\t===============\t======\t=========\t=============\t============================================\n",
    "1\t\t\tSigmoid\t\t2[1,1]\t\t\t1,000\t0.507\t\t0.498\n",
    "2\t\t\tSigmoid\t\t3[1,1,1]\t\t1,000\t0.507\t\t0.498\n",
    "3\t\t\tSigmoid\t\t6[1,1,1,1,1,1]\t1,000\t0.507\t\t0.498\n",
    "4\t\t\tTanh\t\t2[1,1]\t\t\t1,000\t0.392\t\t0.377\t\t\tCould only classify bottom right quadrant\n",
    "5\t\t\tTanh\t\t6[1,1,2,2,2,2]\t1,000\t0.507\t\t0.498\t\t\tCould only classify bottom right quadrant\n",
    "6\t\t\tTanh\t\t3[4,2,2]\t\t1,453\t0.003\t\t0.001\t\t\tClassified 4 quadrants well & less linearly\n",
    "7\t\t\tReLU\t\t2[1,1]\t\t\t1,000\t0.388\t\t0.372\t\t\tCould only classify bottom right quadrant\n",
    "8\t\t\tReLU\t\t3[4,3,1]\t\t1,288\t0.005\t\t0.001\t\t\tClassified 4 quadrants well\n",
    "9\t\t\tReLU\t\t3[6,4,4]\t\t1,143\t0.005\t\t0.001\t\t\tClassified 4 quadrants well & faster; more complex\n",
    "10\t\t\tReLU\t\t3[4,3,2]\t\t  722\t0.005\t\t0.001\t\t\tClassified the 4 quadrants really well & fastest\n",
    "\n",
    "Pairing the right architecture for the problem along with the right decision surfaces are equally important. A linear activation function is never going to find a solution with this dataset not matter how many layers or neurons you add, but neither the Rectified Linear Unit nor the hyperbolic tangent activation functions performed well until the decision surfaces were tuned as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture a screen shot of your final trained model and embed it \n",
    "\n",
    "\n",
    "![M8 E4 Image](./M8_E4_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Question 5\n",
    "\n",
    "Using the Architecture you found as best above, change data sets to the enclosing configuration.\n",
    "\n",
    "![TFPG_Enclosed_DS.png MISSING](../images/TFPG_Enclosed_DS.png)\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    "**<span style=\"background:yellow\">Experiment</span>**\n",
    " \n",
    "\n",
    "#### Architecture:\n",
    "**<span style=\"background:yellow\">Locked to Final Architecture of Exercise Question 4.</span>**\n",
    "\n",
    "\n",
    "#### Task E5\n",
    "  \n",
    "  **Q**: Provide a detailed assessment of what you observed and any changes you made to the hyper parameters.\n",
    " Pay special attention to discussion of the results decision surfaces, error, and epohcs.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E5 Write your Answer Below\n",
    "# ----------------------------\n",
    "***NOTE: input(X_1, X_2), regularization rate(0), layers(3), and neurons[4,3,2] used for all experiments***\n",
    "\n",
    "    Learning Rate\tActivation\tRegularization\tEpochs\tTest Loss\tTrain Loss\tNotes\n",
    "\t=============\t==========\t==============\t======\t=========\t==========\t===========================================\n",
    "1\t0.003\t\t\tTanh\t\tnone\t\t\t1,569\t0.004\t\t0.001\t\tGood classifying data points\n",
    "2\t0.03\t\t\tTanh\t\tnone\t\t\t  185\t0.002\t\t0.001\t\tGood classifying: really fast\n",
    "3\t0.3\t\t\t\tTanh\t\tnone\t\t\t   38\t0.004\t\t0.001\t\tDecent classifying; silly fast\n",
    "4\t1\t\t\t\tTanh\t\tnone\t\t\t  195\t0.008\t\t0.000\t\tOK classifying; really fast; wild swings early\n",
    "5\t0.03\t\t\tReLU\t\tnone\t\t\t  130\t0.388\t\t0.001\t\tOK classifying; really fast\n",
    "6\t0.3\t\t\t\tReLU\t\tnone\t\t\t   17\t0.005\t\t0.001\t\tDecent classifying; ridiculosly fast\n",
    "7\t0.3\t\t\t\tReLU\t\tL1\t\t\t\t   25\t0.002\t\t0.001\t\tDecent classifying; stupid fast\n",
    "8\t0.3\t\t\t\tTanh\t\tL1\t\t\t\t   32\t0.012\t\t0.001\t\tOK classifying; silly fast; wild swings\n",
    "9\t0.3\t\t\t\tReLU\t\tL2\t\t\t\t    8\t0.008\t\t0.001\t\tDecent classifying; went plaid;\n",
    "10\t1\t\t\t\tReLU\t\tL2\t\t\t\t    9\t0.002\t\t0.001\t\tGood classifying; went plaid;\n",
    "11\t0.003\t\t\tReLU\t\tL2\t\t\t\t  924\t0.005\t\t0.001\t\tGood classifying\n",
    "12\t0.03\t\t\tReLU\t\tL2\t\t\t\t  147\t0.004\t\t0.001\t\tGood classifying\n",
    "\n",
    "Regularization really made a big difference in the number of epochs necessary to reach a minimum loss and are great at mitigating the risk of overfitting. All of the \"fastest\" experiments used a larger learning rate, which can be problematic as it may result in learning a sub-optimal set of weights too fast or create an unstable training process. On the other hand, if the learning rate is too small, the training process could become unworkably long or even get stuck. One has to balance those risks against requirements, be they for speed, confidence, or avoiding errors. I would likely choose the hyperparameters from experiment 12. It is not very complex so decreases cost, requires less than 150 epochs, mitigates overfitting, and avoids errors thus increasing accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture a screen shot of your trained model and embed it \n",
    "\n",
    "\n",
    "![M8_E5_image.png MISSING](./M8_E5_image.png)\n",
    "![M8_E5_2_image.png MISSING](./M8_E5_2_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# M5 Exercise Question 6\n",
    "\n",
    "**Starting** with the Architecture you found as best in Exercise Q6, change data sets to the spiral configuration.\n",
    "\n",
    "![TFPG_Enclosed_DS.png MISSING](../images/TFPG_Spiral_DS.png)\n",
    "\n",
    "\n",
    "#### Set the hyperparameters as follows:\n",
    "**<span style=\"background:yellow\">Experiment</span>**\n",
    "\n",
    "#### Architecture:\n",
    "**<span style=\"background:yellow\">Locked to Final Architecture of Exercise Question 4.</span>**\n",
    "\n",
    "\n",
    "#### Task E6\n",
    "  Your goal is the get the lowest training and testing loss with the fewest epochs.\n",
    "  Do what ever it takes!\n",
    "  \n",
    "  **Q**: Describe your \"fun\" with this question!\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# E6 Write your Answer Below\n",
    "# ----------------------------\n",
    "I tried 20 different configurations of hyperparameters and even messed with the biases and weights and the only one that really got close came in at almost 6.5K epochs and a training loss of 0.049.\n",
    "\n",
    "After getting this result I messed with the architecture, Intuitively I looked at the swirl and thought it would need a series of 6 and 4 neurons to account for the color changes accross the swirl and the colors as counted from center respectively. The second image shows the best result of those experiments. The training was more unstable but did a pretty good job in the end.\n",
    "\n",
    "I am definitely going to play around with this more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture a screen shot that includes data set, hyperparameters, architecture, and results model (decision surfaces).\n",
    "\n",
    "![M8_E6_image.png MISSING](./M8_E6_image.png)\n",
    "\n",
    "##### Did some more experimentation. To get my absolute best result I had to mess with the architecture.\n",
    "![M8_E6_4_image.png MISSING](./M8_E6_4_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
