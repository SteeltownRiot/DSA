{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 : Classifying Iris with Neural Networks\n",
    "\n",
    "In this session, you will create a Multilayer Perceptron (MLP) model to practice on the Iris dataset, to get more familiarized with PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some initial library loading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-496ebb3323ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale, LabelEncoder, StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in this dataset\n",
    "print('features', iris.feature_names)\n",
    "\n",
    "# Target classes of this 3-class prediction problem.\n",
    "print('targets', iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Assign features and the target to variables X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P01)\n",
    "# ----------------------------------\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a little more inspection\n",
    "\n",
    "Is `X` a pandas dataframe or a numpy array? What kind of data does it contain? Let's run some descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('type', type(X))\n",
    "print('shape', X.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify to make that more readable.\n",
    "\n",
    "for stat, val in stats.describe(X)._asdict().items():\n",
    "    if stat!='minmax':\n",
    "        print('{:<10}: {}'.format(stat, val))\n",
    "    else:\n",
    "        print('{:<10}: {}'.format('min', val[0]))\n",
    "        print('{:<10}: {}'.format('max', val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print('type: ', type(y))\n",
    "print('shape: ', y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Data \n",
    "\n",
    "We see above that the data features are 4-D.\n",
    "Generate two visualizations of the data using Red, Blue, Green for the colors of\n",
    "`setosa`, `versicolor`, `virginica`, respectively.\n",
    "\n",
    "#### Task 2: First, visualize along dimensions 0,1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P02)\n",
    "# ----------------------------------\n",
    "\n",
    "#'sepal length (cm)', 'sepal width (cm)'\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\",\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Second, visualize along dimensions 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P03)\n",
    "# ----------------------------------\n",
    "\n",
    "# 'petal length (cm)', 'petal width (cm)'\n",
    "\n",
    "\n",
    "x_min, x_max = X[:, 2].min() - .5, X[:, 2].max() + .5\n",
    "y_min, y_max = X[:, 3].min() - .5, X[:, 3].max() + .5\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 2], X[:, 3], c=y, cmap=\"brg\",\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build am NN model\n",
    "\n",
    "#### Task 4: Define the neural architecture with PyTorch with the following config\n",
    "\n",
    "1. A hidden layer with 4 neurons\n",
    "2. Use sigmoid activation fuction for the hidden layer\n",
    "3. Convert the output layer values to probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        D_in: number of input\n",
    "        \"\"\"\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_in, H) # input to hidden layer\n",
    "        self.layer2 = nn.Linear(H, D_out) # hidden layer to output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_pred = torch.sigmoid(self.layer1(x)) # activation function after the first hidden layer. Change it to nn.ReLU to see the difference     \n",
    "        y_pred = torch.softmax(self.layer2(h_pred), dim=1) # softmax function: converts numbers to probabilities \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(X.shape[1], 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Compile the model with the `Cross Entropy` loss function and `rmsprop` optimizer.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P05)\n",
    "# ----------------------------------\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the Data into Train and Test splits\n",
    "\n",
    "To make this this more fun, we are going to use a 70%/30% split on the training and testing data.\n",
    "\n",
    "#### Task 6: Prepare the data for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P06)\n",
    "# ----------------------------------\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(iris.data)\n",
    "\n",
    "# train/test split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
    "\n",
    "# convert train/test to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "# convert y to one hot encoding: not required if we use cross entropy\n",
    "# y_train_one_hot = nn.functional.one_hot(y_train_tensor)\n",
    "# y_test_one_hot = nn.functional.one_hot(y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1  # it is possible to feed more than one istances to the model. \n",
    "# These set of instances is called batch. For simplicity, let's keep one instance per batch\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Train the model on the train data\n",
    "\n",
    "Choose an appropriate number of epochs and batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P07)\n",
    "# ----------------------------------\n",
    "\n",
    "N_EPOCHS = 100  # In each epoch, the model iterate over all the instances \n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for x, y in train_loader:\n",
    "        y_pred = model(x)      # Forward pass: get the network output for this instance\n",
    "        l = loss(y_pred, y.view(-1))    # estimate error for this instance\n",
    "        epoch_loss += l.item() # Aggregate error\n",
    "        \n",
    "        optimizer.zero_grad()  # As backward method accumulates gradient, we need to set it to 0\n",
    "        l.backward()           # Backward pass: Estimate gradient \n",
    "        optimizer.step()\n",
    "        \n",
    "        correct = (torch.argmax(y_pred, dim=1) == y).type(torch.FloatTensor)\n",
    "        epoch_acc += correct.item()\n",
    "\n",
    "    if (epoch%5)==0:\n",
    "        print(f'Epoch {epoch+0:03}: | Total Loss: {epoch_loss:.5f} | \\\n",
    "Avg Loss: {epoch_loss/len(train_loader):.5f} | Training Acc: {epoch_acc/len(train_loader):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Task 8: Evaluate your trained model on the on the test data\n",
    "\n",
    "Print the loss and accuracy obtained using model.evaluate(...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P08)\n",
    "# ----------------------------------\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Interpret the classifier output\n",
    "\n",
    "Run the cell below, then provide your interpretation of the output in the cell below that.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add interpretation below this comment  (Question #P09)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Print the predicted Class Label, then generate a confusion matrix and flassification report\n",
    " * Review the documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix code below this comment  (Question #P10)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_class = np.argmax(y_pred, axis=1)\n",
    "print(pred_class)\n",
    "\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, pred_class))\n",
    "print(classification_report(y_test, pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## This notebook should include things that you are adding to your toolchest in terms of using Scikit-Learn and PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
