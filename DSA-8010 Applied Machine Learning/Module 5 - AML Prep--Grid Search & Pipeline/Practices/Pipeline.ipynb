{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline: Practice\n",
    "\n",
    "We will use sklearn pipeline to build a model sequentially. The purpose of pipeline is to use apply several steps sequentially in a combined manner rather doing one by one. In this Lab we will build a simple pipeline and will also use random serach on the pipeline for hyperparmeter optimization. \n",
    "\n",
    "* [Sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "For this we use the breast cancer dataset from sklearn load_breast_cancer. We will train a svm model. But before this we will apply  min_max_scalar for scaling and PCA for feature reduction. We will do this using sklearn pipeline in a single step rather doing those separately. \n",
    "\n",
    "Later we will do random search on the pipeline for hyperparmeter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset.\n",
    "We are using the breast cancer dataset. A modified version of the dataset is already available in the sklearn dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample and Features: (569, 30)\n",
      "Target class: ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print('Sample and Features:', cancer_data.data.shape)\n",
    "print('Target class:', cancer_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "\n",
    "Split the dataset for testing and training purpose. We are spliting the dataset to training (80%) and testing (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset (P101)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Building a Pipeline \n",
    "We will build a pipeline which will use MinMaxScalar for data scaling, PCA for reducing the dimentionality of the features, and then a classifier for training and predicting with the data.\n",
    "\n",
    "## Defining the segments of the pipe\n",
    "\n",
    "Here we define a pipeline as an ordered list of classes that will take data.\n",
    "\n",
    "In the example below:\n",
    "\n",
    "  1. Data --> Scale --> Scaled_Features\n",
    "  2. Scaled_Features --> PCA --> Data_Features\n",
    "  3. Data_Features --> LinearSVC --> Classifications\n",
    "\n",
    "Therefore, \n",
    "\n",
    "  1. Data --> Pipeline --> Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stage 1, set the pca_components\n",
    "pca_components = 20\n",
    "\n",
    "# Define the pipeline (P102)\n",
    "pipe = Pipeline([\n",
    "    ('Scale', MinMaxScaler()), \n",
    "    ('PCA', PCA(n_components = pca_components)), \n",
    "    ('SVC', SVC(kernel = 'rbf'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Scale', MinMaxScaler()), ('PCA', PCA(n_components=20)),\n",
       "                ('SVC', SVC())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline (P103)\n",
    "pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct prediction:  112 \n",
      "Total test set:  114\n"
     ]
    }
   ],
   "source": [
    "# Predict with the test set (P104)\n",
    "preidcted_y = pipe.predict(X_test)\n",
    "\n",
    "# Check the correct labels\n",
    "correct_prediction = np.sum(preidcted_y  == y_test)\n",
    "\n",
    "print('Total correct prediction: ', correct_prediction, '\\nTotal test set: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation\n",
    "Score function of the pipeline provides the accuracy of the trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the score of the pipeline (P105)\n",
    "pipe.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        43\n",
      "           1       0.99      0.99      0.99        71\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report (P106)\n",
    "print(classification_report(y_test, preidcted_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  1]\n",
      " [ 1 70]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix (P107)\n",
    "print(confusion_matrix(y_test, preidcted_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: RandomSearch over a pipeline \n",
    "\n",
    "It's awesome to have a single pipeline and do preprocessing and train at once. But its not a good idea to use manual params for the each part of the pipeline. One more interesting part is that we could perform `GridSearch` and `RandomSearch` over a pipeline for hyper parameter tuning. \n",
    "\n",
    "To perform the hyperparameter tuning over a pipeline, we need to concatenate the model name as a prefix of param name with underscore `_`. For example, if we want to do `RandomSearch` over the `kernel` params of  `SVC`,  then the name of this parameter in the configuration will be `SVC_kernel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# configure parameters for randomsearch (P108)\n",
    "\n",
    "# select params list to do random search here. all the pramas \n",
    "# name is concatenated with __ preceding the model name\n",
    "\n",
    "param_grid = {'PCA__n_components': [20], \n",
    "              'SVC__C': uniform(1e3, 5e3), \n",
    "              'SVC__gamma': uniform(0, 0.1), \n",
    "              'SVC__kernel': ['rbf']}\n",
    "\n",
    "# Now build the pipeline again (P109)\n",
    "clf_pipe = Pipeline([\n",
    "    ('Scale', MinMaxScaler()),\n",
    "    ('PCA', PCA()), \n",
    "    ('SVC', SVC())\n",
    "])\n",
    "\n",
    "# Now define a random search with the pipe (P110)\n",
    "rand_model = RandomizedSearchCV(clf_pipe, param_distributions = param_grid, cv = 5, n_jobs = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('Scale', MinMaxScaler()), ('PCA', PCA(n_components=20)),\n",
      "                ('SVC', SVC(C=1448.2856706182731, gamma=0.0462140887665545))])\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline (P111)\n",
    "rand_model.fit(X_train, y_train)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(rand_model.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        43\n",
      "           1       1.00      0.99      0.99        71\n",
      "\n",
      "    accuracy                           0.99       114\n",
      "   macro avg       0.99      0.99      0.99       114\n",
      "weighted avg       0.99      0.99      0.99       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report (P112)\n",
    "predicted_y = rand_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  43   0\n",
       "1   1  70"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix (P113)\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
