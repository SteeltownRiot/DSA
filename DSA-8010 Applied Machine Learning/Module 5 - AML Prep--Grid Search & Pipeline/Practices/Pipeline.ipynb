{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline: Practice\n",
    "\n",
    "We will use sklearn pipeline to build a model sequentially. The purpose of pipeline is to use apply several steps sequentially in a combined manner rather doing one by one. In this Lab we will build a simple pipeline and will also use random serach on the pipeline for hyperparmeter optimization. \n",
    "\n",
    "* [Sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "For this we use the breast cancer dataset from sklearn load_breast_cancer. We will train a svm model. But before this we will apply  min_max_scalar for scaling and PCA for feature reduction. We will do this using sklearn pipeline in a single step rather doing those separately. \n",
    "\n",
    "Later we will do random search on the pipeline for hyperparmeter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset.\n",
    "We are using the breast cancer dataset. A modified version of the dataset is already available in the sklearn dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample and Features:', cancer_data.data.shape)\n",
    "print('Target class:', cancer_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "\n",
    "Split the dataset for testing and training purpose. We are spliting the dataset to training (80%) and testing (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset (P101)\n",
    "X_train, X_test, y_train, y_test = <placeholder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Building a Pipeline \n",
    "We will build a pipeline which will use MinMaxScalar for data scaling, PCA for reducing the dimentionality of the features, and then a classifier for training and predicting with the data.\n",
    "\n",
    "## Defining the segments of the pipe\n",
    "\n",
    "Here we define a pipeline as an ordered list of classes that will take data.\n",
    "\n",
    "In the example below:\n",
    "\n",
    "  1. Data --> Scale --> Scaled_Features\n",
    "  2. Scaled_Features --> PCA --> Data_Features\n",
    "  3. Data_Features --> LinearSVC --> Classifications\n",
    "\n",
    "Therefore, \n",
    "\n",
    "  1. Data --> Pipeline --> Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stage 1, set the pca_components\n",
    "pca_components = 20\n",
    "\n",
    "# Define the pipeline (P102)\n",
    "pipe = Pipeline([<placeholder>])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline (P103)\n",
    "pipe.fit(<placeholder>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the test set (P104)\n",
    "preidcted_y = pipe.predict(<placeholder>)\n",
    "\n",
    "# Check the correct labels\n",
    "correct_prediction = np.sum(preidcted_y  == y_test)\n",
    "\n",
    "print('Total correct prediction: ', correct_prediction, '\\nTotal test set: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation\n",
    "Score function of the pipeline provides the accuracy of the trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score of the pipeline (P105)\n",
    "pipe.score(<placeholder>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report (P106)\n",
    "print(classification_report(<placeholder>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (P107)\n",
    "print(confusion_matrix(<placeholder>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: RandomSearch over a pipeline \n",
    "\n",
    "It's awesome to have a single pipeline and do preprocessing and train at once. But its not a good idea to use manual params for the each part of the pipeline. One more interesting part is that we could perform `GridSearch` and `RandomSearch` over a pipeline for hyper parameter tuning. \n",
    "\n",
    "To perform the hyperparameter tuning over a pipeline, we need to concatenate the model name as a prefix of param name with underscore `_`. For example, if we want to do `RandomSearch` over the `kernel` params of  `SVC`,  then the name of this parameter in the configuration will be `SVC_kernel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# configure parameters for randomsearch (P108)\n",
    "\n",
    "# select params list to do random search here. all the pramas \n",
    "# name is concatenated with __ preceding the model name\n",
    "\n",
    "param_grid = <placeholder>\n",
    "\n",
    "\n",
    "# Now build the pipeline again (P109)\n",
    "clf_pipe = Pipeline([\n",
    "    <placeholder>\n",
    "])\n",
    "\n",
    "# Now define a random search with the pipe (P110)\n",
    "rand_model = <placeholder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the pipeline (P111)\n",
    "rand_model.fit(<placeholder>)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(rand_model.<placeholder>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report (P112)\n",
    "predicted_y = <placeholder>\n",
    "\n",
    "print(classification_report(<placeholder>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (P113)\n",
    "pd.DataFrame(<placeholder>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
