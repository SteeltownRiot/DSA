{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Designing a well-defined process for machine learning is a critical step that takes your models from an environment such as this into a reusable state. It is especially necessary to have provenance and repeatability as you search for optimal hyperparameters to set up your machine learning models.\n",
    "\n",
    "In this notebook, we will walk through a non-trivial image data analysis task. We will show how we can use **Grid Search** and **Random Search** for creating processes that would allow us to tune the parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Data\n",
    "\n",
    "(from SciKit Learn Docs)\n",
    "\n",
    "This dataset is a collection of JPEG pictures of famous people collected over the internet, \n",
    "all details are available on the official website:\n",
    "\n",
    "  *  http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Each picture is centered on a single face. \n",
    "The typical task is called Face Verification: \n",
    "given a pair of two pictures, \n",
    "a binary classifier must predict whether the two images are from the same person.\n",
    "\n",
    "An alternative task, \n",
    "Face Recognition or Face Identification is: \n",
    "given the picture of the face of an unknown person, \n",
    "identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\n",
    "\n",
    "Both Face Verification and Face Recognition are tasks that are typically \n",
    "performed on the output of a model trained to perform Face Detection. \n",
    "The most popular model for Face Detection is called Viola-Jones and is \n",
    "implemented in the OpenCV library. \n",
    "The LFW faces were extracted by this face detector from various online websites.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load some Libraries\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "#  Note: This is downloading 200 MB of data!  It may take a few moments.\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train and Test sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the feature vectors is too large. Let's use PCA to reduce the num of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components = 150  # Feel free to vary this number\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Transforming the input data \")\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have reduced the number of features to 150. Let's apply a classifer on this transformed data. We will be using SVC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Train an SVC model with Grid Search\n",
    "\n",
    "In the below code, we will be using _GridSearchCV_   \n",
    "\n",
    " * Please review the [Scikit Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The grid of hyperparameters: We are interested in parameters C and kernel\n",
    "param_grid = {'C': [1e3, 5e3],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "\n",
    "\n",
    "# Defien the grid search object\n",
    "clf = GridSearchCV(SVC(class_weight='balanced'), param_grid)\n",
    "                                                             \n",
    "# Fit the grid search         \n",
    "clf = clf.fit(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above GridSearch method learns 4 different models. We can learn about these models by probing `cv_results_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from `cv_results_` that two models gave the same score in cross validated results: SVC with `{'C': 1000.0, 'kernel': 'rbf'}` and SVC with `{'C': 5000.0, 'kernel': 'rbf'}`. Here the first one is chosen as the best model. The following table shows the rank of each model in terms of cross-validated test scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        'Model': clf.cv_results_['params'],\n",
    "        'Mean Test Score': clf.cv_results_['mean_test_score'],\n",
    "        'Std Test Score': clf.cv_results_['std_test_score'],\n",
    "        'Rank': clf.cv_results_['rank_test_score']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the best model. \n",
    "\n",
    "As this `GridSearchCV` stores the best model, we can simply call `predict` method on this object for evaluating a test set. We will now evaluate the best model in terms of precision, recall, f-1 scores, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_pca)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "In this visualization, we show first 12 images from the test set and their predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show the first 12 PCA components for these faces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Train an SVC model with Random Search\n",
    "\n",
    "Instead of using grid search we can also use sklearn **random search** to perform hyper parameter tuning. Random search samples a parameter from a parameter distribution/list to identify the best params. sklearn has function `RandomizedSearchCV` for searching through given params distribution. In the following code `n_iter` denotes the number of parameter settings that are tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# parameter list and their distirbutions\n",
    "param_grid = {'C': uniform(loc=10, scale = 1000),\n",
    "              'kernel': ['linear', 'rbf']\n",
    "              }\n",
    "\n",
    "\n",
    "# build the Random searchable model.\n",
    "random_clf = RandomizedSearchCV(SVC(kernel='rbf', class_weight='balanced'), \n",
    "                                param_grid, n_iter=10, n_jobs=5)\n",
    "\n",
    "\n",
    "\n",
    "# train the model. it will figure out the best hyper parameter for this dataset\n",
    "random_clf = random_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search may not give us the best parameter setting for the model. But the key reason for using random search is to figure out a sample point in some distribution with a good result when it is hard to manually pick some values for a parameter. Once we have found good setting with random search, we can narrow down our search for getting a better parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_clf.predict(X_test_pca)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
