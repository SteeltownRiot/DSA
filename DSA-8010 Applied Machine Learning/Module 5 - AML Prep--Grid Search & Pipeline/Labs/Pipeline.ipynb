{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline\n",
    "\n",
    "We use sklearn pipeline to build a model sequentially. The purpose of pipeline is to apply several steps sequentially in a combined manner rather doing one by one. In this Lab we will build a simple pipeline and will also use grid serach on the pipeline for hyperparmeter optimization. \n",
    "\n",
    "* [Sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "For this we use the digits dataset from sklearn load_digits. We will train a svm model. But before this we will apply PCA for feature reduction. We will do this using sklearn pipeline in a single step rather doing those separately. Later we apply grid search on the pipeline for hyperparmeter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset.\n",
    "We are using the MNIST digit dataset. A modified version of digit dataset is already available in the sklearn dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample and Features: ', digits.data.shape)\n",
    "print('Target class:', digits.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Randomly visualize some sample from the datatset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "row = 5\n",
    "col = 5\n",
    "total_sample = len(digits.data)\n",
    "\n",
    "pyplot.subplots_adjust(bottom=0, left=.01, right=.99, top=2, hspace=.35)\n",
    "\n",
    "\n",
    "for i in range(row*col):\n",
    "    pyplot.subplot(row, col, i+1)\n",
    "    random_index = np.random.randint(0, total_sample)\n",
    "    single_label = np.copy(digits.data[random_index, :])\n",
    "    single_label = single_label.reshape((8,8))\n",
    "    pyplot.title(digits.target[random_index], size=12)\n",
    "    pyplot.xticks(())\n",
    "    pyplot.yticks(())\n",
    "    pyplot.imshow(single_label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "Split the dataset for testing and training purpose. We are spliting the dataset to training (80%) and  testing (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Building a Vanilla Pipeline \n",
    "We will build a pipeline which first use PCA to reduce the dimentionality of the features data and then will use a classifier to train and predict the data.\n",
    "\n",
    "## Defining the segments of the pipe\n",
    "\n",
    "Here we define a pipeline as an ordered list of classes that will work on the data.\n",
    "\n",
    "In the example below:\n",
    "\n",
    "  1. Data --> PCA --> Data_Features\n",
    "\n",
    "  2. Data_Features --> LinearSVC --> Classifications\n",
    "\n",
    "Therefore, \n",
    "  1. Data --> Pipeline --> Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stage 1 we set the pca_components with a fixed value 20\n",
    "pca_components = 20\n",
    "\n",
    "# This pipeline will run the PCA first then use the output of the PCA as the input of the SVC model\n",
    "pipe = Pipeline([\n",
    "    ('PCA', PCA(n_components= pca_components)), # it will reduce the feature vector to size of 20\n",
    "    ('SVC', SVC(kernel='rbf'))                  # Then it will train an SVC with the reduced 20 size feature vector\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit function will first run the pca and reduce the feature\n",
    "# then train the SVM\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict from test values\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# confusion_matrix \n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline score\n",
    "Score function of Pipeline will provide the accuracy of the trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show classification report \n",
    "It shows the precision, recall, and f1 for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: GridSearch over a Pipeline\n",
    "\n",
    "It's awesome to have a single pipeline and do preprocessing and train at once. But its not a good idea to use manual params for the each part of the pipeline. One more interesting part is that we could perform `GridSearch` and `RandomSearch` over a pipeline for hyper parameter tuning. \n",
    "\n",
    "To perform the hyperparameter tuning over a pipeline, we need to concatenate the model name as a prefix of param name with underscore `_`. For example, if we want to do `GridSearch` over the `kernel` params of  `SVC`,  then the name of this parameter in the configuration will be `SVC_kernel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# select params list to do grid search here all the pramas name is concatenated with __ preceding the model name\n",
    "\n",
    "# param configuration\n",
    "param_grid = {'PCA__n_components': [20, 30],\n",
    "              'SVC__C': [1e3, 5e3],        \n",
    "              'SVC__kernel': ['rbf']}\n",
    "\n",
    "\n",
    "# Now build the pipeline again\n",
    "clf_pipe = Pipeline([\n",
    "    ('PCA', PCA()), \n",
    "    ('SVC', SVC())                 \n",
    "])\n",
    "\n",
    "# Now apply the params to the classifier pipe using grid search\n",
    "grid_model = GridSearchCV(clf_pipe, param_grid = param_grid, n_jobs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It will try out all the combination of parameters and choose the best params set \n",
    "grid_model.fit(X_train, y_train)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(grid_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = grid_model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
