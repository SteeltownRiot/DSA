{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline: Practice\n",
    "\n",
    "We will use sklearn pipeline to build a model sequentially. The purpose of pipeline is to use apply several steps sequentially in a combined manner rather doing one by one. In this Lab we will build a simple pipeline and will also use random serach on the pipeline for hyperparmeter optimization. \n",
    "\n",
    "* [Sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "For this we use the breast cancer dataset from sklearn load_breast_cancer. We will train a svm model. But before this we will apply  min_max_scalar for scaling and PCA for feature reduction. We will do this using sklearn pipeline in a single step rather doing those separately. \n",
    "\n",
    "Later we will do random search on the pipeline for hyperparmeter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset.\n",
    "We are using the breast cancer dataset. A modified version of the dataset is already available in the sklearn dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample and Features:', cancer_data.data.shape)\n",
    "print('Target class:', cancer_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "\n",
    "Split the dataset for testing and training purpose. We are spliting the dataset to training (80%) and testing (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset (P101)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Building a Pipeline \n",
    "We will build a pipeline which will use MinMaxScalar for data scaling, PCA for reducing the dimentionality of the features, and then a classifier for training and predicting with the data.\n",
    "\n",
    "## Defining the segments of the pipe\n",
    "\n",
    "Here we define a pipeline as an ordered list of classes that will take data.\n",
    "\n",
    "In the example below:\n",
    "\n",
    "  1. Data --> Scale --> Scaled_Features\n",
    "  2. Scaled_Features --> PCA --> Data_Features\n",
    "  3. Data_Features --> LinearSVC --> Classifications\n",
    "\n",
    "Therefore, \n",
    "\n",
    "  1. Data --> Pipeline --> Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stage 1, set the pca_components\n",
    "pca_components = 20\n",
    "\n",
    "# Define the pipeline (P102)\n",
    "pipe = Pipeline([\n",
    "    ('scale', MinMaxScaler()),                  # Scale the data\n",
    "    ('PCA', PCA(n_components= pca_components)), # it will reduce the fature vector to size of 20\n",
    "    ('SVC', SVC(kernel='rbf'))                  # Then it will train an SVC with the reduced 20 size feature vector\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline (P103)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the test set (P104)\n",
    "preidcted_y = pipe.predict(X_test)\n",
    "\n",
    "# Check the correct labels\n",
    "correct_prediction = np.sum(preidcted_y  == y_test)\n",
    "\n",
    "print('Total correct prediction: ', correct_prediction, '\\nTotal test set: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation\n",
    "Score function of the pipeline provides the accuracy of the trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score of the pipeline (P105)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report (P106)\n",
    "print(classification_report(y_test, preidcted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (P107)\n",
    "print(confusion_matrix(y_test, preidcted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: RandomSearch over a pipeline \n",
    "\n",
    "It's awesome to have a single pipeline and do preprocessing and train at once. But its not a good idea to use manual params for the each part of the pipeline. One more interesting part is that we could perform `GridSearch` and `RandomSearch` over a pipeline for hyper parameter tuning. \n",
    "\n",
    "To perform the hyperparameter tuning over a pipeline, we need to concatenate the model name as a prefix of param name with underscore `_`. For example, if we want to do `RandomSearch` over the `kernel` params of  `SVC`,  then the name of this parameter in the configuration will be `SVC_kernel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# configure parameters for randomsearch (P108)\n",
    "\n",
    "# select params list to do random search here all the pramas \n",
    "# name is concatenated with __ preceding the model name\n",
    "\n",
    "# due to cpu resource allocation we are only using single options for grid search\n",
    "param_grid = {'SVC__C': uniform(1000, 100000), # select randomly from unifrom distribution of (1000, 1000 + 100000) range\n",
    "              'SVC__gamma': uniform(0, 0.1), \n",
    "              'PCA__n_components': [20],\n",
    "              'SVC__kernel': ['rbf']}\n",
    "\n",
    "\n",
    "# Now build the pipeline again (P109)\n",
    "clf_pipe = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('PCA', PCA()), # it will reduce the fature vector to size of 20\n",
    "    ('SVC', SVC())                  # Then it will train an SVC with the reduced 20 size feature vector\n",
    "])\n",
    "\n",
    "# Now define a random search with the pipe (P110)\n",
    "rand_model = RandomizedSearchCV(clf_pipe, param_distributions = param_grid, n_jobs=5, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the random search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the pipeline (P111)\n",
    "rand_model.fit(X_train, y_train)\n",
    "\n",
    "# Check the best choosen params\n",
    "print(rand_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report (P112)\n",
    "predicted_y = rand_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (P113)\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
