{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we aim to train a pipeline along with parameter tuning and perform prediction with the best model. More specifically we accomplish the following tasks: \n",
    "\n",
    "1. Develop a pipeline with feature extraction and linear regression models  \n",
    "2. Perform grid search with the pipeline for optimizing some hyper-parameters\n",
    "3. Evaluate the best model's performance\n",
    "4. Identify the second best model \n",
    "\n",
    "\n",
    "The dataset we will be using for the dataset is face dataset.\n",
    "\n",
    "## Face Data\n",
    "\n",
    "(from SciKit Learn Docs)\n",
    "\n",
    "This dataset is a collection of JPEG pictures of famous people collected over the internet, \n",
    "all details are available on the official website:\n",
    "\n",
    "  *  http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Each picture is centered on a single face. \n",
    "The typical task is called Face Verification: \n",
    "given a pair of two pictures, \n",
    "a binary classifier must predict whether the two images are from the same person.\n",
    "\n",
    "An alternative task, \n",
    "Face Recognition or Face Identification is: \n",
    "given the picture of the face of an unknown person, \n",
    "identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\n",
    "\n",
    "Both Face Verification and Face Recognition are tasks that are typically \n",
    "performed on the output of a model trained to perform Face Detection. \n",
    "The most popular model for Face Detection is called Viola-Jones and is \n",
    "implemented in the OpenCV library. \n",
    "The LFW faces were extracted by this face detector from various online websites.\n",
    "\n",
    "---\n",
    "\n",
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libs you need:\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "## Load necessary libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "Load the dataset from sklearn datasets using `fetch_lfw_people`. Print the number of samples, features, and  classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset:\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Some Sample Images\n",
    "Randomly sample 9 images from the dataset and visualize them in a 3X3 grid with their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize using matplot lib\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "row = 3\n",
    "col = 3\n",
    "total_sample = len(lfw_people.data)\n",
    "\n",
    "pyplot.subplots_adjust(bottom=0, left=.01, right=.99, top=2, hspace=.35)\n",
    "\n",
    "\n",
    "for i in range(row*col):\n",
    "    pyplot.subplot(row, col, i+1)\n",
    "    random_index = np.random.randint(0, total_sample)\n",
    "    single_label = np.copy(lfw_people.data[random_index, :])\n",
    "    single_label = single_label.reshape((h,w))\n",
    "    pyplot.title(target_names[lfw_people.target[random_index]], size=12)\n",
    "    pyplot.xticks(())\n",
    "    pyplot.yticks(())\n",
    "    pyplot.imshow(single_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset\n",
    "Split the dataset into train (75%) and test (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset (E001)\n",
    "\n",
    "X_train, X_test, y_train, y_test = <placeholder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Pipeline\n",
    "\n",
    "Now we build a pipeline that performs feature extraction with PCA followed by classification using logistic regression. While developing the pipeline, we perform the following tasks. \n",
    "\n",
    "1. For PCA vary the `n_components` parameter and for logistic regression vary the regularization parameter `C`. Use at least 5 different values for each of these parameters. \n",
    "2. Use 10-fold cross validation while using the gridsearch\n",
    "\n",
    "\n",
    "Note: If LogisticRegression doesn't converge increase the value for `max_iter`. \n",
    "\n",
    "## Define the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline (E002)\n",
    "\n",
    "pipe = <placeholder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Parameters for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the parameters for grid search (E003)\n",
    "\n",
    "param_grid = <placeholder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Pipeline with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the Pipeline with Grid Search (E004)\n",
    "\n",
    "model_grid = <placholder>\n",
    "model_grid.fit(<placeholder>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pipline is fitted, we can address the following tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the parameters of the trained models and their rankings within this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters of the trained models and their rankings in a table (E005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best model's parameters (E006)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the parameters of the second best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters of the second best model (E007)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the mean and stddev of the test scores (i.e CV scores) for the second best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mean and stddev of the test scores (i.e CV scores) for the second best model (E008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model using the test data\n",
    "\n",
    "Perform the following tasks: \n",
    "\n",
    "1. Make prediction\n",
    "1. Show confusion matrix\n",
    "1. Show classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction (E009)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix (E010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the classification report (E011)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your Notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
