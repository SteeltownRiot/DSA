{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Training and Testing\n",
    "\n",
    "In this lab you will learn about an important methodology in setting up a reliable framework for evaluating the machine learning models you will be building. \n",
    "The **training and testing** workflow involves the selection of training and testing datasets, as well as a performance measure meaningful to your problem. \n",
    "\n",
    "### Tip: \n",
    "_We will use the same dataset across several labs, so take a little time to get yourself familiarized with the structure of the dataset._\n",
    "\n",
    "#### Scikit Learn\n",
    "\n",
    "Read about Scikit as your time permits: http://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "Relevant sklearn API references:\n",
    " * [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    " * [sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "As an overview, we are going to use [**Decision Tree Classifier**](https://en.wikipedia.org/wiki/Decision_tree_learning) to fit the **red wine quality** dataset, \n",
    "then develop an understanding of why we need to hold out a test set to validate training, by taking a close look at a counterexample and seeing what could go wrong without this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load dataset from files into multi-dimensional array and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to have at least a rough idea on how many rows and columns are there in the dataset, and what are those columns, before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "dataset.describe() # Show the columns and basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column is the quality of wine (0 to 10), other columns are features. We are going to build a classifier to tell quality of wine based on its features. \n",
    "And then come up with way to evaluate the performance of the classifier. \n",
    "Therefore, the classifier has the following input/output.\n",
    "\n",
    "~~~\n",
    "X = all features except last column\n",
    "y = last column\n",
    "~~~\n",
    "\n",
    "In addition, for this lab, we are going to binarize Y into just 0 (ok wine) or 1 (good wine) just for simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset.iloc[:,:-1]) # Pull all rows, each column except the last\n",
    "y = np.array(dataset.quality) # Pull just the quality column\n",
    "\n",
    "# Binarize wine quality just for simplification using 6 as threshold\n",
    "y = (y>=6).astype(int)\n",
    "#          ^^^^^^^^^^ Convert True => 1 and False => 0\n",
    "#   ^^^^^ boolean array (constains True/False as elements)\n",
    "\n",
    "# The above is much similar to:\n",
    "#    y[y<6]=0; y[y>=6]=1\n",
    "# but much safer because The latter can go wrong when\n",
    "# order is reversed by accident.\n",
    "\n",
    "print('X', X.shape, 'y', y.shape)\n",
    "print('Label distribution:', {0: np.sum(y==0), 1: np.sum(y==1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Now that we have done some carpentry, re-running cells is best done starting from the top of the notebook!\n",
    "\n",
    "## Try the simple approach - train and evaluate on the whole dataset\n",
    "\n",
    "Train a Decision Tree Bayes model with the whole dataset and evaluate on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy')  # Create an instance of a model that can be trained\n",
    "model.fit(X, y)       # fit = \"train model parameters using this data and expected outcomes\"\n",
    "model.score(X, y)     # Evaluate a set of data, against the expected outcomes; here score is accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score means the **accuracy** (0 to 1) of the classifier on this dataset. Tranning a model over whole dataset do not address the following questions: \n",
    "\n",
    "+ Would the same predictive performance extend to future data?\n",
    "+ Are there enough data for the model to learn from?\n",
    "+ Could the classifier be learning from features that happen to correlate the result yet without necessary connection (noise).\n",
    "+ How to make more accurate evaluation of the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter-example\n",
    "\n",
    "Pushing this idea to an extreme case, let's train the model on the first 3 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X[:3], y[:3])\n",
    "model.score(X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would score 100%!\n",
    "\n",
    "Let's try predicting some other rows from the dataset.\n",
    "In other words, if the model is applied to new data that was not part of training how well does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ground Truth : ', y[100:150])\n",
    "print('Prediction   : ', model.predict(X[100:150]))\n",
    "print('Score        : ', model.score(X[100:150], y[100:150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demonstration of the phenomenon when building predictive models, known as **overfitting**.\n",
    "\n",
    "Overfitting is when the model was not able to successfully generalize to perform its task on the general population of data. \n",
    "Instead it has been optimized for the specific instances of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold out 25% for testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit has helpers for testing and evaluating models in a proper train/validate paradigm.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This function returns four sets:\n",
    "# Training features\n",
    "#       # Testing features\n",
    "#       #        # Training labels\n",
    "#       #        #        # Testing labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Get \"blank model\"\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train) # Train it \n",
    "model.score(X_test, y_test) # Validate its training with some withheld testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In each run of the above cell we will observe different score because training and testing sets are changing. Previous training should have been approximately: 0.76\n",
    "\n",
    "Compared to the first time we trained the model on the whole dataset, a decreased score would disprove the model's generalization ability by detecting counter example from test set, which implies a better model or training procedure is needed. \n",
    "\n",
    "\n",
    "Alternatively, if the model scored similarly (a necessary but not sufficient condition), it's more probable that the evaluation is accurate. \n",
    "That's why we must adopt a **training and test** process in order to evaluate the model more accurately. \n",
    "In module 2 we will learn about a more sophisticated evaluation approach, cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab we learned about:\n",
    "\n",
    "+ Training and validation workflow\n",
    "+ Splitting dataset into training and validation set\n",
    "+ Concept of overfitting\n",
    "+ Usage of DecisionTreeClassifier() from scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
