{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Classification\n",
    "\n",
    "In this lab you will create a classification model on the same red wine quality dataset and then apply and practice the same training and validation methodology. \n",
    "The classification model will be based on Naive Bayes provided by sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We will load the dataset from file into a Panda data frame and investigate its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "# View some metadata of the dataset and see if that makes sense\n",
    "print('dataset.shape', dataset.shape)\n",
    "\n",
    "X = np.array(dataset.iloc[:,:-1])[:, [1,2,6,9,10]]\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "print('X', X.shape, 'y', y.shape)\n",
    "print('Label distribution:', {i: np.sum(y==i) for i in np.unique(dataset.quality)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the train/test split and then train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have seen this before!\n",
    "# If you are so inclined, you may want to tweak the test_size and see how the model performs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "model1 = DecisionTreeClassifier(criterion='gini', max_depth=4)\n",
    "model1.fit(X_train, y_train)\n",
    "print(f\"Acc with gini: {model1.score(X_test, y_test)}\")\n",
    "\n",
    "\n",
    "model2 = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)\n",
    "print(f\"Acc with gini: {model2.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally you can print out a sample and see for yourself how the classification performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[20:50], \" (True class value)\")\n",
    "print(model.predict(X[20:50]), \" (Predicted class value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A text representation of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = export_text(model1)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "a = plot_tree(model2, \n",
    "              filled=True, \n",
    "              rounded=True, \n",
    "              fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Usually a classifier's peformance quantified in terms of precison, reall, f1, and accuracy measures. These measures are calculated from [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Compute confusion matrix with expected value, predicted values... similar to RMSE \n",
    "confusion_matrix(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Confusion Matrix: Precision, Recall, and F1\n",
    "\n",
    "Here we are going to look at a couple additional measures.\n",
    "\n",
    "First: \n",
    "  * _condition positive_ (P) is the number of real positive cases in the data\n",
    "  * _condition negatives_ (N) is the number of real negative cases in the data \n",
    "\n",
    "Then: \n",
    "  * _true positive_ (TP) is a correct prediction of a class, eqv. with hit in a Yes / No model\n",
    "  * _true negative_ (TN) is a correct prediction of not a class, eqv. with correct rejection in a Yes / No model\n",
    "  * _false positive_ (FP) is misclassification, eqv. with false alarm in a Yes / No model, **Type I error**\n",
    "  * _false negative_ (FN) is misclassification, eqv. with miss in a Yes / No model, **Type II error** \n",
    "\n",
    "Metrics:\n",
    "  * Recall or True Positive Rate:$$ Recall = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$ \n",
    "  * Precision or Positive Predictive Value:$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "  * [F1 is the harmonic mean of precision and recall](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)$$ F_{1} = 2 * \\frac{Precision * Recall}{Precision + Recall}$$\n",
    "  * Accuracy: $$ Accuracy = \\frac{TP + TN}{TP+FP+TN+FN} $$\n",
    "  \n",
    "#### More details on scikit-learn model scoring:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Check the API of these functions to learn more about the parameters\n",
    "\n",
    "print(\"Precision  :\", np.round(precision_score(y_test, model.predict(X_test), average='weighted'), 2))\n",
    "print(\"Recall     :\", np.round(precision_score(y_test, model.predict(X_test), average='weighted'), 2))\n",
    "print(\"F1-Score   :\", np.round(f1_score(y_test, model.predict(X_test), average='weighted'), 2))\n",
    "print(\"Accuracy   :\", np.round(accuracy_score(y_test, model.predict(X_test)), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scores could be estimated with a call to `classification_report`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The F1 score integrates the two metrics precision and recall into one.\n",
    "As it is one type of mean operation implies, the value of F1 score lies in between the two metrics.  \n",
    "In the scikit-learn package, `f1_score()` is generalized to multiclass targets. Therefore the last parameter `average` is referring to the algorithm of choice for averaging over multiple classes.  \n",
    "There is a more detailed explanation on this parameter in the documentation, as the discussion of different types of methodologies for integrating metrics would extend to a whole another subject of data fusion: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
