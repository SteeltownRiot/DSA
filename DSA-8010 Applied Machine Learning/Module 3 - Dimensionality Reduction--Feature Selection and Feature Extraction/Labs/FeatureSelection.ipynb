{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Feature Selection\n",
    "\n",
    "In this lab you will learn about **feature selection**, \n",
    "which reduces the dimensionality of data for the following reasons:\n",
    "\n",
    "1. Reduces overfitting by removing noise introduced by some of the features.\n",
    "2. Reduces training time, which allows you to experiment more with different models and hyperparameters.\n",
    "3. Reduces data acquisition requirements.\n",
    "4. Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. That will enable you to focus on the main sources of predictability, make the model more justifiable to another person.\n",
    "\n",
    "For this session, we are going to use **red wine quality** dataset as a starting point for learning feature selection,\n",
    "and then select the most relevant feature for predicting wine quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature selection methods generally falls into two categories known as **wrapper methods** and **filter methods**. There is a 3rd category, **embedded methods**, which includes those models where features are identified while learning the model parameters (e.g., LASSO). In this notebook we will discuss wrapper and filter methods.\n",
    "\n",
    "\n",
    "Wrappers utilize the learning machine of interest as a black box to score subsets of\n",
    "variable according to their predictive power. Filters select subsets of variables as a pre-processing\n",
    "step, independently of the chosen predictor. Embedded methods perform v\n",
    "\n",
    "\n",
    "<span style=\"text-decoration: underline;\">**Filter methods**</span> apply a statistical measure and assign a score to each feature one at a time. These methods select features independent of predictive methods and run very fast. \n",
    "In this lab, you will go through **Pearson's χ²** and **ANOVA F-value based feature selection** in this section. \n",
    "\n",
    "\n",
    "<span style=\"text-decoration: underline;\">**Wrapper methods**</span> use predictive methods for selecting a subset of features. The idea is to learn a set of features with feedbacks from the predictive method of interest. Based on the results drawn from the predictive model trained on that subset of features, \n",
    "they are either added or removed from the subset.\n",
    "The problem is essentially reduced to a search problem.\n",
    "[Greedy algorithms](https://en.wikipedia.org/wiki/Greedy_algorithm) \n",
    "are the most desirable in multivariate feature selection scenario because 1) \n",
    "wrapper methods are usually computationally very expensive; 2) \n",
    "greedy algorithms don't necessary provide the optimal solution,\n",
    "which is good becuase it makes them less prone to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "sklearn API reference:\n",
    "\n",
    "+ [sklearn.feature_selection.SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "+ [sklearn.feature_selection.chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "+ [sklearn.feature_selection.f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "+ [sklearn.feature_selection.mutual_info_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)\n",
    "+ [sklearn.feature_selection.RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Uncomment following line to view original output.\n",
    "# np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store features and labels into variables **X** and **y** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].to_numpy()\n",
    "y = dataset.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysis point of view, a solution to feature selection problems can be respresented as a boolean vector,\n",
    "each component indicating whether the corresponding feature has been selected.\n",
    "For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sci-kit learn calls the corresponding indices to feature columns selected \"support\", \n",
    "which can be obtained using [np.flatnonzero()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a naive approach that exhaustively search all subsets of features would have to verify $2^p$ solutions for $p$ features,\n",
    "which translates into $\\Omega(2^p)$ [time complexity](https://en.wikipedia.org/wiki/Time_complexity).\n",
    "That is to say it would be very inefficient in practice.\n",
    "\n",
    "However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline.\n",
    "This limits time complexity to $O(p^5 \\cdot n)$, assuming scoring a model takes linear time $O(n)$.\n",
    "Therefore, the following cell checks all $\\begin{pmatrix} 11 \\\\ 5 \\end{pmatrix} = \\frac{11\\times10\\times9\\times8\\times7}{5\\times4\\times3\\times2\\times1}=462$ solutions, \n",
    "and displays top 3 subset of features ranked by accuracy.\n",
    "\n",
    "In Part 1 \"Wrapper methods\", we will use these solutions as comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def search_combinations(estimator, X, y, k=5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        print(subset)\n",
    "        sc = score(X[:, subset])\n",
    "        print(sc)\n",
    "        yield score(X[:, subset]), subset\n",
    "        \n",
    "sorted(search_combinations(LinearRegression(), X, y), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Wrapper methods\n",
    "\n",
    "Within wrapper methods, there are different strategies for selecting the features. Here we will learn about **forward selection**, **backward elimination** and **recursive feature elimination** strategies.\n",
    "\n",
    "### A. Forward selection\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. \n",
    "In each iteration, we keep adding the feature which best improves our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_select(estimator, X, y, k=5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.zeros(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)  # accurary is the measure\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat till k features are selected\n",
    "    while np.sum(selected) < k:\n",
    "        # indices to unselected columns\n",
    "        rest_indices = list(np.flatnonzero(~selected))\n",
    "    \n",
    "        # compute model scores with an additional feature\n",
    "        scores = [score(X[:, selected_indices() + [i]]) for i in rest_indices]\n",
    "        print('\\n%accuracy if adding column:\\n   ',\n",
    "              {i:int(s*100) for i,s in zip(rest_indices,scores)})\n",
    "        \n",
    "        # find index within `rest_indices` that points to the most predictive feature not yet selected \n",
    "        idx_to_add = rest_indices[np.argmax(scores)]\n",
    "        print('add column', idx_to_add)\n",
    "        \n",
    "        # select this new feature\n",
    "        selected[idx_to_add] = True\n",
    "        \n",
    "    return selected_indices()\n",
    "\n",
    "support = sorted(forward_select(LinearRegression(), X, y))\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Backward elimination\n",
    "\n",
    "In backward elimination, \n",
    "we start with all the features and remove the _least significant_ feature at each iteration,\n",
    "which improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_eliminate(estimator, X, y, k=5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.ones(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat till k features are selected\n",
    "    while np.sum(selected) > k:\n",
    "        # Compute model scores with one of the features removed\n",
    "        scores = [score(X[:, list(set(selected_indices()) - {i})]) for i in selected_indices()]\n",
    "        print('\\n%accuracy if removing column:\\n   ',\n",
    "              {i:int(s*100) for i,s in zip(selected_indices(), scores)})\n",
    "        \n",
    "        # Find index that points to the least predictive feature\n",
    "        idx_to_remove = selected_indices()[np.argmax(scores)]\n",
    "        print('remove column', idx_to_remove)\n",
    "        \n",
    "        # Remove this feature\n",
    "        selected[idx_to_remove] = False\n",
    "        \n",
    "    return selected_indices()\n",
    "\n",
    "support = sorted(backward_eliminate(LinearRegression(), X, y))\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Recursive feature elimination\n",
    "\n",
    "**Recursive Feature elimination** is an even more greedy algorithm provided by sklearn, \n",
    "which finds good performing feature subset with high efficiency. This method has similarity with backward elimination technique. \n",
    "\n",
    "The importance of each feature is obtained either through a **`coef_`** attribute \n",
    "or through a **`feature_importances_`** attribute.\n",
    "So in order for recursive feature elimination algorithm in sklearn to work, \n",
    "the model is required to provide either of these attributes.\n",
    "\n",
    "Usually, we start off using a low complexity model and use it as a benchmark for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "selector = RFE(model, n_features_to_select=5)\n",
    "selector.fit(X, y)\n",
    "print(\"Num Features:\", selector.n_features_)\n",
    "print(\"Selected Features:\", np.flatnonzero(selector.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can transform dataset to include only these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filter methods\n",
    "\n",
    "In filter methods, features are ranked in terms of predictiveness one by one, \n",
    "as opposed to considering a subset.\n",
    "They incorporate statistical or information theoretic measures to rank each feature instead of measuring accuracy of a model trained on selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pearson's χ² test based feature selection\n",
    "\n",
    "The following cell shows the usage of a feature selector based on Pearson's χ² test.\n",
    "This constructs the approximate χ² distribution and scores each feature vs \n",
    "the label in order to determine which feature is more relevant, \n",
    "one at a time, then selects features according to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=5)\n",
    "selector.fit(X, y)\n",
    "print('χ² statistic', selector.scores_)\n",
    "print('Selected indices', selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the **transform()** method to select those feature columns from dataset and \n",
    "store into a new variable **X_selected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = selector.transform(X)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**selector.transform()** does the same as slicing out these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_selected, X[:, [1, 2, 5, 6, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take a closer look at this procedure by implementing one so we can better distinguish different feature selection methods. \n",
    "χ² test has many applications.\n",
    "For feature selection, we utilize χ² statistic to test for dependence of each feature towards determining label.\n",
    "\n",
    "**Note:** _You do NOT need to learn by heart the details of the computation to work on practices._\n",
    "\n",
    "#### Step 1: Encode labels into orthogonal vector space\n",
    "\n",
    "This is also know as **one-hot encoding**, which is applicable to classification problems.\n",
    "Consider an example where you have defined 3 categories for possible outcomes: A, B and C.\n",
    "In order for machine learning algorithms to be able to handle this type of data,\n",
    "we have to convert them into numbers.\n",
    "One-hot encoding uses a vector $ (y_1, y_2, y_3) $ where \n",
    "$$ y_i = \\left[ \\text{result falls into i}^{th}\\text{ category} \\right] \\in \\left\\{ 0, 1 \\right\\} $$\n",
    "Therefore, one-hot encoding for A, B and C categories becomes (1, 0, 0), (0, 1, 0) and (0, 0, 1) respectively.\n",
    "This has an advantage over plainly translating A, B and C into 1, 2 and 3 (a.k.a **sparse encoding**)\n",
    "in a way that orthogonal vectors do not impose assumptions of their order or magnitudes between categories like numbers would.\n",
    "\n",
    "For example, 3>1 is true, how ever it doesn't mean to imply C>A or C is superior to A in any way.\n",
    "However this would affect the model's numerical stability.\n",
    "\n",
    "Therefore one-hot encoding is an widely adopted technique for processing categories. \n",
    "Sparse encoding could be used when persisting a dataset in order to save storage space.\n",
    "\n",
    "**Note:** If you recall _regression in R_ from 8610 (Stat/Math), \n",
    "one-hot encoding is how the categorical / nominal variables are encoded as \n",
    "independent predictors in the regression formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "Y = np.array(LabelBinarizer().fit_transform(y))\n",
    "print(Y.shape)\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the [contingency table](https://en.wikipedia.org/wiki/Contingency_table) of observed frequencies\n",
    "\n",
    "**observed** is a (#classes)-by-(#features) matrix that contains the \"number of occurrences\" for each combination of feature and classes.\n",
    "\n",
    "**Note:** You previously saw contingency tables in the 8610 (Stat/Math) class and computed the Chi-Squared (χ²) statistic there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = np.dot(Y.T, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute the expected frequencies using marginal frequencies\n",
    "\n",
    "**expected** has the same shape as **observed** matrix, but represent the expected frequencies in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.dot(\n",
    "    Y.mean(axis=0).reshape(1, -1).T, # Mean value for all classes (transposed)\n",
    "    X.sum(axis=0).reshape(1, -1) # Marginal frequencies for all features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute the χ² statistic between **observed** and **expected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = np.sum((observed-expected)**2 / expected, axis=0)\n",
    "print(chi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with sklearn **chi2()**, which returns two arrays containing χ² statistic and p-value, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_sklearn, pvalue_sklearn = chi2(X, y)\n",
    "print(\"Chi-Squared Statistic\")\n",
    "print(chi2_sklearn)\n",
    "\n",
    "print(\"P-Values\")\n",
    "print(pvalue_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Rank features descendingly by χ² statistic\n",
    "\n",
    "Optionally, then sort these indices so they remain relative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = sorted(np.argsort(-chi_squared)[:5])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Select these features and transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[:, support]\n",
    "np.allclose(X_new, X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the p-value\n",
    "\n",
    "sklearn not only has provided the **χ² statistic** but also **p-value**,\n",
    "which is very useful because p-value could tell you quantitively how probable each feature is relevant,\n",
    "which in turn helps you decide how many and which features are worthwhile to retain.\n",
    "\n",
    "Here's part of a **χ² distribution** table.\n",
    "In our dataset, we have (#classes - 1) = 5 degrees of freedom (d.o.f.).\n",
    "\n",
    "<table cellspacing=\"2\" cellpadding=\"3\" border=\"1\" align=\"center\">\n",
    "<tbody>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">df</td><td bgcolor=\"#009bff\">0.995</td><td bgcolor=\"#009bff\">0.99</td><td bgcolor=\"#009bff\">0.975</td><td bgcolor=\"#009bff\">0.95</td><td bgcolor=\"#009bff\">0.9</td><td bgcolor=\"#009bff\">0.1</td><td bgcolor=\"#009bff\">0.05</td><td bgcolor=\"#009bff\">0.025</td><td bgcolor=\"#009bff\">0.01</td><td bgcolor=\"#009bff\">0.005</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">1</td><td bgcolor=\"#cdebfa\">0</td><td bgcolor=\"#edfbfa\">0</td><td bgcolor=\"#cdebfa\">0.001</td><td bgcolor=\"#edfbfa\">0.004</td><td bgcolor=\"#cdebfa\">0.016</td><td bgcolor=\"#edfbfa\">2.706</td><td bgcolor=\"#cdebfa\">3.841</td><td bgcolor=\"#edfbfa\">5.024</td><td bgcolor=\"#cdebfa\">6.635</td><td bgcolor=\"#edfbfa\">7.879</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">2</td><td bgcolor=\"#edfbfa\">0.01</td><td bgcolor=\"#cdebfa\">0.02</td><td bgcolor=\"#edfbfa\">0.051</td><td bgcolor=\"#cdebfa\">0.103</td><td bgcolor=\"#edfbfa\">0.211</td><td bgcolor=\"#cdebfa\">4.605</td><td bgcolor=\"#edfbfa\">5.991</td><td bgcolor=\"#cdebfa\">7.378</td><td bgcolor=\"#edfbfa\">9.21</td><td bgcolor=\"#cdebfa\">10.597</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">3</td><td bgcolor=\"#cdebfa\">0.072</td><td bgcolor=\"#edfbfa\">0.115</td><td bgcolor=\"#cdebfa\">0.216</td><td bgcolor=\"#edfbfa\">0.352</td><td bgcolor=\"#cdebfa\">0.584</td><td bgcolor=\"#edfbfa\">6.251</td><td bgcolor=\"#cdebfa\">7.815</td><td bgcolor=\"#edfbfa\">9.348</td><td bgcolor=\"#cdebfa\">11.345</td><td bgcolor=\"#edfbfa\">12.838</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">4</td><td bgcolor=\"#edfbfa\">0.207</td><td bgcolor=\"#cdebfa\">0.297</td><td bgcolor=\"#edfbfa\">0.484</td><td bgcolor=\"#cdebfa\">0.711</td><td bgcolor=\"#edfbfa\">1.064</td><td bgcolor=\"#cdebfa\">7.779</td><td bgcolor=\"#edfbfa\">9.488</td><td bgcolor=\"#cdebfa\">11.143</td><td bgcolor=\"#edfbfa\">13.277</td><td bgcolor=\"#cdebfa\">14.86</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">5</td><td bgcolor=\"#cdebfa\">0.412</td><td bgcolor=\"#edfbfa\">0.554</td><td bgcolor=\"#cdebfa\">0.831</td><td bgcolor=\"#edfbfa\">1.145</td><td bgcolor=\"#cdebfa\">1.61</td><td bgcolor=\"#edfbfa\">9.236</td><td bgcolor=\"#cdebfa\">11.07</td><td bgcolor=\"#edfbfa\">12.833</td><td bgcolor=\"#cdebfa\">15.086</td><td bgcolor=\"#edfbfa\">16.75</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">6</td><td bgcolor=\"#edfbfa\">0.676</td><td bgcolor=\"#cdebfa\">0.872</td><td bgcolor=\"#edfbfa\">1.237</td><td bgcolor=\"#cdebfa\">1.635</td><td bgcolor=\"#edfbfa\">2.204</td><td bgcolor=\"#cdebfa\">10.645</td><td bgcolor=\"#edfbfa\">12.592</td><td bgcolor=\"#cdebfa\">14.449</td><td bgcolor=\"#edfbfa\">16.812</td><td bgcolor=\"#cdebfa\">18.548</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">7</td><td bgcolor=\"#cdebfa\">0.989</td><td bgcolor=\"#edfbfa\">1.239</td><td bgcolor=\"#cdebfa\">1.69</td><td bgcolor=\"#edfbfa\">2.167</td><td bgcolor=\"#cdebfa\">2.833</td><td bgcolor=\"#edfbfa\">12.017</td><td bgcolor=\"#cdebfa\">14.067</td><td bgcolor=\"#edfbfa\">16.013</td><td bgcolor=\"#cdebfa\">18.475</td><td bgcolor=\"#edfbfa\">20.278</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">8</td><td bgcolor=\"#edfbfa\">1.344</td><td bgcolor=\"#cdebfa\">1.646</td><td bgcolor=\"#edfbfa\">2.18</td><td bgcolor=\"#cdebfa\">2.733</td><td bgcolor=\"#edfbfa\">3.49</td><td bgcolor=\"#cdebfa\">13.362</td><td bgcolor=\"#edfbfa\">15.507</td><td bgcolor=\"#cdebfa\">17.535</td><td bgcolor=\"#edfbfa\">20.09</td><td bgcolor=\"#cdebfa\">21.955</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">9</td><td bgcolor=\"#cdebfa\">1.735</td><td bgcolor=\"#edfbfa\">2.088</td><td bgcolor=\"#cdebfa\">2.7</td><td bgcolor=\"#edfbfa\">3.325</td><td bgcolor=\"#cdebfa\">4.168</td><td bgcolor=\"#edfbfa\">14.684</td><td bgcolor=\"#cdebfa\">16.919</td><td bgcolor=\"#edfbfa\">19.023</td><td bgcolor=\"#cdebfa\">21.666</td><td bgcolor=\"#edfbfa\">23.589</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">10</td><td bgcolor=\"#edfbfa\">2.156</td><td bgcolor=\"#cdebfa\">2.558</td><td bgcolor=\"#edfbfa\">3.247</td><td bgcolor=\"#cdebfa\">3.94</td><td bgcolor=\"#edfbfa\">4.865</td><td bgcolor=\"#cdebfa\">15.989</td><td bgcolor=\"#edfbfa\">18.307</td><td bgcolor=\"#cdebfa\">20.483</td><td bgcolor=\"#edfbfa\">23.209</td><td bgcolor=\"#cdebfa\">25.188</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis is that a class labels is independent of a feature. \n",
    "Since here we used **χ² statistic** for testing what the null hypothesis has claimed : **independence between features and labels**.\n",
    "We can choose `p-value` $>2.5\\%$ as the critical level at which the null hypothesis **can not be rejected**,\n",
    "and reject the hypothesis otherwise.\n",
    "This is effectively saying that, if the feature is more that 10% chance to be independent of the class, then it is not a good predictor.\n",
    "The alternative hypothesis is, therefore, that the feature is probable predictor of the class, i.e., the class is dependent on the feature.\n",
    "The following table would summarize our conclusion based on this criterion:\n",
    "\n",
    "<table>\n",
    "<tr><td><strong>feature idx</strong></td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr>\n",
    "<tr><td><strong>χ²</strong></td><td>11.3</td><td>15.6</td><td>13.0</td><td>4.12</td><td>0.752</td><td>162</td><td>2.76e+03</td><td>2.30e-04</td><td>0.155</td><td>4.56</td><td>46.4</td></tr>\n",
    "<tr><td><strong>p-value</strong></td><td>~5%</td><td>~0.5%</td><td>~1%</td><td>90%~95%</td><td>100%</td><td>0%</td><td>0%</td><td>100%</td><td>100%</td><td>90%~95%</td><td>0%</td></tr>\n",
    "<tr><td><strong>interpretation</strong></td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td>independent</td><td><strong>dependent</strong></td></tr>\n",
    "</table>\n",
    "\n",
    "So we should select feature 1, 2, 5, 6 and 10.\n",
    "\n",
    "<!-- future work:\n",
    "sklearn learn can help make this process more precise and easy than looking up the **χ² distribution** table.\n",
    "Following cells prints the all the p-values:\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please review the documentation of the chi2 function from `sklearn.feature_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "Here's how to generate χ² table, which may come in handy now and then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_table(degree_of_freedoms):\n",
    "    from scipy.stats import chi2 as chi2_distribution\n",
    "    pvalue = np.array([0.995, 0.99, 0.975, 0.95, 0.90, 0.10, 0.05, 0.025, 0.01, 0.005])\n",
    "    return pd.DataFrame(chi2_distribution.isf(\n",
    "            # isf(p) = inverse(1-cdf)(p) which takes p-value returns chi square value\n",
    "            #     where cdf is short for cumulative distribution function\n",
    "        pvalue, np.expand_dims(degree_of_freedoms, 1)),\n",
    "        columns = pvalue, index = degree_of_freedoms)\n",
    "\n",
    "chi2_table(range(5, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### B. ANOVA F-value based feature selection\n",
    "\n",
    "The following cell shows the usage of a feature selector based on ANOVA F-value and Pearson's correlation.\n",
    "This calculates Pearson's correlation of each feature vs the label in order to determine which feature is more relevant, \n",
    "one at a time, then selects features according to the ANOVA F-value derived from Pearson's correlation.\n",
    "\n",
    "See also: https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "**Note:** You previously saw the ANOVA and MANOVA within the 8610 (Stat/Math) course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_regression, k=5)\n",
    "selector.fit(X, y)\n",
    "print('score', selector.scores_)\n",
    "print('Selected indices', selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take a closer look at this procedure to gain a more solid understanding.\n",
    "\n",
    "**Note:** _You do NOT need to learn by heart the details of the computation to work on practices._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Compute cross correlation\n",
    "\n",
    "$$r_j = \\frac{\\sigma_{X_j y}}{\\sigma_{X_j} \\sigma_y} = \\frac{(y-\\bar y)^T (X_j-\\bar {X_j})}{\\lVert X_j-\\bar {X_j}\\rVert \\cdot \\lVert y-\\bar y\\rVert}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_centered = scale(X.astype('float'), with_std = False)\n",
    "y_centered = scale(y.astype('float'), with_std = False)\n",
    "corr = np.dot(y_centered, X_centered) / np.linalg.norm(X_centered, axis = 0) / np.linalg.norm(y_centered)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print([pearsonr(X[:,i], y)[0] for i in range(X.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute ANOVA F-value\n",
    "\n",
    "This is a F-test for correlation coefficients.\n",
    "Read more [here](https://onlinecourses.science.psu.edu/stat501/lesson/2/2.6).\n",
    "Similar to the way that χ² test is provided by sklearn, [f_regression()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression) also supplied p-value for quantitatively assessing how relevant each feature is.\n",
    "\n",
    "$$ F = t^2 = \\left( \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} \\right) ^2 = \\frac{r^2/1 }{(1-r^2)/(n-2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2 = corr ** 2\n",
    "Fvalue = corr2 / (1 - corr2) * (y.shape[0] - 2)\n",
    "print(Fvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to f_regression()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fvalue_sklearn, pvalue2_sklearn = f_regression(X,y)\n",
    "print(Fvalue_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Select these features and transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = sorted(np.argsort(-Fvalue)[:5])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on F-value and p-value\n",
    "\n",
    "F-test can be used as hypothesis testing for a ratio of two χ² distributions.\n",
    "The F-test for correlation coefficients is derived from testing a ratio between regression sum square (SSR)\n",
    "and error sum square (SSE).\n",
    "\n",
    "$$ SSR = \\sum _{i=1}^n {\\left(\\hat {y_i} - \\bar y \\right)^2}$$\n",
    "\n",
    "$$ SSE = \\sum _{i=1}^n {\\left(\\hat {y_i} - y_i \\right)^2}$$\n",
    "\n",
    "$$ F  = \\frac{SSR/(v-1) }{SSE/(n-2)} $$\n",
    "\n",
    "With correlation coefficients, this is \n",
    "\n",
    "$$ F = \\frac{r^2/1 }{(1-r^2)/(n-2)} $$\n",
    "\n",
    "with degree of freedom 1 and (n-2) respectively.\n",
    "\n",
    "Its null hypothesis claims r = 0, i.e. the independence of each feature and label.\n",
    "We could theoretically make hypothesis testing using an F-distribution table.\n",
    "\n",
    "Here's how to generate F table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_table(alpha, v1, v2):\n",
    "    from scipy.stats import f as f_distribution\n",
    "    v1 = np.array(list(v1)); v2 = np.array(list(v2))\n",
    "    return pd.DataFrame(f_distribution.isf(alpha, v1[np.newaxis, ...], v2[..., np.newaxis]),\n",
    "        columns = v1, index = v2)\n",
    "\n",
    "import itertools\n",
    "f_table(0.1, range(1, 9), itertools.chain(range(1,10), range(10, 260, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, directly compute p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f as f_distribution\n",
    "f_distribution.sf(Fvalue, 1, y.shape[0] - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to those obtained from f_regression()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue2_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print in percentages perhaps makes it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pvalue2_sklearn * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We just went through **χ²** and **F-value** based feature selection in detail because \n",
    "these two different methods work better on classification and regression respectively.\n",
    "The general steps for doing feature selection with sklearn was:\n",
    "\n",
    "1. Choose a feature scoring method\n",
    "2. Initialize a feature selector\n",
    "3. Fit feature selector on the data\n",
    "\n",
    "Other feature selection methods provided by sklearn include:\n",
    "\n",
    "* Classification: chi2, f_classif, mutual_info_classif\n",
    "* Regresssion: f_regression, mutual_info_regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, we learned about:\n",
    "\n",
    "* Wrapper methods\n",
    "    * Forward selection\n",
    "    * Backward elimination\n",
    "    * Recursive feature elimination\n",
    "    \n",
    " \n",
    "* Filter methods\n",
    "    * Pearson's χ²\n",
    "    * ANOVA F-value\n",
    "\n",
    "\n",
    "And once again,\n",
    "1. Wrapper methods are usually computationally expensive\n",
    "2. Greedy algorithms don't necessary provide the optimal solution, which may be good becuase it makes them less prone to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
