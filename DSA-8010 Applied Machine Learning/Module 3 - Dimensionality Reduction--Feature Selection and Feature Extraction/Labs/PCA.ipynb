{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Principal componentent analysis\n",
    "\n",
    "In this lab you will learn about **Principal componentent analysis** (PCA).\n",
    "It is a linear transformation that transforms data points into feature space representation,\n",
    "which has the following properties:\n",
    "\n",
    "+ Exactly the **same** dimension as the original data space\n",
    "+ Reconstruction of data points is optimal in the mean-square-error sense.\n",
    "+ Principal components are sorted by \"effectiveness\" at capturing variance.\n",
    "+ Dimensionality reduction can be done simply by truncating (down selecting the number of) principal components.\n",
    "\n",
    "For this session, we are going to use **red wine quality** dataset for principal componentent analysis \n",
    "and transform its data space into feature space with top 5 principal components.\n",
    "\n",
    "sklearn API reference:\n",
    "\n",
    "+ [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "\n",
    "**Note:** Previously you saw PCA in the 8610 (Stat/Math) course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset.iloc[:, :-1])\n",
    "\n",
    "X_centered = scale(X, with_std = False)\n",
    "\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with sklearn\n",
    "\n",
    "The following cell performs principal component analysis and transforms data space into feature space,\n",
    "which is done by a matrix transform followed by truncation of low variance principal components.\n",
    "Then you are free to use the obtained features **X_features** as input to your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_centered)\n",
    "print(pca.explained_variance_ratio_)\n",
    "X_features = pca.transform(X_centered)\n",
    "print('Features shape', X_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of data space\n",
    "\n",
    "An approximation of original dataset can be recovered by linearly combining high variance principal components.\n",
    "This demonstrates the connection between data space and feature space.\n",
    "\n",
    "$$ \\hat X = AQ + \\mu \\approx X $$\n",
    "\n",
    "**Note**: The difference between **X_synthesized** and **X_reconstructed** is that the former is centered\n",
    "at the origin because PCA assumes mathematical expectation of input to be zero and input needs centered otherwise.\n",
    "Therefore, in order to reconstruct the original dataset, the approximation obtained using PCA has to be\n",
    "shifted back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Principal components shape', pca.components_.shape)\n",
    "X_synthesized = np.dot(X_features, pca.components_)\n",
    "X_reconstructed = X_synthesized + np.mean(X, axis = 0)[np.newaxis, ...]\n",
    "print('Reconstructed dataset shape', X_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows a reconstructed dataset which is an approximation of original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_reconstructed, columns = dataset.columns[:-1]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the original dataset for comparison, and that we would not\n",
    "lose a whole lot of information by throwing away those low variance principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:5, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation error of principal components\n",
    "\n",
    "As a result, we were able to represent original dataset using principal components\n",
    "without losing too much information. \n",
    "Space consumption was essentially halved.\n",
    "And these principal components can be used as features for subsequent machine learning pipeline.\n",
    "The following calculates mean squared error between original dataset and reconstructed dataset for all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = X - X_reconstructed\n",
    "np.mean((error**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify eigenstructure of PCA\n",
    "\n",
    "The solution of PCA could be found by performing **eigen-decomposition** of the covariance matrix of **X**.\n",
    "This section quickly verifies some of its properties pertaining to eigenstructure.\n",
    "\n",
    "Some other interesting facts about PCA include that the error and approximation are orthogonal.\n",
    "Because the approximation consists of high variance principal components while the error solely \n",
    "consists of low variance principal components, and all principal components are orthogonal pairwise.\n",
    "\n",
    "$$ E^T (\\hat X- \\mu) = 0$$\n",
    "\n",
    "Moreover, this orthogonality is a desirable property because it is tantamount to achieving minimum mean square error. \n",
    "See [Principle of Orthogonality](https://en.wikipedia.org/wiki/Orthogonality_principle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.dot(error.T, X_synthesized), np.zeros((X.shape[1],X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(np.dot(error.T, X_synthesized), np.zeros((X.shape[1],X.shape[1]))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is also closely related to eigen-decomposition and the singular value decomposition problem.\n",
    "The former can be quickly verified that **pca.explained\\_variance_** and **pca.components_** are, in fact, the **eigenvalues** and **eigenvectors** of the covariance matrix,\n",
    "which also explains the orthogonality among all principal components.\n",
    "\n",
    "$$ cov(X)Q = \\Lambda Q $$\n",
    "\n",
    "or for each column vector $\\vec{q_j}$ in **Q**, this is the eigenequation:\n",
    "\n",
    "$$ R \\vec{q_j}=\\lambda_j \\vec{q_j}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$R=cov(X) = E(X-\\mu)(X-\\mu)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix of X\n",
    "R = np.dot(X_centered.T, X_centered) / X.shape[0]\n",
    "\n",
    "for eigenvalue, eigenvector in zip(pca.explained_variance_, pca.components_):\n",
    "    print(np.allclose(\n",
    "        np.dot(R, eigenvector),          # left hand side\n",
    "        np.dot(eigenvalue, eigenvector)  # right hand side\n",
    "    , rtol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree plot\n",
    "\n",
    "This plots variances (y-axis) against components (x-axis).\n",
    "As one moves to the right, toward later components, the variances (or the eigenvalues) drop.\n",
    "It helps us to decide on number of principal components to be retained, although this is subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(pca.components_))+1\n",
    "plt.xticks(x_ticks) # this enforces integers on the x-axis\n",
    "plt.plot(x_ticks, pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, you could also plot the explained variance ratio.\n",
    "A full PCA without components truncated would have total explained variance ratio equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(x_ticks)\n",
    "plt.plot(x_ticks, pca.explained_variance_ratio_)\n",
    "print('total expained variance ratio', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two plots show that the first component explained most of the variance present in the data. We can simply work with first 2/3 compoents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab we learned about:\n",
    "+ Apply PCA to a dataset for feature extraction.\n",
    "+ Reconstruct an approximation of original dataset using principal components.\n",
    "+ Eigenstructure of PCA.\n",
    "+ Scree plot of PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
