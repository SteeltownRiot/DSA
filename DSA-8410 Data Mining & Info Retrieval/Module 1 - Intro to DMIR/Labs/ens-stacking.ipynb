{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning: Stacking.\n",
    "\n",
    "Stacking is an ensemble learning technique which is used to combine the predictions of diverse classification models into one single model also known as the meta-classifier. This notebook is adapted from [[1](#References)].\n",
    "\n",
    "\n",
    "\n",
    "All the individual models are trained separately on the complete training data set and fine-tuned to achieve a greater accuracy. The bias and variance trade-off is taken care off for each model. The final model, also known as the meta-classifier is fed either the class labels predicted by the base models or the predicted probabilities for each class label. The meta-classifier is then trained based on the outputs given by the ensemble models. \n",
    "\n",
    "The predicted probabilities of each model from stage 1 are fed as an input to all the models at stage 2. The models at stage 2 are then fine-tuned and the corresponding outputs are fed to models at stage 3 and so on. This process occurs multiple times based on how many layers of stacking one would like to use. The final stage consists of one single powerful model, which gives us the final output by combining the output of all the models present in the previous layers. This single powerful model at the end of a stacking pipeline is called the meta-classifier. Often times, using stacking classifiers increases the prediction accuracy of a model. But in no way can there be a guarantee that using stacking will increase the prediction accuracy at all times! Take a look at the below diagrams to understand how stacking works. You can refer to the MLEXTEND GitHub page highlighted in bold to get more ideas on how to implement stacking in different scenarios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"../images/StackingFINAL.jpg\"> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code sample, we will use eight different base learners and train each of them on the whole dataset. Each of these models can be fine-tuned using grid search cross-validation. Each of these N models will predict eight class labels. At the final stage, the predictions of all the base models are combined using majority voting (for classification tasks), to create a final model called the meta-classifier. The meta-classifier in our case is the logistic regression model. As we can see from the outputs below, stacking has indeed managed to increase the accuracy of the final model although the increase is very less. But you get the idea! Like the previous examples, we will use a 3 fold cross-validation. Again, you can experiment with this value when you work with some real-world datasets. Further down, in a separate example, we will try grid search cross-validation on the base learners and see if the overall accuracy increases (well, it should actually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3, pandas as pd, numpy as np, nltk, string, matplotlib.pyplot as plt, seaborn as sns\n",
    "import string, math, pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from xgboost.sklearn import XGBModel, XGBRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Iris dataset + pair plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Iris.csv into a pandas dataFrame.\n",
    "iris_dataset = pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")\n",
    "X, y = iris_dataset.iloc[:,0:4], iris_dataset.iloc[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_object = LabelEncoder()\n",
    "y = encoder_object.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "\n",
    "#Base Learners\n",
    "rf_clf = RandomForestClassifier(n_estimators=10, random_state=RANDOM_SEED)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=5, random_state=RANDOM_SEED)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "svc_clf = SVC(C=10000.0, kernel='rbf', random_state=RANDOM_SEED)\n",
    "rg_clf = RidgeClassifier(alpha=0.1, random_state=RANDOM_SEED)\n",
    "lr_clf = LogisticRegression(C=20000, penalty='l2', random_state=RANDOM_SEED)\n",
    "dt_clf = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=RANDOM_SEED)\n",
    "adab_clf = AdaBoostClassifier(n_estimators=100)\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED) # meta classifier\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[rf_clf, et_clf, knn_clf, svc_clf, rg_clf, lr_clf, dt_clf, adab_clf], meta_classifier=lr)\n",
    "\n",
    "classifier_array = [rf_clf, et_clf, knn_clf, svc_clf, rg_clf, lr_clf, dt_clf, adab_clf, sclf]\n",
    "labels = [clf.__class__.__name__ for clf in classifier_array]\n",
    "\n",
    "acc_list = []\n",
    "var_list = []\n",
    "\n",
    "for clf, label in zip(classifier_array, labels):\n",
    "    cv_scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (cv_scores.mean(), cv_scores.std(), label))\n",
    "    acc_list.append(np.round(cv_scores.mean(),4))\n",
    "    var_list.append(np.round(cv_scores.std(),4))\n",
    "    #print(\"Accuracy: {} (+/- {}) [{}]\".format(np.round(scores.mean(),4), np.round(scores.std(),4), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "n_groups = 9\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.30\n",
    "\n",
    "opacity = .7\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "normal_clf = ax.bar(index, acc_list, bar_width, alpha=opacity, color='g', yerr=var_list, error_kw=error_config, label='Classifiers')\n",
    "\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Improving accuracy using stacking classifiers')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels((labels))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will look at the decision regions obtained using three of our base learners and also the final stacked meta-classifier. The three base learners that we will select for this purpose are RandomForestClassifier, SupportVectorClassifer, and RidgeClassifier. As like before, I will train and fit the model to two of the most important features, i.e. \"petal_length\" and \"petal_width\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Regions for 4 algorithms.\n",
    "X = np.array(iris_dataset[['petal_length','petal_width']])\n",
    "y = np.array(y)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "\n",
    "\n",
    "for clf, label, grd in zip([rf_clf, svc_clf, rg_clf, sclf], \n",
    "                           [\"Random Forest Classifier\", \"Support Vector Classifer\", \"Ridge Classifier\", \"Stacking Classifier\"], \n",
    "                           itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] S. Paual. [Ensemble Learning â€” Bagging, Boosting, Stacking and Cascading Classifiers in Machine Learning using SKLEARN and MLEXTEND libraries](https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
