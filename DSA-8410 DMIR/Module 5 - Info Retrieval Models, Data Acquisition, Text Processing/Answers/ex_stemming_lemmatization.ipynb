{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for Module-1 Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "mini_df = df[['v1', 'v2']][:100]\n",
    "mini_df.columns = ['class', 'text']\n",
    "\n",
    "mini_df.to_csv('messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_df = pd.read_csv('messages.csv')\n",
    "msgs = cur_df.T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, we give you 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. We have converted the dataframe into a dictionary for this exercise.\n",
    "\n",
    "In the given dictionary, there are 100 entries, starting from 0 to 99 as the keys. The value for each of them is two strings, `class` and `text`. `class` contains either \"spam\" or \"ham\", based on the category of the sms, and `text` contains the original text message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Create a list of strings from this dictionary with the `text` values, and convert all of the strings into lowercase. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...', 'ok lar... joking wif u oni...', \"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\", 'u dun say so early hor... u c already then say...', \"nah i don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "strs = [value['text'].lower() for key, value in msgs.items()]\n",
    "print(strs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Use `nltk` packages tokenize functionality on each of the strings in your list. The result should be a list of lists. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk import word_tokenize\n",
    "\n",
    "msg = [word_tokenize(sent) for sent in strs]\n",
    "print(msg[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Remove the stopwords, punctuations and numbers from your list (list of lists). Punctuations and numbers can be removed by checking against the built-in string `string.punctuation`. If a particular character is found in `string.punctuation`, you can remove that from your string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', '87121', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 's', 'apply', '08452810075over18', 's'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'nt', 'think', 'goes', 'usf', 'lives', 'around', 'though']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/scottgs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "clean_msg = []\n",
    "\n",
    "for cur in msg:\n",
    "    # Remove stop-words\n",
    "    r_st = [word for word in cur if word not in stop_words]\n",
    "    r_punc = []\n",
    "    # Remove punctuations\n",
    "    for nxt in r_st:\n",
    "        r_punc.append(\"\".join([lett for lett in nxt if lett not in string.punctuation]))\n",
    "    # Eliminate empty strings caused by removing punctuations\n",
    "    r_punc = [word for word in r_punc if word]\n",
    "    clean_msg.append(r_punc)\n",
    "\n",
    "print(clean_msg[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Use `nltk` packages `PorterStemmer` to stem the cleaned-text list that you got as a result of **Task 3**. Use a new variable to store the stemmed-word list, and keep the result from the **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['free', 'entri', '2', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', '21st', 'may', '2005', 'text', 'fa', '87121', 'receiv', 'entri', 'question', 'std', 'txt', 'rate', 'c', 's', 'appli', '08452810075over18', 's'], ['u', 'dun', 'say', 'earli', 'hor', 'u', 'c', 'alreadi', 'say'], ['nah', 'nt', 'think', 'goe', 'usf', 'live', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stems = []\n",
    "for cur in clean_msg:\n",
    "    stem = [porter.stem(word) for word in cur]\n",
    "    stems.append(stem)\n",
    "\n",
    "print(stems[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.** Use `nltk` packages `WordNetLemmatizer` to find the lemma (or root word) from the cleaned-text list that you got as a result of **Task 3**. Consider all of the words to be a `Verb`. Use a new variable to store the lemmatized-word list, and keep the result from the **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'get', 'amore', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', '87121', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 's', 'apply', '08452810075over18', 's'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'nt', 'think', 'go', 'usf', 'live', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for cur in clean_msg:\n",
    "    lemma = [wordnet.lemmatize(word, pos=\"v\") for word in cur]\n",
    "    lemmas.append(lemma)\n",
    "\n",
    "print(lemmas[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.** For each lemma in the list that we got from **Task 5**, calculate how many times they occur in all of the messages. Sort them in descending order by the number of total occurances, and print out the top ten (10) words and their number of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: u 17\n",
      "word: call 15\n",
      "word: nt 14\n",
      "word: s 13\n",
      "word: m 13\n",
      "word: go 11\n",
      "word: get 11\n",
      "word: free 10\n",
      "word: like 10\n",
      "word: ok 8\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "occurs = {}\n",
    "\n",
    "for cur in lemmas:\n",
    "    for nxt in cur:\n",
    "        if (nxt not in occurs):\n",
    "            occurs[nxt] = 1\n",
    "        else:\n",
    "            occurs[nxt] += 1\n",
    "\n",
    "cnt = 0\n",
    "for k in sorted(occurs, key=occurs.get, reverse=True):\n",
    "    print('word:', k, occurs[k])\n",
    "    cnt += 1\n",
    "    if (cnt >= 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7.** From the result we got from **Task 6**, remove all of the words with length of 1, and select the top hundred (100) most frequent words from it. We will use this list of words in our next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 100 -> ['call', 'nt', 'go', 'get', 'free', 'like', 'ok', 'sorry', 'txt', 'already', 'home', 'smile', 'say', 'still', 'want', 'reply', 'yes', 'way', 'ur', 'ha', 'finish', 'know', 'anything', 'please', 'see', 'lt', 'gt', 'back', 'send', 'prize', 'claim', 'mobile', 'time', 'try', 'll', 'tell', 'later', 'pain', 'come', 'hi', 'great', 'lar', 'joke', 'text', 'think', 'word', 'even', 'customer', 'na', 'tonight', 'cash', 'make', 'feel', 'miss', 'first', 'lor', 'meet', 'lol', 'catch', 'love', 'wait', 'yeah', 'look', 'do', 'need', 'sms', 'man', 'end', 'check', 'wat', 'wif', 'entry', 'win', 'fa', 'may', '87121', 'std', 'apply', 'early', 'live', 'around', 'though', 'week', 'å£150', 'treat', 'request', 'melle', 'callertune', 'value', 'network', 'months', 'update', 'gon', 'stuff', 'anymore', 've', 'enough', 'today', '16', 'urgent']\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "rows = list()\n",
    "for k in sorted(occurs, key=occurs.get, reverse=True):\n",
    "    if (len(k) > 1):\n",
    "        rows.append(k)\n",
    "        if (len(rows) >= 100):\n",
    "            break\n",
    "\n",
    "print('length', len(rows), '->', rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.** For each message (use the lemma-list we created for **Task 5**), calculate the number of time each word from **Task 7** (top-100 words) occurs in that message. \n",
    "Create a **Data-Matrix** using your calculations. Each row should correspond to a message, and each column should correspond to a word from the list we got in **Task 7**. Each cell should correspond to how many times that particular word (from column) occurs in that particular message (from row).\n",
    "\n",
    "You can use Pandas-DataFrame to store your **Data-Matrix**. Print the first 5 rows of the Data-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>nt</th>\n",
       "      <th>go</th>\n",
       "      <th>get</th>\n",
       "      <th>free</th>\n",
       "      <th>like</th>\n",
       "      <th>ok</th>\n",
       "      <th>sorry</th>\n",
       "      <th>txt</th>\n",
       "      <th>already</th>\n",
       "      <th>...</th>\n",
       "      <th>months</th>\n",
       "      <th>update</th>\n",
       "      <th>gon</th>\n",
       "      <th>stuff</th>\n",
       "      <th>anymore</th>\n",
       "      <th>ve</th>\n",
       "      <th>enough</th>\n",
       "      <th>today</th>\n",
       "      <th>16</th>\n",
       "      <th>urgent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   call  nt  go  get  free  like  ok  sorry  txt  already  ...  months  \\\n",
       "0     0   0   1    1     0     0   0      0    0        0  ...       0   \n",
       "1     0   0   0    0     0     0   1      0    0        0  ...       0   \n",
       "2     0   0   0    0     1     0   0      0    1        0  ...       0   \n",
       "3     0   0   0    0     0     0   0      0    0        1  ...       0   \n",
       "4     0   1   1    0     0     0   0      0    0        0  ...       0   \n",
       "\n",
       "   update  gon  stuff  anymore  ve  enough  today  16  urgent  \n",
       "0       0    0      0        0   0       0      0   0       0  \n",
       "1       0    0      0        0   0       0      0   0       0  \n",
       "2       0    0      0        0   0       0      0   0       0  \n",
       "3       0    0      0        0   0       0      0   0       0  \n",
       "4       0    0      0        0   0       0      0   0       0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "import pandas as pd\n",
    "\n",
    "dm = []\n",
    "\n",
    "idx = 0\n",
    "for cur in lemmas:\n",
    "    tf = {}\n",
    "    for word in rows:\n",
    "        tf[word] = cur.count(word)\n",
    "    dm.append(tf)\n",
    "\n",
    "df = pd.DataFrame(dm)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
