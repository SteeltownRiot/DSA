{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition: Extract data from the web\n",
    "\n",
    "Data acquisition is a crucial set for developing an information retrieval system. With the boom of the internet, there is so much data lying on the web in the form of websites. \n",
    "There are many ways to extract data from the web, and APIs are probably the best way to extract data from a website. \n",
    "Most big websites like Twitter, Facebook, Amazon, and New York Times provide APIs to access their data.\n",
    "But not all websites have an API. \n",
    "Some websites don't provide one because of privacy concerns or they lack technical use-case to provide one. \n",
    "\n",
    "**Web scraping** is a technique of extracting information from websites. \n",
    "It focuses on transforming partially structures data (HTML format) on the web into structured data (database or spreadsheet).\n",
    "\n",
    "Python has a rich ecosystem to scrape data from the web, and it is somewhat easy to use once you get used to the API and understand the hierarchical nature of web documents.\n",
    "\n",
    "The library `BeautifulSoup` assists this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIbraries used\n",
    "\n",
    "**`requests`**: \n",
    "This library is used for fetching data from web pages. \n",
    "[Click here for documentation](http://docs.python-requests.org/en/master/)\n",
    "\n",
    "**`BeautifulSoup`**: \n",
    "Use this library to extract tables, lists, paragraph from html web pages. \n",
    "It also allows filters to extract information from web pages. \n",
    "[Click here for documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "For a small and less complex project, Beautiful Soup can do the task pretty well. For a beginner, it's a good option. There are other packages available for handling more complex scraping tasks: \n",
    "\n",
    "1. [Selenium](https://selenium-python.readthedocs.io/): When you are dealing with Javascript featured website then Selenium could be the best choice. \n",
    "2. [Scrapy](https://scrapy.org/): For a large/complex project, Scrapy is the best choice. It's much easier to create web-crawler with Scrapy. \n",
    "\n",
    "---\n",
    "\n",
    "### Example: https://en.wikipedia.org/wiki/List_of_World_Series_champions\n",
    "\n",
    "Please click on the URL and review the page, visually.\n",
    "Then proceed with the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library to query a website\n",
    "import requests\n",
    "# import Beautiful soup library to access functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the url\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_World_Series_champions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open website URL and return the html to the variable 'response'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we get from web is typically html content. \n",
    "We can read the content of the server's response. \n",
    "Below, when a `BeautifulSoup` object is created from an html response, we explicitly reference the text format(`response.text`).\n",
    "\n",
    "The default encoding format is 'UTF-8' as shown below. \n",
    "\n",
    "[Click here for additional documentations about the response object.](http://docs.python-requests.org/en/master/user/quickstart/#response-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the html in the 'response' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(response.text, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Inspection\n",
    "Use `prettify` function to print the data in its nested html structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the table which has list of all baseball world series champions. \n",
    "\n",
    "This table should be present in one of the html tags.\n",
    "\n",
    "We can work with the tags to extract data present in them.  \n",
    "\"**soup.tag**\": will return the content between opening and closing tag including tag. \n",
    "\n",
    "Additionally, the `.string` value is the data between the tags.\n",
    "Compare the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return string within given tag \n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify the html tag**: \n",
    "The data is in a table. \n",
    "You can use inspect element option when you right click the mouse to identify the tag which has the data. \n",
    "\n",
    " * [Additional guide on webpage inspection](../resources/AnalyzingHTMLwithTheWebInspector.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the right table:** \n",
    "As we are seeking a table to extract information about baseball champions, we should identify the right table first. \n",
    "Let’s write the command to extract information within all table tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables=soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to identify the right table, we will use attribute “class” of table and use it to filter the right table.\n",
    "In _Chrome_, you can check the class name by **right clicking** on the required table of web page, then  `–> Inspect element –> Copy` the class name OR go through the output of above command find the class name of right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "right_table=soup.find('table', class_='wikitable sortable plainrowheaders')\n",
    "right_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are ready to process the data (scrape it), we will accumulate the columns, then assemble the final columns into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate lists\n",
    "Year=[]\n",
    "Winning_team=[]\n",
    "Winning_Manager=[]\n",
    "Games=[]\n",
    "Losing_team=[]\n",
    "Losing_Manager=[]\n",
    "Ref=[]\n",
    "\n",
    "# skip first iteration as we dont need headers \n",
    "for row in right_table.findAll(\"tr\")[1:]: \n",
    "#     print(row)\n",
    "    \n",
    "    game_year=row.findAll('th') # To store game year which is in <th> tag\n",
    "    cells = row.findAll('td') # To store all other details\n",
    "    \n",
    "    if len(cells)>2: # Only extract information if there is table body not heading\n",
    "        Year.append(game_year[0].find(text=True))\n",
    "        Winning_team.append(cells[0].find(text=True))\n",
    "        Winning_Manager.append(cells[1].find(text=True))\n",
    "        Games.append(cells[2].find(text=True))\n",
    "        Losing_team.append(cells[3].find(text=True))\n",
    "        Losing_Manager.append(cells[4].find(text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can push the extracted data into a DataFrame.\n",
    "\n",
    "Here, we need to iterate through each row (tr) and then assign each element of tr (td) to a variable and append it to a list. \n",
    "Let’s first look at the HTML structure of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(Year,columns=['Year'])\n",
    "df['Winning_team']=Winning_team\n",
    "df['Winning_Manager']=Winning_Manager\n",
    "df['Games']=Games\n",
    "df['Losing_team']=Losing_team\n",
    "df['Losing_Manager']=Losing_Manager\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "We can see that we have a Pandas dataframe filled with the data.\n",
    "We can now proceed to do data cleaning, manipulation, exploration, and persistence.\n",
    "\n",
    "\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
