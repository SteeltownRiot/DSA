{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. **As bulk of the information retrieval applications are build upon textual data, we need to learn about various text processing and representation techniques.**\n",
    "\n",
    "In this Lab, you will learn about Stemming and Lemmatization in a practical approach, applications of Stemming and Lemmatization, and how to stem and lemmatize sentences using the Python `nltk` package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root form of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change. The degree of inflection may be higher or lower in a language. As you have read the definition of inflection with respect to grammar, you can understand that an inflected word(s) will have a common root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,  \n",
    "> am, are, is $\\Rightarrow$ be  \n",
    "> car, cars, car's, cars' $\\Rightarrow$ car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above examples must have helped you understand the concept of normalization of text, although normalization of text is not restricted to only written document but to speech as well. Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words. Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization both tries to find the root word from an inflected work, but they differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are widely used in tagging systems, indexing, SEOs, Web search results, and information retrieval. For example, searching for fish on Google will also result in fishes, fishing as fish is the stem of both words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming using `nltk` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. Stem is the part of the word to which you add inflectional affixes. So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subroutine or function that stems word may be called a stemming algorithm, or stemmer. There are English and Non-English Stemmer classes/modules available in the `nltk` package. For our example here, we will use the PorterStemmer class. To use this class, first we have to import it from the `nltk` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need to create an object of this class, and with that we'll be able to use it to stem words as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(porter.stem(\"car\"))\n",
    "print(porter.stem(\"cars\"))\n",
    "print(porter.stem(\"cart\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(porter.stem(\"cat\"))\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"catch\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStemmer uses Suffix Stripping to produce stems. Notice how the PorterStemmer is giving the root of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. But if you look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because PorterStemmer algorithm does not follow linguistics rather a set of **5 rules** for different cases that are applied in phases to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.\n",
    "\n",
    "PorterStemmer is known for its simplicity and speed. It is commonly useful in Information Retrieval for fast recall and fetching of search queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other stemming algorithms in the `nltk` package, like `SnowballStemmer`, `LancasterStemmer`, etc. Students are encouraged to try them out themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization using `nltk` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know: Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words are words which do not contain important significance to be used in Search Queries. Usually, these words are filtered out from search queries because they return a vast amount of unnecessary information. Each programming language will give its own list of stop words to use. Mostly they are words that are commonly used in the English language such as 'as, the, be, are' etc. You can tell the stemmer to ignore stop-words, but for that you will need to download the stop-words corpus using `nltk.download()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "snowball = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(snowball.stem(\"from\"))\n",
    "print(snowball.stem(\"to\"))\n",
    "print(snowball.stem(\"having\"))\n",
    "print(snowball.stem(\"the\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the stopwords are ignored and root-words for them are not computed using the Stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are dealing with documents or sentences instead of individual words, we can also use the `tokenizer` that is built into the `nltk` package. These tokenizers will separate the document or sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization using `nltk` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "For example, troubles, troubling, troubled are all forms of the word trouble, therefore trouble is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words. We'll need to download the WordNet Database first to be able to use the Lemmatizer successfully.\n",
    "\n",
    "We will also need to provide the context in which we want to lemmatize, that is the parts-of-speech (POS). By default each word is automatically tagged as a **Noun**, and thus is not correctly lemmatized. We will need to tag the words we used as **Verbs (using \"v\")**. This is done by setting the value for the `pos` parameter in the `lemmatize()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"troubling\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"troubled\", pos=\"v\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know: POS-Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an option to tag each word programmatically, using the POS-Tagger provided by the `nltk` package. It can easily be used via the `pos_tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(nltk.pos_tag([\"going\", \"she\", \"car\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some additional checking after tagging each tokens (words) with their corresponding Parts-of-Speech, to be able to provide the parameter passed with the `pos` option. Students are encouraged to try Lemmatization with programmatically tagging the POS to each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use what"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "We can summarize that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
