{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Loading Text Search in Python Whoosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "\n",
    "For this exercise, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously worked with the _`book`_ data. In this exercise, we will work with some wiki data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "From the lab with the text files:\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content)`\n",
    "\n",
    "For this exercise, we will be using a few Wikipedia pages for our data source.\n",
    "\n",
    "### 1) For this exercise, you should look at a few of these web pages:\n",
    "\n",
    "  * https://en.wikipedia.org/wiki/Nyctimantis\n",
    "  * https://en.wikipedia.org/wiki/Osteocephalus\n",
    "  * https://en.wikipedia.org/wiki/Osteopilus\n",
    "  \n",
    "Specifically, inspect the HTML source and the \n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/table_inspect.png\" height=400 width=600 />\n",
    "\n",
    "\n",
    "\n",
    "**Task: You need to extend the above schema definition to collect this frog table data when available.**\n",
    "\n",
    "* Content will be the all visible text on the html page\n",
    "* Table information such as kingdom, phylum, class, order, family, subfamily, genus should be searchable "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# change this to a code cell and run if you have trouble with a locked writer\n",
    "from whoosh import writing\n",
    "writer.commit(mergetype=writing.CLEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                # Extend the schema definition to capture relevant table data\n",
    "                \n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this exercise, we have created a small folder of a few Wikipedia pages under the `en.wikipedia.org/wiki` folder in the common datasets folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /dsa/data/all_datasets/en.wikipedia.org/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will create the _whoosh_ index files in the `modules/module6/exercises/wiki_index` folder then ingest the files.\n",
    "\n",
    "To load the data, write a python script that follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    "## Follow the lab for Python IR with whoosh to complete this exercise.\n",
    "\n",
    "### 2) Create / Initialize the whoosh index and get the `writer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Step 2 below this comment\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Adapt the helper functions\n",
    "\n",
    "Note the subtle changes.\n",
    "Please adapt the code below such as provided recursive parsing of the HTML (.html) files, indexing with the Whoosh API.\n",
    "Trust no code, verify all code segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visible(element):  # return those html elements that are visible as text \n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']: #html tags\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)): # html comments\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def pullBiota(soup):  \n",
    "        \n",
    "    data = {}   \n",
    "   \n",
    "    table = soup.find(<write your code>)\n",
    "        \n",
    "    for row in table.find_all(<write your code>):\n",
    "        cells = row.find_all(<write your code>)\n",
    "        # TODO: process the cells and populate the data variable\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r') as infile:\n",
    "        content=infile.read()   # read html content\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        texts = soup.find_all(text=True)\n",
    "        \n",
    "        # Process all the visible text\n",
    "        visible_texts = filter(visible, texts)\n",
    "        \n",
    "        # TODO: Assemble all visible_texts into a content string\n",
    "        # Hint: Iterate over visible_texts line by line; remove newlines; create a concatenated string\n",
    "        \n",
    "\n",
    "        # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "        infotable = pullBiota(<write your code>)\n",
    "        \n",
    "        # Write to the index\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Parse with our defined functions in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start processing the folder and commit the work\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "### 5) Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html\n",
    "  \n",
    "Previously, we hard-coded query strings into the code cells.\n",
    "\n",
    "Now, use the `input()` function collect a query string from the user. \n",
    "Then execute the search. For this task, focus only on the `content` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write two example queries to ensure you can search the index \n",
    "\n",
    "That is, make sure you can search on the fields you added to the index from the infobox biota table.\n",
    "\n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "For this search, we will ignore `content` field and search over the other fields. We can use `MultifieldParser` to specify the fields of our interest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "from whoosh.qparser import MultifieldParser\n",
    "\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK WITH ALL EXECUTED CELLS\n",
    "# Then, `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
