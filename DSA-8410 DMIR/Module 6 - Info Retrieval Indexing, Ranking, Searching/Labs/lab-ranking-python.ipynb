{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring or Ranking Search Results\n",
    "\n",
    "Until this point you should be familiar with creating index and writing search queries. In coming cells, we will be using a scoring criteria while searching the indexes below. \n",
    "\n",
    "\n",
    "Normally the list of result documents is sorted by score. \n",
    "The `whoosh.scoring` module contains implementations of various scoring algorithms. \n",
    "The default is [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25). \n",
    "You can set the scoring object to use when you create the searcher using the weighting keyword argument: \n",
    "\n",
    "```python\n",
    "from whoosh import scoring\n",
    "\n",
    "with myindex.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    ... \n",
    "```\n",
    "    \n",
    "    \n",
    "A weighting model is a `WeightingModel` subclass with a `scorer()` method that produces a “scorer” instance. \n",
    "This instance has a method that takes the current matcher and returns a floating point score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IFD\n",
    "\n",
    "So why do we have to score the terms? \n",
    "Previously, we have simply used the number of times a token occurs in a document to classify the document. \n",
    "Even with the removal of stop words, however, \n",
    "this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). \n",
    "An alternative technique that often provides robust improvements in classification \n",
    "accuracy is to employ the frequency of token occurrence, \n",
    "normalized over the frequency with which the token occurs in all documents. \n",
    "In this manner, we give higher weight in the classification process to tokens \n",
    "that are more strongly tied to a particular label. \n",
    "\n",
    "Formally this concept is known as [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf–idf) (or tf-idf). \n",
    "We will use this scoring method and compare our the search results with those from the normal vector space model.\n",
    "\n",
    "In the below code cell, documents with a better TF-IDF score will appear higher in the search results list. \n",
    "Compare the results below with the results of the above cell which used the basic vector space model for scoring documents. \n",
    "Read the below documents to understand what TF-IDF is about and how it is applied in whoosh. \n",
    " \n",
    "\n",
    "-----\n",
    "\n",
    "Reference: \n",
    "\n",
    "- [Scoring and sorting](http://whoosh.readthedocs.io/en/latest/searching.html#scoring-and-sorting)\n",
    "- [TF-IDF](http://www.tfidf.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we processed only 3 documents. Before experimenting with the scoring method, let's process all the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "Earlier we stored file id and content for the schema. This time, we aim to index at the line level. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                line_num=ID(stored=True),  # this is new; we want to show line number in search result\n",
    "                content=TEXT(analyzer=StemmingAnalyzer(),stored=True)\n",
    "               )\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index, scoring\n",
    "\n",
    "# create an index dir\n",
    "\n",
    "os.makedirs(\"book_index\", exist_ok=True)  # create a directory for indexing\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"book_index\", schema)\n",
    "\n",
    "# Get a writer to form the created index in \n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions for processing files\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    line_no = 1\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as infile:\n",
    "        for line in infile:  # since we want to show line number is search, we need to process\n",
    "            # the document line by line. \n",
    "            line = line.rstrip('\\n')\n",
    "            line_no += 1\n",
    "            writer.add_document(filename=fname, line_num=str(line_no), content=line)\n",
    "    print(\"Indexed: \", fname)\n",
    "\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        # add a new line to separate folders in the output\n",
    "        print(\"\\nroot = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "processFolder(writer,\"/dsa/data/all_datasets/book\")\n",
    "\n",
    "writer.commit() # save changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow; font-weight:bold\">Comprehension Check:</span>  \n",
    "- What does the loadFile function do? \n",
    "- What does the processFolder function do? \n",
    "    - Do you understand how each directory gets processed?\n",
    "- Why are the two code lines at the bottom flush left? \n",
    "- Why is the line 'writer.commit' necessary?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below:\n",
    "# ------------------------\n",
    "\n",
    "What does the loadFile function do?\n",
    "- The loadFile function reads the file contents line by line, strips the new line character, and adds the line to the index with a line number.\n",
    "\n",
    "What does the processFolder function do?\n",
    "Do you understand the recursion in processFolder?\n",
    "- The processFolder function finds all the files and directories in a folder. \n",
    "    - For each text file, it creates a full filename path by concatenating the root and simple file name. It then prints \"Processing File\" and calls the function loadFile. \n",
    "        - If it is not a text file, it prints \"Unhandled File\".\n",
    "    - \"Under the hood\" the os.walk() function will recurse, or call itself, on each subfolder. This way, all files in each subfolder get processed in our function. \n",
    "\n",
    "Why are the two code lines at the bottom flush left?\n",
    "- The two lines at the bottom are flush left because they are not a part of the function. They are code that gets executed to call the processFolder function and then to write the results.\n",
    "\n",
    "Why is the line 'writer.commit' necessary?\n",
    "- writer.commit() finishes writing the index, and importantly, unlocks the index so it can be used by other processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "\n",
    "We now execute a query on this index. For ranking the search results, we first use the default one (i.e., [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25)), and then compare against the TF-IDF scoring system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "ix = index.open_dir(\"book_index\")   # open the index created earlier\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "\n",
    "q = qp.parse(\"love\")\n",
    "\n",
    "with ix.searcher() as s:  # using default scoring method\n",
    "    results = s.search(q)  \n",
    "    for hit in results:\n",
    "        filename = hit['filename']\n",
    "        line_num = hit['line_num']\n",
    "        \n",
    "        print(f\"filename: {filename:40s} line_num: {line_num:>4} score: {hit.score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the scoring method to TF-IDF and see if there is any difference in the ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"love\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "\n",
    "    for hit in results:\n",
    "        filename = hit[\"filename\"]\n",
    "        line_num = hit[\"line_num\"]\n",
    "        print(f\"filename: {filename:40s} line_num: {line_num:>4} score: {hit.score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that hits from the files **\"1samuel.txt\"** and **\"hosea.txt\"** have made it to top 10 while the lines from file **john.txt** which were at position 2 and 6 are changed to positions 4 and 3 because of the ranking based on TDIDF scores. \n",
    "\n",
    "<span style=\"background:yellow; font-weight:bold\">Comprehension Check:</span>  \n",
    "- Do you understand why (or how) the lines from 1samuel.txt and hosea.txt moved into the top 10 while the lines from john.txt changed in the rankings?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below:\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "- The differences are because the second cell is sorted by TD_IDF score and the first is sorted by BM25F score, with the better (higher) scored lines listed first. Since different scoring mechanism were used, the order is different. (This sorting order is followed whether we print out the scores or not.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Filtering results\n",
    "\n",
    "You can use the filter keyword argument in `search()` to white list the set of documents permitted in the results. \n",
    "The argument can be a `whoosh.query.Query` object, a `whoosh.searching.Results` object, \n",
    "or a set-like object containing document numbers. \n",
    "The searcher caches filters so if you use the same query filter with a searcher \n",
    "multiple times, the additional searches will be faster because the searcher will \n",
    "use the cache of results from previous runs of the query. \n",
    "You can also specify a mask keyword argument to specify a set of documents that are not permitted in the results. \n",
    "\n",
    "Lets first look up documents where `hate` is appearing.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(\"hate\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code cell, we are using the filter argument to only allow `john.txt` in the results and mask the word `hate`. \n",
    "So if you observe the results below, indexes in `john.txt` have appeared and none of the indexes have hate in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.query import *\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(\"love\")\n",
    "\n",
    "    # Only show documents in the \"rendering\" chapter\n",
    "    allow_q = Term(\"filename\", \"/dsa/data/all_datasets/book/john.txt\")\n",
    "    \n",
    "    # Don't show any documents where the \"content\" field contains \"hate\"\n",
    "    restrict_q = Term(\"content\",\"hate\")\n",
    "\n",
    "    results = s.search(user_q, mask=restrict_q, filter=allow_q)      #   \n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"], hit[\"content\"], hit.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow; font-weight:bold\">Comprehension Check:</span>  \n",
    "- How are filtering and masking different?\n",
    "- How could filtering and masking be useful?\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below:\n",
    "# ------------------------\n",
    "\n",
    "How are filtering and masking different?\n",
    "- Both are ways to combine queries\n",
    "- Filtering says what TO allow through (like whitelisting). It is like a logical AND : the results to keep must be in both the user_query results and the filter query results.\n",
    "- Masking says what to NOT allow through - what to keep out (like blacklisting). It is like in SQL saying user_query results LEFT JOIN mask query results.\n",
    "\n",
    "How could filtering and masking be useful?\n",
    "- Both are ways to narrow what is returned from the results, in order to construct very specific queries. \n",
    "\n",
    "NOTE: for restrict_q = Term('content', word), make sure word is stemmed and lowercase, because we specified that all the contents in the database are processed using StemmingAnalyzer. For example: if wanting to mask 'commandment' the word should be 'command', and there will be no commandment in the processed content.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "Let's put our results into a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.searching import Hit \n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(\"love\")\n",
    "    \n",
    "    results = s.search(user_q)\n",
    "    print(\"Total no of matches: \",len(results))\n",
    "    \n",
    "    rank=[]\n",
    "    docnum=[]\n",
    "    score=[]\n",
    "    filenames=[]\n",
    "    lines=[]\n",
    "    line_num=[]\n",
    "    \n",
    "    display(results[0])\n",
    "    \n",
    "    for i in np.arange(0,10):\n",
    "        rank.append(results[i].rank)\n",
    "        docnum.append(results[i].docnum)\n",
    "        score.append(results[i].score)\n",
    "        filenames.append(results[i]['filename'])\n",
    "        line_num.append(results[i]['line_num'])\n",
    "        lines.append(results[i]['content'])\n",
    "       \n",
    "    df = pd.DataFrame({'filename' : filenames, 'line_num' : line_num, 'line' : lines, 'docnum' : docnum, \\\n",
    "                            'score' : score, 'rank' : rank})\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
