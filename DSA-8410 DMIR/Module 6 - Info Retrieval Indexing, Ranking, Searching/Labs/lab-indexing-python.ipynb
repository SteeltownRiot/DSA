{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Text Search in Python\n",
    "\n",
    "In this notebook, we will explore a search engine library, Whoosh (funny name!). We can use this library to  create a naive search engine for our application. \n",
    "\n",
    "In Module 5, we learned about the first two steps in creating a search engine: data acquisition and data transformation, such as stemming and lemmatization. The next two steps are index creation and query processing. \n",
    "\n",
    "<img src=\"../images/indexing_process.png\" height=400 width=600 />\n",
    "\n",
    "Fig. An overview of document indexing\n",
    "\n",
    "<img src=\"../images/query_process.png\" height=400 width=600 />\n",
    "\n",
    "Fig. An overview of query processing\n",
    "\n",
    "\n",
    "Whoosh search engine library implements the following steps for us: \n",
    "\n",
    "1. Data transformation \n",
    "2. Document indexing\n",
    "3. Processing queries\n",
    "    * Parsing the query\n",
    "    * Ranking the results\n",
    "\n",
    "So all we need is to collect data and the rest will be taken care of by this library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='Whoosh_text' ></a>\n",
    "\n",
    "## Whoosh\n",
    "\n",
    "Whoosh was started as a quick and dirty search server for the online documentation of the Houdini 3D animation software package. \n",
    "They generously allowed the code to be open source, in case it might be useful to anyone else who needs a very flexible or pure-Python search engine (or both!).\n",
    "\n",
    "  * Whoosh is fast, but uses only pure Python, so it will run anywhere Python runs, without requiring a compiler.\n",
    "  * By default, Whoosh uses the Okapi BM25F ranking function, but like most things the ranking function can be easily customized.\n",
    "  * Whoosh creates fairly small indexes compared to many other search libraries.\n",
    "  * All indexed text in Whoosh must be unicode.\n",
    "  * Whoosh lets you store arbitrary Python objects with indexed documents.\n",
    "\n",
    "### What is Whoosh?\n",
    "\n",
    "Whoosh is a fast, pure Python search engine library.\n",
    "\n",
    "The primary design impetus of Whoosh is that it is pure Python. \n",
    "You should be able to use Whoosh anywhere you can use Python, no compiler or Java required.\n",
    "\n",
    "Like one of its ancestors, Lucene, Whoosh is not really a search engine, it’s a programmer library for creating a search engine.\n",
    "\n",
    "Practically no important behavior of Whoosh is hard-coded. \n",
    "Indexing of text, the level of information stored for each term in each field, parsing of search queries, \n",
    "the types of queries allowed, scoring algorithms, etc. are all customizable, replaceable, and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "**For this lab, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.**\n",
    "\n",
    "You previously read about the _`book`_ data and you have seen the data used for a corpus in a PostgreSQL full text search.\n",
    "Now, we are going to walk through the similar process to build the search engine in pure Python.\n",
    "The process will take very little time and the usability of the full text search is multiplied by degree of heterogeneous data that can be integrated with the full text search.\n",
    "\n",
    "Throughout these steps, try to recognize the similarities to the PostgreSQL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "In this module we will explore the Bible scripture. It is 4.6 megabytes of text and 31 thousand lines. These files are physically located here: `/dsa/data/all_datasets/book/`. \n",
    "\n",
    "**In whoosh, we structure the retrieval system by defining a storage schema, which defines the fields of documents in an index.**\n",
    "\n",
    "Ref: https://whoosh.readthedocs.io/en/latest/schema.html\n",
    "\n",
    "> Each document can have multiple fields, such as title, content, url, date, etc.\n",
    "\n",
    "> Some fields can be indexed, and some fields can be stored with the document so the field value is available in search results. Some fields will be both indexed and stored.\n",
    "\n",
    "\n",
    "Whoosh provides some useful predefined field types: TEXT, KEYWORD, ID, STORED, NUMERIC, DATETIME, BOOLEAN. We can use the appropriate types for defining the fields of a document. \n",
    "\n",
    "By default index is created for filed types TEXT and KEYWORD, but they are not stored. Within the TEXT type, we can specify a text analyzer. There is default analyzer, but we can also pass `StemmingAnalyzer`, `RegexAnalyzer`, and `CompositeAnalyzer` (Ref: https://whoosh.readthedocs.io/en/latest/analysis.html) \n",
    "\n",
    "\n",
    "Let's create a schema for these book chapters. Two make it simpler, we will store only two fields: name of the file and it's content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(\n",
    "    filename=ID(stored=True),  \n",
    "    # by default ID is indexed, but not stored; You would want to store the value of a url field so you \n",
    "    # could provide links to the original in your search results.\n",
    "    \n",
    "    # Use ID for fields like url or path (the URL or file path of a document), date, category – \n",
    "    # fields where the value must be treated as a whole, and each document only has one value for the field.\n",
    "    \n",
    "    content=TEXT(analyzer=StemmingAnalyzer())\n",
    "    \n",
    "    # TEXT filed indexes the text and stores term positions to allow phrase searching. \n",
    "    # For identifying terms, we can use various analyzers, and StemmingAnalyzer is one of them. \n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data and Creating Index\n",
    "\n",
    "For this lab, we have created a small folder of a few books in the datasets folder. We will process all the documents in the next lab. \n",
    "\n",
    "```Bash\n",
    "=> ls /dsa/data/all_datasets/book_lite\n",
    "acts.txt  numbers.txt  romans.txt\n",
    "```\n",
    "We will create the _whoosh_ index object on the folder and then a writer object, then ingest the files to add them to the reverse index. In _whoosh_, this is basically a table listing every word in the corpus, and for each word, the list of documents in which it appears. \n",
    "\n",
    "To load the data, we will execute a python script with follow the basic crawling behavior:\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    " We will create the index in the labs/book_lite_index folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "os.makedirs(\"book_lite_index\", exist_ok=True) \n",
    "\n",
    "# First we need an index for our directory\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"book_lite_index\", schema)\n",
    "\n",
    "# In whoosh, you need a writer object to write to an index\n",
    "# So we get a writer for the created index ix \n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will two functions: \n",
    "\n",
    "* loadFile: this function process a file \n",
    "* processFolder: this function recursively crawls and process all the files using the first function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(writer, fname):   # process a file \n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r') as infile:\n",
    "        content=infile.read()\n",
    "        writer.add_document(filename=fname, content=content)\n",
    "        print(\"Indexed: \", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFolder(writer, folder):  # recursively process all the files and folders \n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('# Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):  # identify all the subdirs and files in the current dir\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:     # process all the files in the currrent dir\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('=> Processing File:',filename)\n",
    "                loadFile(writer,filename)             # process a file by calling the above function\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "                \n",
    "        # Recurse into subfolders\n",
    "        for d in dirs:           # go inside these subdirs and process\n",
    "            print(\"recursing into \",d)\n",
    "            processFolder(writer,d)      # recursive call \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions defined,  get the party started:\n",
    "processFolder(writer,\"/dsa/data/all_datasets/book_lite\")\n",
    "writer.commit() # save changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "## Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct a QueryParser object to use in our queries. It takes parameters for the fieldname of the field we will be searching and the whoosh.fields.schema object to use when parsing. We only have two fields - filename and content, and content is what we'll be searching!\n",
    "\n",
    "```python\n",
    "\"QueryParser(\"content\", schema=ix.xchema) \n",
    "```\n",
    "Calling parse() on the query parser will parse the input string and return a whoosh.query.Query object/tree. \n",
    "\n",
    "```python\n",
    "q = qp.parse(\"abode\")\n",
    "```\n",
    "So q is our query in the form of a _whoosh_ query object. We want to search through the index and see what we find. We need to use a searcher object and use the search method on that object to look for our query. Then we can print out all the hits in our results!\n",
    "```python\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "\n",
    "ix = index.open_dir(\"book_lite_index\")   # open the index created earlier\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "q = qp.parse(\"abode\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some other queries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"judged OR power\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"wealth\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"mightest\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"weak AND powerless\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qp.parse(\"strong AND powerful\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can experiment with some queries of your own in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own query here\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own query here\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK, the `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
