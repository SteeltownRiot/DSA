{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Vector-Space-Models\" data-toc-modified-id=\"Vector-Space-Models-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Vector Space Models</a></div><div class=\"lev2 toc-item\"><a href=\"#Term-document-incidence\" data-toc-modified-id=\"Term-document-incidence-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Term-document incidence</a></div><div class=\"lev2 toc-item\"><a href=\"#Term-Frequency-(TF)\" data-toc-modified-id=\"Term-Frequency-(TF)-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Term Frequency (TF)</a></div><div class=\"lev2 toc-item\"><a href=\"#Inverse-Document-Frequency-(IDF)\" data-toc-modified-id=\"Inverse-Document-Frequency-(IDF)-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Inverse Document Frequency (IDF)</a></div><div class=\"lev2 toc-item\"><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>TF-IDF</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Models\n",
    "\n",
    "We have already explored a glimpse of vector space models in Module 1 where we converted each document as a vector of term (feature)  counts. In this module we will explore this notion a little deeper. According to this [wikipage](https://en.wikipedia.org/wiki/Vector_space_model), **Vector space model** or **term vector model** represents text documents as algebraic vectors of terms. \n",
    "\n",
    "\n",
    "## Term-document incidence \n",
    "\n",
    "The is the most simplest numerical representation of documents. Why do we need numerical representation for documents? Numerical representation allows us to apply many data mining and machine learning methods without any changes. A **term** is an indexed unit within a corpus (in a broad sense we can say terms are words). For each document we record whether a term is present or not; so we get a boolean vector for document and a boolean matrix for a corpus, which is known as **term-document incidence matrix**. \n",
    "\n",
    "\n",
    "## Term Frequency (TF)\n",
    "\n",
    "If a term occurs multiple times within a document, this term should receive more importance in the text analysis which cannot be handled by a term-document incidence vector. We can assign **weight** to each term based on the number of occurrences the term in the document. The simplest approach is to assign the weight to be equal to the number of occurrences of the term in document. This weighting scheme is called **term frequency**. Given the term frequencies (TFs) of document, it represents the document quantitatively. This representation is also known as **Bag of Words** model. Why? Because the exact ordering of the terms in a document is ignored but the number of occurrences of each term bears importance. So in this representation, the document ``A is greater than B`` is identical to the document ``B is greater than A``.\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "Raw term frequency poses a problem: all terms of considered equally important. Consider an example where the word \"sport\" occurs in each of the document of the corpus on sports. Does this word have any discriminating power for assessing relevancy or classification? No. To reduce the effect of terms that occur frequently across documents, the notion of **inverse document frequency** (IDF) is introduced. Formally, idf is defined as follows: \n",
    "\n",
    "$$\n",
    "\\text{idf}_t = \\log \\frac{N}{\\text{df}_t}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "$N$: Number of documents in the corpus\n",
    "$df_t$: Document frequency of term t; number of documents that contain term t\n",
    "$idf_t$: Inverse document frequency of t\n",
    "\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Both term frequency and inverse document frequency can be combined to give a composite score (importance) for term in each document. Formally, the tf-idf weighting scheme assigns to term $t$ a weight in document $d$ as follows: \n",
    "\n",
    "$$\n",
    "\\text{tf-idf}_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t.\n",
    "$$\n",
    "\n",
    "**Observations: (see Ch 6 of Introduction to Information Retrieval)**\n",
    "\n",
    "TF-IDF score has following properties.\n",
    "\n",
    "1. highest when t occurs many times within a small number of documents (thus lending high discriminating power to those documents);\n",
    "2. lowerwhenthetermoccursfewertimesinadocument,oroccursinmany documents (thus offering a less pronounced relevance signal);\n",
    "3. lowest when the term occurs in virtually all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additonal Reading\n",
    " - [Video on TF and IDF](https://web.dsa.missouri.edu/static/videos/DMIR/DATA_SCI_8630_IDF.mp4)\n",
    " - [What does tf-idf mean?](http://www.tfidf.com/)\n",
    " - [Term and Frequency Weighting](https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html)\n",
    " - [Inverse Document Frequency](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html)\n",
    " - [TFIDF Overview](https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html)\n",
    " - Extra Reading\n",
    "     - [Even More TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30.1705px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
