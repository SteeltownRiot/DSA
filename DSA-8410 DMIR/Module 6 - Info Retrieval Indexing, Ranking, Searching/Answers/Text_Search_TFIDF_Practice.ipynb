{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building and Loading Text Search in Python Whoosh using TFIDF\n",
    "\n",
    "For this Practice, \n",
    "we will be creating full text search capability using Python as we did in the Lab, using TFIDF scoring. \n",
    "\n",
    "This time, our data is in the folder **`/dsa/data/all_datasets/hp`**  - but no, \n",
    "this is not Hewlett Packard documentation. \n",
    "It is something much more enchanting!\n",
    "\n",
    "**Please review some of the files in /dsa/data/all_datasets/hp).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /dsa/data/all_datasets/hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the first 10 lines of a file, we can use `head` command from bash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 10 /dsa/data/all_datasets/hp/CHAPTER\\ 1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the practice, reflection questions are asked. \n",
    "Take the time to answer them - consult the documentation for libraries and functions if needed, \n",
    "experiment with the code, and ask your classmates.\n",
    "\n",
    "\n",
    "## 1. Building the Whoosh Schema that support searches at the line level\n",
    "\n",
    "Import the necessary libraries and build a schema including filename, line_num and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "#TO DO: build the schema\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                line_num=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    "\n",
    " - Explain how you built the schema - did you use ID, TEXT, KEYWORD or STORED? \n",
    " - If so, where and why? ([Documentation available here](http://whoosh.readthedocs.io/en/latest/schema.html))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "Which libraries did you import and why?\n",
    "\n",
    "The whoosh.fields module contains functions and classes related to fields. We are specifically importing the following classes:\n",
    "\n",
    "- whoosh.fields.schema lets us define a schema. It represents a collection of fields in an index.\n",
    "- whoosh.fields.TEXT: a predefined field type for text fields\n",
    "- whoosh.fields.ID: a predefined field type that indexes the entire value of the field as one token.\n",
    "These classes are not needed, as we are not using them as field types. They can be ommitted.\n",
    "- whoosh.fields.KEYWORD: a predefined field type for space or comma separated keywords, like tags.\n",
    "- whoosh.fields.STORED: a predefined field type for fields you want to store but not index\n",
    "\n",
    "The whoosh.analysis module has classes and functions that allow tokenizing a block of text. Different analyzers that tokenize in different ways or with different filters are a part of this module.\n",
    "\n",
    "# -----------------------\n",
    "Explain how you built the schema - did you use ID, TEXT, KEYWORD or STORED?\n",
    "If so, where and why?\n",
    "\n",
    "We used ID for the fieldname and the line number because we wanted the entire field to be one token and we wanted them indexed. We used the stored parameter so they would be stored with the document.\n",
    "\n",
    "We used TEXT for the content, because it is a block of text. We used the analyzer parameter to assign a StemmingAnalyzer to index the field contents.\n",
    "\n",
    "We did not use KEYWORD since no field needed to be entered as tags. \n",
    "\n",
    "We did not use STORED because we had no field we wanted stored but not indexed. The STORED predefined field type is not the same as using the stored parameter with ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Data\n",
    "\n",
    "* In the first cell, import any libraries you need, create the index in the folder `hp_index` within the practices folder, and get a writer for the index.\n",
    "* In the second cell, complete the function for loadFile\n",
    "* In the third cell, process the folder and persist your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "#import os, os.path\n",
    "import  os\n",
    "from whoosh import index\n",
    "\n",
    "#TO DO: Create the index\n",
    "\n",
    "# create index dir\n",
    "os.makedirs(\"hp_index\", exist_ok=True)\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"hp_index\", schema)\n",
    "\n",
    "\n",
    "#TO DO: Get a writer form the created index in \n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete code below \n",
    "# -----------------------\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    line_no = 1\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as infile:\n",
    "        # TODO: create indexes for each line in the input file\n",
    "        for line in infile:\n",
    "            line = line.rstrip('\\n')\n",
    "            line_no += 1\n",
    "            writer.add_document(filename=fname, line_num=str(line_no),content=line)\n",
    "        #-------------------------------------------------------\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        # add a new line to separate folders in the output\n",
    "        print(\"\\nroot = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('root:', root, '; file:', file, '; filename:', filename)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "# TODO: process the folder and persist your changes \n",
    "# Functions defined,  get the party started:\n",
    "processFolder(writer,\"/dsa/data/all_datasets/hp\")\n",
    "\n",
    "writer.commit() # save changes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    " - In loadFile, how did you get the line number for each line?\n",
    " - In loadFile, which code line adds an index for the processed line?\n",
    " - In processFolder, what does the following line do? Give an example.\n",
    "```\n",
    "filename = os.path.join(root, file)\n",
    "```\n",
    " - What code line makes sure the index get persisted? (How is it saved so it can be used?)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "Which libraries did you import and why?\n",
    "We need either os or os.path - not really both. os.path would only import what we need, and would take up less memory\n",
    "- The os module is an interface to operating system functionality. \n",
    "- The os.path module lets us see and manipulate paths\n",
    "- whoosh.index contains the main functions/classes for creating, maintaining, and using a whoosh index. These include create_in() and writer(). Apparently it also contains the writer class (which is returned when calling writer() because we did not have to load the writer module separately.\n",
    "\n",
    "In loadFile, how did you get the line number for each line?\n",
    "- creating a variable line_no and incrementing it each time through the loop\n",
    "\n",
    "In loadFile, which code line adds an index for the processed line?\n",
    "- writer.add_document(filename=fname, line_num=str(line_no),content=line)\n",
    "\n",
    "In processFile, what does the following line do? [filename = os.path.join(root, file)] Give an example.\n",
    "- concatenates the root directory (hp) to the name of the file (CHAPTER 1.txt) to form the path to the file (hp/CHAPTER 1.txt)\n",
    "\n",
    "What code line makes sure the index get persisted? \n",
    "- writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Queries\n",
    "* In the first cell, import any libraries you need, and find the indexes of lines where the string 'Harry' appears. Display the top 10 hits.\n",
    "* In the second cell, import any additional libraries you need, and find the indexes of lines where the string 'Harry' appears using TF-IDF as the scoring mechanism. Display the top 10 hits.\n",
    "* In the third cell, import any additional libraries you need, and use a filter to list the indexes in chapter 6 corresponding to the search string 'Harry' using TF_IDF as the scoring mechanism. Display the top 10 hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "#TO DO: Find the indexes of lines where the string 'Harry' appears. \n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(\"Harry\")\n",
    "\n",
    "#TO DO: display the top 10 hits\n",
    "# NOTE: By default the results contains at most the first 10 matching documents. \n",
    "# So using a count is not necessary\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit['line_num'], hit.score, hit.rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "#TO DO: Find the indexes of lines where the string 'Harry' appears using TF_IDF as the scoring mechanism. \n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(\"Harry\")\n",
    "\n",
    "#TO DO: display the top 10 hits\n",
    "w = scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)\n",
    "#with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "with ix.searcher(weighting=w) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit['line_num'], hit.score, hit.rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "#from whoosh.query import *\n",
    "from whoosh.query import Term\n",
    "\n",
    "#TO DO: Use a filter to list the indexes in chapter 6 corresponding to the search string 'Harry' \n",
    "# using TF_IDF as the scoring mechanism. \n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(\"Harry\")\n",
    "\n",
    "    # Only show documents in the \"rendering\" chapter\n",
    "    allow_q = Term(\"filename\", \"/dsa/data/all_datasets/hp/CHAPTER 6.txt\")\n",
    "\n",
    "    #TO DO: display the top 10 hits\n",
    "    results = s.search(user_q, filter=allow_q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit['line_num'], hit.score, hit.rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    " - What differences do you see in the results of the first two cells?\n",
    " - What do those differences mean?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "Which libraries did you import and why?\n",
    "- whoosh.qparser.QueryParser: This class allows a user-entered string to be converted to a query object.\n",
    "- whoosh.scoring module includes classes for scoring and sorting search results\n",
    "- `from whoosh.query import *` imports all the classes in the query module.\n",
    "- Or alternatively, `from whoosh.query import Term` would import the Term class which is neededto Only show documents in the \"rendering\" chapter\n",
    "\n",
    "What differences do you see in the results of the first two cells?\n",
    "- The 10 selected results are different. Only two lines appear in both lists, but they are in different places in the lists.\n",
    "\n",
    "What do those differences mean?\n",
    "- The differences are because the second cell is sorted by TD_IDF score and the first is sorted by BM25F score, with the better (higher) scored lines listed first. Since different scoring mechanism were used, the order is different.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "175.625px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
