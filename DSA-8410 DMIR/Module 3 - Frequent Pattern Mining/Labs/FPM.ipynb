{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Frequent Pattern Mining and Association Rule Analysis\n",
    "\n",
    "A **frequent pattern** is a pattern, such as a set of items, subsequences, or substructures, that frequently occurs in a dataset. **Frequent pattern mining** searches for such recurring relationships in a given data set.\n",
    "For example, a **frequent itemset** is a set of items, such as bread and milk, that frequently appear together in a transaction data set. A subsequence, such as buying first a PC, then a digital camera, and then a memory card, if it frequently occurs in a shopping history database, is a **frequent sequential pattern**. A substructure can refer to different structural forms, such as subgraphs, subtrees, or sublattices. If A substructure frequently occurs in a graph database is called a **frequent structural pattern**. Frequent pattern mining is also known as **frequent episode mining** or **affinity analysis**. In this notebook, we will focus on mining itemsets.\n",
    "\n",
    "\n",
    "A typical example of frequent itemset mining is market basket analysis. This process analyzes customer buying habits by finding associations between the different items that customers place in their \"shopping baskets.\" The discovery of these associations can help retailers develop marketing strategies by gaining insight into which items are frequently purchased together by customers. For instance, if customers buy milk, how likely are they to also buy bread (and what kind of bread) on the same trip to the supermarket? This information can lead to increased sales by helping retailers do selective marketing and plan their shelf space. \n",
    "\n",
    "<img src='../images/market_data.PNG'>\n",
    "\n",
    "**Fig. A transaction dataset.** \n",
    "\n",
    "\n",
    "In this dataset, bread and butter occur together in 60% of the transactions. So the occurrence of {bread, butter} is a frequent pattern. How can we take advantage of this knowledge? In one strategy, items that are frequently purchased together can be placed in proximity to further encourage the combined sale of such items. In an alternative strategy, placing bread and butter at opposite ends of the store may entice customers who purchase such items to pick up other items along the way. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing collections of itemsets\n",
    "\n",
    "\n",
    "A transaction database contains sets of items (itemsets) together with additional information. For example, a transaction in the database contains a transaction ID and an itemset. Collections of itemsets used for transaction databases can be represented as **binary incidence matrices** with columns corresponding to the items and rows corresponding to the itemsets. The matrix entries represent the presence (1) or absence (0) of an item in a particular itemset. Shown below on the left, an example of a binary incidence matrix containing itemsets for above supermarket data example and vertical layout for the same is shown on right. \n",
    "\n",
    "<img src=\"../images/data_representation.PNG\">\n",
    "\n",
    "\n",
    "Since a typical frequent itemset or a typical transaction (e.g., a supermarket transaction)\n",
    "only contains a small number of items compared to the total number of available items, the\n",
    "binary incidence matrix will in general be very sparse with many items and a very large\n",
    "number of rows. A natural representation for such data is a sparse matrix format.\n",
    "\n",
    "\n",
    "A set of items is referred to as an **itemset**. An itemset that contains $k$ items is a $k$-itemset. The set {bread, butter} is a 2-itemset. The **occurrence frequency** of an itemset is the number of transactions that contain the itemset. This is also known, simply, as the **frequency**, **support count**, or **count** of the itemset. If the support of an itemset satisfies a prespecified **minimum support threshold**, then the itemset is a frequent itemset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining or Affinity Analysis or Frequent Episode Mining\n",
    "\n",
    "In general, association rule mining can be viewed as a two-step process:\n",
    "\n",
    "1. Find all frequent itemsets: By definition, each of these itemsets will occur at least as\n",
    "frequently as a predetermined minimum support count, min sup.\n",
    "2. Generate strong association rules from the frequent itemsets: By definition, these\n",
    "rules must satisfy minimum support and minimum confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Finding Frequent Itemsets\n",
    "\n",
    "**Apriori** Method is a basic algorithm for finding frequent itemsets. Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore (k + 1)-itemsets. First, the set of frequent 1-itemsets is found by scanning the database to accumulate the count for each item, and collecting those items that satisfy minimum support. The resulting set is denoted by L1. Next, L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database. To improve the efficiency of the level-wise generation of frequent itemsets, an important property called the Apriori property is used to reduce the search space.\n",
    "\n",
    "**Apriori property: All nonempty subsets of a frequent itemset must also be frequent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:a40c94ef-fcf8-4c03-ae78-d3d210f5a073.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generating Association Rules from Frequent Itemsets\n",
    "\n",
    "Frequent itemset patterns can be represented in the form of **association rules.** For example, the information that customers who purchase bread also tend to buy butter at the same time is represented in the following association rule: \n",
    "\n",
    "```\n",
    "bread => butter [support = 60%, confidence = 75%]. \n",
    "```\n",
    "\n",
    "Frequent if-then associations called association rules which consists of an **antecedent** (if) and a **consequent** (then). Rule **support** and **confidence** are two measures of rule interestingness. They respectively reflect the usefulness and certainty of discovered rules. A support of 60% for a rule means that 60% of all the transactions under analysis show that bread and butter are purchased together. A confidence of 75% means that 75% of the customers who purchased bread also bought butter. Typically, association rules are considered **interesting** if they satisfy both a **minimum support threshold** and a **minimum confidence threshold**. These thresholds can be set by users or domain experts. Additional analysis can be performed to discover interesting statistical correlations between associated items. \n",
    "\n",
    "**A formal definition of association rules is as follows:**\n",
    "\n",
    "Let $I = {i_1, i_2, . . . , i_n}$ be a set of **items**. Let $D = {t_1, t_2, . . . , t_m}$\n",
    "be a set of **transactions called the database**. Each transaction in $D$ has a unique transaction ID and contains a subset of the items in $I$. A **rule** is defined as an implication of the form $X \\Rightarrow Y$ where $X,Y \\subseteq I$ and $X \\cap Y = \\emptyset$. The sets of items (for short **itemsets**) $X$ and $Y$ are called **antecedent** (LHS) and **consequent** (RHS) of the rule, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Frequent Itemset Patterns and Association Rules\n",
    "\n",
    "*Association rules* are rules which surpass a user-specified **minimum support** and **minimum confidence thresholds**. \n",
    "\n",
    "### Support \n",
    "\n",
    "The **support supp(X) of an itemset** $X$ is the number of transactions that contain $X$, i.e., $sup(X) = \\left| t(X) \\right|$. The **relative support** is defined as the proportion of transactions in the dataset which contain the itemset $X$, i.e., $rsup(X) = \\frac{\\left| t(X) \\right|}{\\left| D \\right|}$. Sometimes support and and relative support are used interchangeablly. In this lab we will use the word **support** to denote both **support** and **relative support**. \n",
    "\n",
    "The **support of the rule** is defined as the proportion of transactions that contain both $X$ and $Y$, that is, $sup(X \\Rightarrow Y ) = sup(XY) = \\frac{sup(X \\cup Y )}{sup(D)}$. This estimation is the **empirical joint probability** of the items ($X \\cup Y$) comprising the rule. \n",
    "\n",
    "The range of support is $[0, 1]$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence\n",
    "\n",
    "The **confidence of a rule** is defined as \n",
    "\n",
    "$\\text{conf}(X \\Rightarrow Y ) = \\frac{\\text{sup}(X \\cup Y )}{\\text{sup}(X)} = \\frac{\\text{count}(X \\cup Y )}{\\text{count}(X)}$ \n",
    "\n",
    "This is the **conditional probability** estimation of $P(Y|X)$. The range of confidence is $[0, 1]$.\n",
    "\n",
    "\n",
    "Rules that satisfy both a minimum support threshold (min sup) and a minimum confidence threshold (min conf ) are called **strong**.\n",
    "\n",
    "\n",
    "Therefore, an association rule $X \\Rightarrow Y$ will satisfy the following condition where $\\sigma$ and $\\delta$ are the minimum support and minimum confidence, respectively.\n",
    "\n",
    "$$ sup(X \\cup Y ) â‰¥ \\sigma $$\n",
    "\n",
    "                                                and\n",
    "\n",
    "$$ conf(X \\Rightarrow Y) â‰¥ \\delta $$\n",
    "\n",
    "\n",
    "In addition to support and confidence scores, other scores are also used to assess rules.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift \n",
    "\n",
    "Another popular measure for association rules is lift. Lift is defined as the ratio of the observed joint probability of X and Y to the expected joint probability if they were statistically independent. This score is used to measure surprise in a rule. The lift of a rule is defined as:\n",
    "\n",
    "$$ \\text{lift}(X \\Rightarrow Y) = \\frac{sup(X \\cup Y )}{(sup(X)sup(Y))} =  \n",
    "   \\frac{conf(\\Rightarrow Y)}{sup(Y)}$$\n",
    "\n",
    "In other words, the above formula can be interpreted as the deviation of the support of the whole rule from the support expected under independence. The range of lift is $[0, \\inf]$. A lift value close to 1 means that the support of a rule is expected considering the supports of its components. We usually look for values that are much larger (i.e., above expectation) or smaller than 1 (i.e., below expectation).\n",
    "\n",
    "- If $\\text{lift}(X \\Rightarrow Y) \\sim 1$, expected\n",
    "- If $\\text{lift}(X \\Rightarrow Y) < 1$, below expectation\n",
    "- If $\\text{lift}(X \\Rightarrow Y) > 1$, above expectation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage\n",
    "\n",
    "Leverage computes the difference between the support of X and Y appearing together and the support  that would be expected if X and Y were independent. A leverage value of 0 indicates independence. The range of leverage is $[âˆ’1,1]$. \n",
    "\n",
    "$$ leverage(X \\Rightarrow Y) = sup(X \\Rightarrow Y) -  sup(X)sup(Y)$$\n",
    "\n",
    "\n",
    "Leverage gives an \"absolute\" measure of how surprising a rule is and it should be used\n",
    "together with lift. Considering lift in isolation may be misleading because rules with different support may have the same lift. Like lift it is symmetric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conviction\n",
    "\n",
    "Conviction measures the dependency of the consequent upon the antecedent. The higher the conviction score, the higher the dependence. From another perspective, conviction measures the expected error of the rule, that is, how often X occurs in a transaction where Y does not. It is thus a measure of the strength of a rule with respect to the complement of the consequent. \n",
    "\n",
    "Formally, \n",
    "\n",
    "$$ \n",
    "conviction(X \\Rightarrow Y) \n",
    "= \\frac{sup{X})}{1 - \\text{conf}(X \\Rightarrow Y)}\n",
    "= \\frac{1 - sup(Y)}{1 - \\text{conf}(X \\Rightarrow Y)}  \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For a perfect confidence score (i.e 1), the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Measures like support, confidence, lift, leverage, and conviction are generally called interest measures because they help with focusing on potentially more interesting rules.**\n",
    "\n",
    "[Reading](https://en.wikipedia.org/wiki/Association_rule_learning#Lift) for a more detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this module we will be using [mlxtend](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/) python package to demonstrate frequent pattern mining and association rule mining. We will apply these mining algorithms on a toy transacton dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "In this practice, we will explore some retail dataset. The data is located here: `/dsa/data/DSA-8410/association-mining/retail_dataset.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this to read data from the csv file on local system.\n",
    "df = pd.read_csv('/dsa/data/DSA-8410/association-mining/retail_dataset.csv') \n",
    "\n",
    "## Print first 10 rows \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of transactions = {df.shape[0]}\")\n",
    "print(f\"Maximum num of items per transaction = {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s find out the unique items in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To apply `apriori` method, we need to convert the dataset to a binary incidence matrix. We can us sklearn's `MultiLabelBinarizer` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "trans_data = []\n",
    "for indx, row in df.iterrows():\n",
    "    trans_data.append(row.dropna().values)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = mlb.fit_transform(trans_data)\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data_enc = pd.DataFrame(data, columns=mlb.classes_)\n",
    "trans_data_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indentify Frequent Patterns with Apriori Method\n",
    "\n",
    "Apriori module from mlxtend library provides fast and efficient apriori implementation.\n",
    "\n",
    "```\n",
    "Default Parameters: apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False)\"\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- df : One-Hot-Encoded DataFrame or DataFrame that has 0 and 1 or True and False as values\n",
    "- min_support : Floating point value between 0 and 1 that indicates the minimum support required for an itemset to be selected. Number of observation with item / total observation# of observation with item / total observation\n",
    "- use_colnames : This allows to preserve column names for itemset making it more readable.\n",
    "- max_len : Max length of itemset generated. If not set, all possible lengths are evaluated.\n",
    "- verbose : Shows the number of iterations if >= 1 and low_memory is True. If =1 and low_memory is False , shows the number of combinations.\n",
    "- low_memory : If True, uses an iterator to search for combinations above min_support. Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3â€“6x slower than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_items = apriori(trans_data_enc, min_support=0.2, use_colnames=True, verbose=1)\n",
    "freq_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_items = freq_items.reindex(columns=['itemsets', 'support'])\n",
    "freq_items['length'] = freq_items['itemsets'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Association Rules from Frequent Itemsets\n",
    "\n",
    "Once the frequent itemsets from transactions in a database have been found, it is straightforward to generate strong association rules from them (where strong association rules satisfy both minimum support and minimum confidence). This can be done using  confidence, which we show again here for completeness:\n",
    "\n",
    "$\\text{conf}(X \\Rightarrow Y ) = P(Y|X) = \\frac{\\text{sup}(X \\cup Y )}{\\text{sup}(X)} = \\frac{\\text{count}(X \\cup Y )}{\\text{count}(X)}$. \n",
    "\n",
    "\n",
    "We now apply `association_rules` on the frequent itemsets found in the earlier step. The function definition is as follows: \n",
    "\n",
    "```\n",
    "association_rules(df, metric=â€™confidenceâ€™, min_threshold=0.8, support_only=False)\n",
    "```\n",
    "\n",
    "Metric can be set to `confidence`, `lift`, `support`, `leverage` and `conviction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.6)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given measures such as supports, confidence, leverage, and conviction, we can subset rules and inspect them. E.g, let's see the rules that has occured at least 20% of the time with at least 80% of confidence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[(rules['support'] >= .20) & (rules['confidence'] >= .80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and probing interesting rules\n",
    "\n",
    "Sometimes visualizing various measures may help us identify interesting rules. Let's perform some exploration on these scores to see the landscape of patterns.  \n",
    "\n",
    "### 1. Confidence vs Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rules['support'], rules['confidence'], alpha=0.5)\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('Confidence vs Support')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Lift vs Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rules['support'], rules['lift'], alpha=0.5)\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('lift')\n",
    "plt.title('Lift vs Support')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confidence vs Lift\n",
    "\n",
    "Both lift and confidence are used for identifying strong rules. In the following plot, we can see confidence increases with the increase of lift except for one pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(rules['lift'], rules['confidence'], 1)\n",
    "fit_fn = np.poly1d(fit)\n",
    "\n",
    "plt.plot(rules['lift'], rules['confidence'], 'yo')\n",
    "plt.plot(rules['lift'], fit_fn(rules['lift']));\n",
    "\n",
    "plt.ylabel('confidence')\n",
    "plt.xlabel('lift')\n",
    "plt.title('Confidence vs Lift')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interestng patterns\n",
    "\n",
    "Lift and leverage are used to identify interesting patterns. These measures assess how surprising a rule is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rules['lift'], rules['leverage'], alpha=0.5)\n",
    "plt.xlabel('lift')\n",
    "plt.ylabel('leverage')\n",
    "plt.title('Lift vs Leverage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_sub = rules[(rules['lift'] >= 1.6)]\n",
    "rules_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_sub = rules[(rules['leverage'] >= 0.08)]\n",
    "rules_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
