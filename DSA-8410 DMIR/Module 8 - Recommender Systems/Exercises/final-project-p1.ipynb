{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Computational Linguistics over Reddit Data\n",
    "\n",
    "For this project we are going to ingest Reddit posts, process the data and perform computational linguistics over the Reddit posts.\n",
    "\n",
    "This project will build off of some work you have previously done. However, beyond that exercise of processing and cataloging the feeds, in this instance you will access the referenced Reddit post and perform computational linguistics over the post itself.\n",
    "\n",
    "![DataScraper_To_NLP.png MISSING](../images/DataScraper_To_NLP.png)\n",
    "\n",
    "---\n",
    "\n",
    "### From the site:\n",
    "\n",
    "reddit: https://www.reddit.com/  \n",
    "Reddit gives you the best of the Internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos just for you.\n",
    "\n",
    "\n",
    "### From Wikipedia:\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. \n",
    "Registered members submit content to the site such as links, text posts, and images, \n",
    "which are then voted up or down by other members. \n",
    "Posts are organized by subject into user-created boards called \"subreddits\", \n",
    "which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. \n",
    "Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough votes, ultimately on the site's front page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Posting:\n",
    "\n",
    "The below link is an example post from someone that was tinkering with sentiment analysis; specifically they looked at the text of [Moby Dick](https://en.wikipedia.org/wiki/Moby-Dick).\n",
    "\n",
    "**Spoiler:** The conclusion was that the book is rather negative in sentiment.\n",
    "It is after all, about vengeance!\n",
    "\n",
    "https://www.reddit.com/r/LanguageTechnology/comments/9whk23/a_simple_nlp_pipeline_to_calculate_running/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From: https://www.redditinc.com/\n",
    "![REDDIT_About.png MISSING](../images/REDDIT_About_latest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example Code:\n",
    "\n",
    "In this exercise, we will be using Reddit API for fetching the latest messages. We can also fetch recent posts from Reddit using web feeds (check [here](./rss-feeds.ipynb)), but it seems our IP got banned for excessive requests to Reddit over the last few days. So we will be using Reddit API for which you are required to create your Reddit account and an app. \n",
    "\n",
    "Follow [this article](https://gilberttanner.com/blog/scraping-redditdata) to create your credentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reddit API\n",
    "\n",
    "For fetching Reddit data using API, we will be using a Python wrapper to Reddit API: [PRAW: The Python Reddit API Wrapper](https://github.com/praw-dev/praw)\n",
    "\n",
    "Documentation: https://praw.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='', \n",
    "                     client_secret='', \n",
    "                     user_agent='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "hot_posts = reddit.subreddit('datascience').hot(limit=10)  # hot posts\n",
    "\n",
    "# new_posts = reddit.subreddit('datascience').new(limit=10)  # new posts\n",
    "\n",
    "# get hottest posts from all subreddits\n",
    "# hot_posts = reddit.subreddit('all').hot(limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = list(hot_posts)  \n",
    "\n",
    "# this line will initiate the fetching of posts as PRAW use a lazy approach (i.e, fetch when required)\n",
    "# this part is done to avoid calling Reddit API multiple times while developing our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for post in all_posts:\n",
    "    print(f\"id : {post.id}\")\n",
    "    print(f\"title : {post.title}\")\n",
    "    print(f\"url : {post.url}\")\n",
    "    print(f\"author : {str(post.author)} {type(str(post.author))}\")\n",
    "    print(f\"score : {post.score} {type(post.score)} \")\n",
    "    print(f\"subreddit : {post.subreddit} {type(post.subreddit)} \")\n",
    "    print(f\"num_comments : {post.num_comments}\")\n",
    "    print(f\"body : {post.selftext}\")\n",
    "    print(f\"created : {post.created}\")\n",
    "    print(f\"link_flair_text : {post.link_flair_text}\")\n",
    "    break  # break the loop after printing information about the first post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Reddits\n",
    "\n",
    "As described above, sub-reddits are communities organized around particular topics.\n",
    "\n",
    "Some example sub-reddits:\n",
    " * https://www.reddit.com/r/datascience/\n",
    " * https://www.reddit.com/r/MachineLearning/\n",
    " * https://www.reddit.com/r/LanguageTechnology/\n",
    " * https://www.reddit.com/r/NLP/\n",
    " * https://www.reddit.com/r/Python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Tasks\n",
    "\n",
    "## Part I: Data Acquisition and Loading \n",
    "1. Choose a subreddit of your choice. Preferably something of interest to you. \n",
    "1. Conceptualize a database design that can collect the data.\n",
    "    * Make sure your items (posts) are unique and not duplicated!\n",
    "    * Make sure you capture at least title, author, subreddit, tags, title link, and timestamp\n",
    "    * Along with the metadata, capture all the text into one or more data field(s) suitable for information retrieval\n",
    "    * Write triggers for auto updates of IR related fields\n",
    "    * Add index (either GIN or GiST) for the IR related fields\n",
    "    * Additionally, design a field to hold:\n",
    "        * Sentiment\n",
    "1. Implement the database in your PostgreSQL schema\n",
    "1. Implement cells of Python Code that \n",
    "    * collect the latest posts from a subreddit of your choice (**should be text-dominant not image/video**) and collect at least 500 posts (if possible), \n",
    "    * processes the messages to extract metadata, \n",
    "    * process the text for IR, and \n",
    "    * perform computational linguistics (i.e, extract sentiment scores), \n",
    "    * then insert the data into your database.\n",
    "1. After you have loaded data from a subreddit, choose a few more subreddits and load those!\n",
    "\n",
    "## Part II: Analytics \n",
    "\n",
    "1. Write some test queries following the text vectors from Module 7.\n",
    "1. Produce **interesting visualizations** of the linguistic data.\n",
    "    * Try to look for trends (within a subreddit) and and variations of topics across subreddits\n",
    "    * Some comparative plots across feeds\n",
    "1. Write a summary of your findings!\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Acquisition and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Design your database\n",
    "\n",
    "Conceptualize a database design that can collect the data.\n",
    "* Make sure your items (posts) are unique and not duplicated!\n",
    "* Make sure you capture at least title, link, author, subreddit, tag/flair, and timestamp\n",
    "* Capture all the body text into fields suitable for information retrieval\n",
    "* Write triggers for auto updates of IR related fields\n",
    "* Add index (either GIN or GiST) for the IR related fields\n",
    "* Additionally, design a field to hold:\n",
    "    - Sentiment\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Your Design here. You can describe your design with text or picture\n",
    "## ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 2: Implement the database in your PostgreSQL schema\n",
    "\n",
    "You can choose any of the three ways to implement your database. \n",
    "\n",
    "* sql magic \n",
    "* sql terminal \n",
    "* psycopg2 or sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement cells of Python Code that\n",
    "\n",
    "* collect the latest posts from a subreddit of your choice (should be text-dominant not image/video) and collect at least 500 posts (if possible),\n",
    "* processes the messages to extract id, title, link, author, subreddit, tag/flair, timestamp, etc. \n",
    "* process the text for IR, and\n",
    "* perform computational linguistics (e.g., get sentiment scores)\n",
    "* then insert the data into your database.\n",
    "\n",
    "\n",
    "Notes: \n",
    "* Each call to Reddit API returns 100 entries max. If we set a limit of more than 100, PRAW will handle multiple API calls internally and lazily fetches data. Check obfuscation and API limitation in https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html. \n",
    "* Develop and test your code with less than 100 messages from a subreddit. Then increase the limit and add few more subreddits. \n",
    "* While loading the table, test with one row \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='', \n",
    "                     client_secret='', \n",
    "                     user_agent='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: After you have loaded data from a subreddit, choose a few more subreddit and load those!\n",
    "\n",
    "Add cells if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In part II, we will search your database as `dsa_ro_user user`. To prepare your DB to be read, you will need to grant the dsa_ro_user schema access and select privileges on your table.\n",
    "\n",
    "```SQL\n",
    "GRANT USAGE ON SCHEMA <your schema> TO dsa_ro_user;  -- NOTE: change to your schema\n",
    "GRANT SELECT ON <your table> TO dsa_ro_user;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
