{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering\n",
    "\n",
    "In this exercise, we will explore the biclustering method on a spam dataset. Perform the following tasks: \n",
    "\n",
    "1. Create feature matrix using the raw text using TFIDF vectorizer \n",
    "    - you can use the one that is defined in the lab\n",
    "2. Remove empty rows/columns from the TFIDF matrix\n",
    "3. Apply spectral coclustering method on this matrix; extract two biclusters\n",
    "4. Compare results against k-means\n",
    "5. Identify the combination of spam and ham as well as important words in each bicluster\n",
    "    - do these biclusters make sense in terms of topic mixture and words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralCoclustering,SpectralBiclustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, there are 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df[['v1', 'v2']][:100]\n",
    "df_sub.columns = ['class', 'text']\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_sub['class']\n",
    "categories =['spam','ham']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Create a TFIDF matrix from the raw text. You can use `NumberNormalizingVectorizer` defined in the lab. Use min_df=2 and max_features = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\" Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2, max_features=100)\n",
    "X = vectorizer.fit_transform(df_sub['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Remove emty rows (i.e. all values are 0) from the feature matrix. Also, update y_true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>16</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "      <th>87121</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anymore</th>\n",
       "      <th>apply</th>\n",
       "      <th>...</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>week</th>\n",
       "      <th>wif</th>\n",
       "      <th>win</th>\n",
       "      <th>won</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  100  1000   16   20   50     87121  afternoon  anymore     apply  ...  \\\n",
       "0  0.0  0.0   0.0  0.0  0.0  0.0  0.000000        0.0      0.0  0.000000  ...   \n",
       "1  0.0  0.0   0.0  0.0  0.0  0.0  0.000000        0.0      0.0  0.000000  ...   \n",
       "2  0.0  0.0   0.0  0.0  0.0  0.0  0.404579        0.0      0.0  0.404579  ...   \n",
       "3  0.0  0.0   0.0  0.0  0.0  0.0  0.000000        0.0      0.0  0.000000  ...   \n",
       "4  0.0  0.0   0.0  0.0  0.0  0.0  0.000000        0.0      0.0  0.000000  ...   \n",
       "\n",
       "   watching  way  week       wif       win  won  word  work  yeah  yes  \n",
       "0       0.0  0.0   0.0  0.000000  0.000000  0.0   0.0   0.0   0.0  0.0  \n",
       "1       0.0  0.0   0.0  0.638962  0.000000  0.0   0.0   0.0   0.0  0.0  \n",
       "2       0.0  0.0   0.0  0.000000  0.404579  0.0   0.0   0.0   0.0  0.0  \n",
       "3       0.0  0.0   0.0  0.000000  0.000000  0.0   0.0   0.0   0.0  0.0  \n",
       "4       0.0  0.0   0.0  0.000000  0.000000  0.0   0.0   0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame.sparse.from_spmatrix(X, columns = vectorizer.get_feature_names())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty_rows = np.logical_not(df_clean.sum(axis=1) == 0)\n",
    "len(non_empty_rows)\n",
    "df_clean= df_clean[non_empty_rows]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_true[non_empty_rows]\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Define and fit a spectral coclustering model; use n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralCoclustering(n_clusters=2, random_state=0, svd_method='arpack')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocluster = SpectralCoclustering(n_clusters=len(categories),\n",
    "                                 svd_method='arpack', random_state=0)\n",
    "cocluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4. Compare the peformance against k-Means using V-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cocluster = cocluster.row_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-measure: 0.0218\n"
     ]
    }
   ],
   "source": [
    "v_score = v_measure_score(y_cocluster, y_true)\n",
    "print(f\"V-measure: {v_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=len(categories), \n",
    "                         random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-measure: 0.0081\n"
     ]
    }
   ],
   "source": [
    "y_kmeans = kmeans.fit_predict(X)\n",
    "v_score = v_measure_score(y_kmeans, y_true)\n",
    "print(f\"V-measure: {v_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5. Identify the combination of spam and ham as well as important words in each bicluster. Do these biclusters make sense in terms of topic mixture and words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = y_true.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(categories)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 89 documents, 98 words\n",
      "categories   : 81% ham, 19% spam\n",
      "words        : did, like, free, don, just, sorry, ok, way, ll, yes\n",
      "\n",
      "bicluster 1 : 3 documents, 2 words\n",
      "categories   : 100% ham\n",
      "words        : watching, wat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(', '.join(important_words)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first bicluster contains messages from the ham category, and the top words make sense. On the other hand, the 2nd bicluster is a mixture of ham and spam categories. We see words like free and prize are occurring within this bicluster which could be related to spam. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
