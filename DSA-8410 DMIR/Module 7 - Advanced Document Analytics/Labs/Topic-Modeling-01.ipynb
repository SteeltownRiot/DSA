{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Computational Linguistics - Topic Modeling\n",
    "\n",
    "In this lab, we will explore two topic modeling methods: LDA and NMF. Please check the readings to learn about these methods. \n",
    "\n",
    "To understand LDA and NMF we will create a toy dataset by downloading some wikipedia text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "wikipedia.set_lang(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this class fetches summary text from a wiki page\n",
    "\n",
    "# class TextFetcher:\n",
    "\n",
    "#     def __init__(self, title):\n",
    "#         self.title = title\n",
    "#         page = wikipedia.page(title)\n",
    "#         self.text = page.summary\n",
    "\n",
    "#     def getText(self):\n",
    "#         return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "#     nltk.download('stopwords')  # you might have uncomment this \n",
    "    tokens = word_tokenize(text)\n",
    "    return (\" \").join([word for word in tokens if word not in stopwords.words()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a toy dataset\n",
    "\n",
    "Let's create toy dataset of 6 wiki articles. For simplitycity we will extract the summary text (see above code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc = wikipedia.page(\"New York City\", auto_suggest=False)\n",
    "text1 = nyc.summary\n",
    "nlp = wikipedia.page(\"Natural Language Processing\", auto_suggest=False)\n",
    "text2 = nlp.summary\n",
    "tgg = wikipedia.page(\"The Great Gatsby\", auto_suggest=False)\n",
    "text3 = tgg.summary\n",
    "ml = wikipedia.page(\"Machine Learning\", auto_suggest=False)\n",
    "text4 = ml.summary\n",
    "la = wikipedia.page(\"Los Angeles\", auto_suggest=False)\n",
    "text5 = la.summary\n",
    "covid = wikipedia.page(\"Coronavirus\",  auto_suggest=False)\n",
    "text6 = covid.summary\n",
    "\n",
    "docs = [text1, text2, text3, text4, text5, text6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a term frequency matrix\n",
    "\n",
    "LDA works on term frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# count_vectorizer = CountVectorizer(stop_words='english', max_features=100)\n",
    "term_frequency = count_vectorizer.fit_transform(docs)\n",
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of term freq matrix = {term_frequency.shape}\")\n",
    "print(f\"Num of features identified = {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `CountVectorizer` method, we can use `max_features` to set the number of features if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit an LDA model\n",
    "\n",
    "Let's fit an LDA model with 5 topics (aka components). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)  # random_state is for replicating the result\n",
    "lda.fit(term_frequency)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the topics\n",
    "\n",
    "Now, let's print the top 10 words based on the words's weight learned by the LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Num of topics = {len(lda.components_)}\")\n",
    "\n",
    "# words' weights associated with topic 0\n",
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, term_weights in enumerate(model.components_):\n",
    "        \n",
    "        # get the index of top-k terms\n",
    "        sorted_indx = term_weights.argsort()\n",
    "#         print(sorted_indx)\n",
    "#         topk_words = [(feature_names[i], term_weights[i])for i in sorted_indx[-no_top_words - 1:]]\n",
    "        topk_words = [feature_names[i] for i in sorted_indx[-no_top_words :]]\n",
    "        print(f\"Topic {topic_idx}:\", end=None)\n",
    "        print(\";\".join(topk_words))\n",
    "\n",
    "#         print(\" \".join([feature_names[i]\n",
    "#              for i in term_weights.argsort()[:-no_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found the following output: \n",
    "\n",
    "```\n",
    "Topic 0:\n",
    "cause viruses coronaviruses rna birds genome lethal mild humans order\n",
    "Topic 1:\n",
    "language natural documents computers computer understanding processing speech recognition intelligence\n",
    "Topic 2:\n",
    "los angeles city learning machine data area largest california metropolitan\n",
    "Topic 3:\n",
    "new york city world largest area united metropolitan county states\n",
    "Topic 4:\n",
    "novel fitzgerald gatsby american great work following island believed title\n",
    "```\n",
    "\n",
    "If we observe the top-10 words in each of the topics, we could name each of topic as follows: \n",
    "\n",
    "* Topic 0: Corona Virus\n",
    "* Topic 1: Natural Language Processing\n",
    "* Topic 2: Los Angles \n",
    "* Topic 3: New York\n",
    "* Topic 4: The Great Gatsby Novel\n",
    "\n",
    "Naming topics are subjective, and there are no correct answers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a topic\n",
    "\n",
    "Word cloud or tag cloud is popular way to visualize topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code produces error, then install the package and execute the line again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list frequent terms with their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = lda.components_[1]  # take the corona topic\n",
    "no_top_words = 10\n",
    "\n",
    "weights_lda = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    print(feature_names[i], topic[i])\n",
    "    weights_lda[feature_names[i]] = topic[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "wc.generate_from_frequencies(weights_lda)\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic mixture in a document\n",
    "\n",
    "For each of the document, we can see how each topic is represented there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mixture = lda.transform(term_frequency[-1:])\n",
    "np.around(topic_mixture, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector represent topic mixtures for the Corona Virus wiki page. As expected Topic 1 is the dominant one for this document. \n",
    "\n",
    "Remember the dimentionality reduction methods from the Applied Machien Learning course. **In a shallow sense, topic modeling could be considered as a dimensionality reduction method.** Here we represent a document as a topic mixture, and the number of topics is a way less than the number of terms in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a topic model for classification/clustering\n",
    "\n",
    "Although the aim of a topic model is to identify the underlying structure of a document in terms of topics, given a corpus, we can use this method for classification and clustering. E.g., we can identify topic mixture of a new document, and label the document with the topic with maximum proportion as a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = wikipedia.page(\"Chicago\", auto_suggest=False)\n",
    "text7 = chicago.summary\n",
    "print(lda.transform(count_vectorizer.transform([text7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code, we estimate the topic mixture of the \"Chicago\" wikipage and see that the dominant topic is Topic 4, which is City topic. \n",
    "\n",
    "Given the topic mixture of all of the documents, we can perform clustering on the documents in the topic space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a topic model and choosing the number of topics\n",
    "\n",
    "As topic model is an unsupervised method, it is hard to evaluate as there is no gold standard. Here are few approaches that are commonly used for the evaluation (see [here](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)):\n",
    "\n",
    "- Eye Balling Models\n",
    "    - Top N words\n",
    "    - Topics / Documents\n",
    "- Intrinsic Evaluation Metrics\n",
    "    - Capturing model semantics\n",
    "    - Topics interpretability\n",
    "- Human Judgements\n",
    "    - What is a topic\n",
    "- Extrinsic Evaluation Metrics/Evaluation at task\n",
    "    - Is model good at performing predefined tasks, such as classification\n",
    "\n",
    "\n",
    "Like clustering, we manually set number of topics for topic models. But it is possible to use an intrinsic or extrinsic measure to identify the desirable number of topics. One such intrinsic measure is **perplexity** score (aka predictive likelihood), and it measures the goodness-of-fit. The lower perplexity is better.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for n_topics in range(2, 7): \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(term_frequency)    \n",
    "    score = lda.perplexity(term_frequency)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(2,7), scores)\n",
    "plt.xlabel('Num of Topics')\n",
    "plt.ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above plot, we can pick either 4 or 5 as the total number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "We can repeat the above practice with a Non-negative Matrix Factorization (NMF) method. For LDA we use TF matrix as input, but NMF method can take either of TF and TFIDF matrix as input. This time we will create a TFIDF matrix for representing the documents and then apply NFM. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of tfidf matrix = {tfidf.shape}\")\n",
    "print(f\"Num of features identified = {len(tfidf_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit an NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf = NMF(n_components=5, random_state=0)\n",
    "nmf.fit(term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize corona topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = nmf.components_[4]  # take the corona topic\n",
    "no_top_words = 10\n",
    "\n",
    "weights_nmf = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    weights_nmf[tfidf_feature_names[i]] = topic[i]\n",
    "weights_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "wc.generate_from_frequencies(weights_nmf)\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(weights_lda.items())\n",
    "df2 = pd.DataFrame(weights_nmf.items())\n",
    "\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see both LDA and NMF has the same word set for the Corona Virus topic, although the ordering is little bit different.\n",
    "\n",
    "---\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "230.17px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
