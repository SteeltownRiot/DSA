{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Computational Linguistics: Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When examining a corpora of text, like a person's tweets, what can we learn about how and what is expressed in that corpora by examining words and combinations of words? Is there a story that emerges? What patterns emerge or can be confirmed?\n",
    "\n",
    "Fortunately, we live in an era where famous people frequently use Twitter; and Twitter is good at producing a large number of finite (small) sets of words for us to examine. \n",
    "\n",
    "Let's take a look at some tweets from Donald Trump during his presidency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Lets import some libraries form mathplotlib ... its helpful for plotting. \n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#All the packages we are using in this project\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All tweets from Donald Trump are stored in the file 'realDonaldTrump_tweets.txt' listed below.\n",
    "\n",
    "## Text Analysis\n",
    "For this text analysis we will look at the following:\n",
    " - Text [collocations](https://en.wikipedia.org/wiki/Collocation) to find common words that go together\n",
    " - Regular expressions to parse out hashtags and high frequency user accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_path = '/dsa/data/all_datasets/linguistic/realDonaldTrump_tweets.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we use a regular expression to strip away symbols and web links in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as f:\n",
    "    raw_text = f.read()  # fetching all the tweets together\n",
    "    print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove all non-alphanumeric characters  and urls\n",
    "# \\w: alphanumeric charc; ^\\w: not alphanumeric\n",
    "# https.*\\b: matches strings that starts with https and ends with \n",
    "# \\b (\\b matches the empty string at the beginning or end of a word)\n",
    "\n",
    "raw = re.sub(r'[^\\w]|https.*\\b', ' ', raw_text)\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's \n",
    "tokens = word_tokenize(raw)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "# NLTK is our friend!\n",
    "freq_dist.plot(25);  # top 25 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_wo_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "freq_dist = nltk.FreqDist(tokens_wo_stopwords)\n",
    "freq_dist.plot(25);  # top 25 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend the stopword list by adding the following words. \n",
    "\n",
    "* `amp`: a part of ampersand (&amp;) \n",
    "* `RT`: represents retweet\n",
    "* `U`: not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystop_words = stop_words + ['amp', 'rt', 'u']\n",
    "tokens_wo_stopwords = [word for word in tokens if word.lower() not in mystop_words]\n",
    "freq_dist = nltk.FreqDist(tokens_wo_stopwords)\n",
    "freq_dist.plot(25);  # top 25 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "N-grams are an examination of sets of co-occuring terms in a corpus.\n",
    "You will often see bi-gram or tri-gram, an examination of word-pairs or word-triples, respectively.\n",
    " * Read more here: https://en.wikipedia.org/wiki/N-gram\n",
    " \n",
    "## First, let's look at bi-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def plot_ngram(tokens, num):\n",
    "    ngram = ngrams(tokens, num)\n",
    "    ngram_dist = nltk.FreqDist(ngram)\n",
    "    ngram_dist.plot(25)\n",
    "\n",
    "plot_ngram(tokens_wo_stopwords, 2) ##bigram frequency distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and now, tri-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngram(tokens_wo_stopwords, 3) ##trigram frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflecting on the plots above, is this what you expected?**\n",
    "\n",
    "---\n",
    "Now let's keep digging with some additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique trump tweet vocab: %i (including capitalized letters)\" % len(set(tokens)))\n",
    "print(\"Unique trump tweet vocab wo stop words: %i (including capitalized letters)\" % len(set(tokens_wo_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that \"Crooked Hillary\", \"FAKE NEWS\" are all the slogans that Trump frequently used in his campaign. \n",
    "\n",
    "---\n",
    "\n",
    "## Looking at hashtags with regular expressions and text analysis\n",
    "\n",
    "Examine the code below.\n",
    "You have seen regular expressions and NLTK frequency distributions.\n",
    "What things can we discover from the hashtags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"#(\\w+)\": a pattern that starts with # and then # is followed by one or more alphanumeric char\n",
    "hashtags = re.findall(r\"#(\\w+)\", raw_text)\n",
    "print(len(hashtags))\n",
    "hashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_hashtags = nltk.FreqDist(hashtags)\n",
    "\n",
    "sort_freq_hashtags = sorted(freq_hashtags.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "for k, v in sort_freq_hashtags[:10]:  # top 10 hashtags\n",
    "    print(f\"{k: <25}: {v: > 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a plot which is more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_hashtags.plot(25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection:\n",
    "\n",
    " * What do these hashtags tell you about the nature of Trumps Tweets? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answer below\n",
    "# -----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    " * What are the common topics? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answer below\n",
    "# -----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is addressed the most in Trump's tweets (via mentions)\n",
    "\n",
    "Note that, below, the code is looking for the \"@\" symbol, which is how people are mentioned on Twitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_users = re.findall(r\"@(\\w+)\", raw_text)\n",
    "freq_users = nltk.FreqDist(tag_users)\n",
    "freq_users.plot(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection:\n",
    "\n",
    " * What types of accounts are most frequently mentioned in Trump's tweets? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answer below\n",
    "# -----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection :\n",
    "\n",
    "\n",
    " * Based on your analysis of the data from Trump's tweets, what can you say about how Twitter is used? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answer below\n",
    "# -----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    " * Does this validate or invalidate how you thought about Trump's tweets previously? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answer below\n",
    "# -----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266.534px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
